file;linedbasedConf
/home/taes/taes/projects/jodd/revisions/rev_fec02fa_d3a363b/rev_fec02fa-d3a363b/jodd-petite/src/main/java/jodd/petite/PetiteContainer.java;<<<<<<< MINE
 * <ul>
 * <li>PetiteContainer - top layer that provides business usage</li>
 * <li>{@link PetiteRegistry}</li>
 * <li>{@link PetiteBeans}</li>
 * </ul>
||||||| BASE
 * <li>PetiteContainer - top layer that provides business usage
 * <li>{@link PetiteRegistry}
 * <li>{@link PetiteBeans}
=======
 * <ul>
 * <li>PetiteContainer - top layer that provides business usage methods
 * <li>{@link PetiteRegistry} - beans storage methods
 * <li>{@link PetiteBeans} - base layer for storing beans in scopes
>>>>>>> YOURS
/home/taes/taes/projects/jodd/revisions/rev_fec02fa_d3a363b/rev_fec02fa-d3a363b/jodd-petite/src/main/java/jodd/petite/PetiteBeans.java;<<<<<<< MINE
	 * <ul>
	 * <li>if name is missing, it will be resolved from the class (name or annotation)</li>
	 * <li>if wiring mode is missing, it will be resolved from the class (annotation or default one)</li>
	 * <li>if scope type is missing, it will be resolved from the class (annotation or default one)</li>
	 * </ul>
||||||| BASE
	 * <li>if name is missing, it will be resolved from the class (name or annotation)
	 * <li>if wiring mode is missing, it will be resolved from the class (annotation or default one)
	 * <li>if scope type is missing, it will be resolved from the class (annotation or default one)
=======
	 * <ul>
	 * <li>if name is missing, it will be resolved from the class (name or annotation)
	 * <li>if wiring mode is missing, it will be resolved from the class (annotation or default one)
	 * <li>if scope type is missing, it will be resolved from the class (annotation or default one)
>>>>>>> YOURS
/home/taes/taes/projects/liferay-plugins/revisions/rev_fc5104c_379a202/rev_fc5104c-379a202/portlets/opensocial-portlet/docroot/WEB-INF/src/com/liferay/opensocial/gadget/portlet/BaseGadgetPortlet.java;null
/home/taes/taes/projects/liferay-plugins/revisions/rev_fc5104c_379a202/rev_fc5104c-379a202/portlets/opensocial-portlet/docroot/WEB-INF/src/com/liferay/opensocial/gadget/portlet/BaseGadgetPortlet.java;null
/home/taes/taes/projects/liferay-plugins/revisions/rev_fc5104c_379a202/rev_fc5104c-379a202/portlets/opensocial-portlet/docroot/WEB-INF/src/com/liferay/opensocial/gadget/portlet/BaseGadgetPortlet.java;null
/home/taes/taes/projects/liferay-plugins/revisions/rev_fc5104c_379a202/rev_fc5104c-379a202/portlets/opensocial-portlet/docroot/WEB-INF/src/com/liferay/opensocial/gadget/portlet/BaseGadgetPortlet.java;null
/home/taes/taes/projects/liferay-plugins/revisions/rev_fc5104c_379a202/rev_fc5104c-379a202/portlets/opensocial-portlet/docroot/WEB-INF/src/com/liferay/opensocial/gadget/portlet/BaseGadgetPortlet.java;null
/home/taes/taes/projects/ribbon/revisions/rev_f1160f3_1e333eb/rev_f1160f3-1e333eb/ribbon-core/src/main/java/com/netflix/loadbalancer/BaseLoadBalancer.java;<<<<<<< MINE
import com.netflix.niws.client.ClientFactory;
import com.netflix.niws.client.NIWSClientException;
||||||| BASE
import com.netflix.niws.client.ClientFactory;
import com.netflix.niws.client.NIWSClientException;
import com.netflix.niws.client.NiwsClientConfig;
=======
import com.netflix.niws.client.NiwsClientConfig;
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_f1160f3_1e333eb/rev_f1160f3-1e333eb/ribbon-core/src/main/java/com/netflix/loadbalancer/BaseLoadBalancer.java;null
/home/taes/taes/projects/ribbon/revisions/rev_f1160f3_1e333eb/rev_f1160f3-1e333eb/ribbon-core/src/main/java/com/netflix/loadbalancer/AbstractLoadBalancer.java;<<<<<<< MINE
import com.netflix.niws.client.IClientConfigAware;

||||||| BASE
import com.netflix.niws.client.NiwsClientConfig;
import com.netflix.niws.client.IClientConfigAware;

=======
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_f1160f3_1e333eb/rev_f1160f3-1e333eb/ribbon-core/src/main/java/com/netflix/niws/client/AbstractLoadBalancerAwareClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_f1160f3_1e333eb/rev_f1160f3-1e333eb/ribbon-core/src/main/java/com/netflix/niws/client/NiwsClientConfig.java;<<<<<<< MINE
||||||| BASE
package com.netflix.niws.client;

import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.Properties;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.TimeUnit;

import org.apache.commons.configuration.AbstractConfiguration;
import org.apache.commons.configuration.Configuration;
import org.apache.commons.lang.StringUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.base.Strings;
import com.netflix.config.AbstractDynamicPropertyListener;
import com.netflix.config.ConfigurationManager;
import com.netflix.config.DynamicPropertyFactory;
import com.netflix.config.DynamicStringProperty;
import com.netflix.config.ExpandedConfigurationListenerAdapter;
import com.netflix.config.util.HttpVerbUriRegexPropertyValue;
import com.netflix.loadbalancer.DummyPing;
import com.netflix.niws.VipAddressResolver;

/**
 * Class that holds the NIWS Client Configuration to be used for <code>{@link RestClient}</code>)
 * @author Sudhir Tonse <stonse@netflix.com>
 *
 */
public class NiwsClientConfig implements IClientConfig {

    public static final Boolean DEFAULT_PRIORITIZE_VIP_ADDRESS_BASED_SERVERS = Boolean.TRUE;

    public static final String DEFAULT_NFLOADBALANCER_PING_CLASSNAME = DummyPing.class.getName();

    public static final String DEFAULT_NFLOADBALANCER_RULE_CLASSNAME = com.netflix.niws.client.AvailabilityFilteringRule.class.getName();

    public static final String DEFAULT_NFLOADBALANCER_CLASSNAME = com.netflix.niws.client.ZoneAwareNIWSDiscoveryLoadBalancer.class.getName();
    
    public static final String DEFAULT_CLIENT_CLASSNAME = "com.netflix.niws.client.http.RestClient";
    
    public static final String DEFAULT_VIPADDRESS_RESOLVER_CLASSNAME = com.netflix.niws.SimpleVipAddressResolver.class.getName();

    public static final String DEFAULT_PRIME_CONNECTIONS_URI = "/";

    public static final int DEFAULT_MAX_TOTAL_TIME_TO_PRIME_CONNECTIONS = 30*1000;

    public static final int DEFAULT_MAX_RETRIES_PER_SERVER_PRIME_CONNECTION = 2;

    public static final Boolean DEFAULT_ENABLE_PRIME_CONNECTIONS = Boolean.FALSE;

    public static final int DEFAULT_MAX_REQUESTS_ALLOWED_PER_WINDOW = Integer.MAX_VALUE;

    public static final int DEFAULT_REQUEST_THROTTLING_WINDOW_IN_MILLIS = 60*1000;

    public static final Boolean DEFAULT_ENABLE_REQUEST_THROTTLING = Boolean.FALSE;

    public static final Boolean DEFAULT_ENABLE_GZIP_CONTENT_ENCODING_FILTER = Boolean.FALSE;

    public static final Boolean DEFAULT_CONNECTION_POOL_CLEANER_TASK_ENABLED = Boolean.TRUE;

    public static final Boolean DEFAULT_FOLLOW_REDIRECTS = Boolean.TRUE;

    public static final float DEFAULT_PERCENTAGE_NIWS_EVENT_LOGGED = 0.0f;

    public static final int DEFAULT_MAX_AUTO_RETRIES_NEXT_SERVER = 0;

    public static final int DEFAULT_MAX_AUTO_RETRIES = 0;

    public static final int DEFAULT_READ_TIMEOUT = 5000;

    public static final int DEFAULT_CONNECTION_MANAGER_TIMEOUT = 2000;

    public static final int DEFAULT_CONNECT_TIMEOUT = 2000;

    public static final int DEFAULT_MAX_HTTP_CONNECTIONS_PER_HOST = 50;

    public static final int DEFAULT_MAX_TOTAL_HTTP_CONNECTIONS = 200;
    
    public static final Boolean DEFAULT_ENABLE_NIWSSTATS = Boolean.TRUE;
    
    public static final Boolean DEFAULT_ENABLE_NIWSERRORSTATS = Boolean.TRUE;
    
    public static final Boolean DEFAULT_USE_HTTP_CLIENT4 = Boolean.FALSE;
    
    public static final float DEFAULT_MIN_PRIME_CONNECTIONS_RATIO = 1.0f;
    
    public static final String DEFAULT_PRIME_CONNECTIONS_CLASS = "com.netflix.niws.client.HttpPrimeConnection";
    
    public static final String DEFAULT_SEVER_LIST_CLASS = "com.netflix.niws.client.DiscoveryEnabledNIWSServerList";
    
    public static final int DEFAULT_CONNECTION_IDLE_TIMERTASK_REPEAT_IN_MSECS = 30*1000; // every half minute (30 secs)
    
    public static final int DEFAULT_CONNECTIONIDLE_TIME_IN_MSECS = 30*1000; // all connections idle for 30 secs


    
    volatile Map<String, Object> properties = new ConcurrentHashMap<String, Object>();
    
    private static final Logger LOG = LoggerFactory.getLogger(NiwsClientConfig.class);

    private String clientName = null;
    
    private VipAddressResolver resolver = null;

    private boolean enableDynamicProperties = true;
    /**
     * Defaults for the parameters for the thread pool used by batchParallel
     * calls
     */
    public static final int DEFAULT_POOL_MAX_THREADS = DEFAULT_MAX_TOTAL_HTTP_CONNECTIONS;
    public static final int DEFAULT_POOL_MIN_THREADS = 1;
    public static final long DEFAULT_POOL_KEEP_ALIVE_TIME = 15 * 60L;
    public static final TimeUnit DEFAULT_POOL_KEEP_ALIVE_TIME_UNITS = TimeUnit.SECONDS;
    public static final Boolean DEFAULT_ENABLE_ZONE_AFFINITY = Boolean.FALSE;
    public static final Boolean DEFAULT_ENABLE_ZONE_EXCLUSIVITY = Boolean.FALSE;
    public static final int DEFAULT_PORT = 7001;
    public static final Boolean DEFAULT_ENABLE_LOADBALANCER = Boolean.TRUE;

    private static final String PROPERTY_NAMESPACE = "niws.client";

    public static final Boolean DEFAULT_OK_TO_RETRY_ON_ALL_OPERATIONS = Boolean.FALSE;
    
    private static ConcurrentHashMap<String, NiwsClientConfig> namedConfig = new ConcurrentHashMap<String, NiwsClientConfig>();

    public static final Boolean DEFAULT_ENABLE_NIWS_EVENT_LOGGING = Boolean.TRUE;

    private static ConcurrentHashMap<String, ConcurrentHashMap<String, Map<String, HttpVerbUriRegexPropertyValue>>> dynamicConfigMap =
        new ConcurrentHashMap<String, ConcurrentHashMap<String, Map<String, HttpVerbUriRegexPropertyValue>>>();
    
    private static final String[] DYNAMIC_PROPERTY_PREFIX = {"SLA", "NIWSStats", "ResponseCache", "MethodURI"};
    
    private Map<String, DynamicStringProperty> dynamicProperties = new ConcurrentHashMap<String, DynamicStringProperty>();
        
    static{
        ConfigurationManager.getConfigInstance().addConfigurationListener(
                new ExpandedConfigurationListenerAdapter(new NiwsConfigListener()));
    }    
    
    public NiwsClientConfig() {
        this.dynamicProperties.clear();
        this.enableDynamicProperties = false;
    }

    public NiwsClientConfig(Map<String, Object> properties) {
        if (properties != null) {
            for (IClientConfigKey niwsKey: CommonClientConfigKey.values()) {
                String key = niwsKey.key();
                Object value = properties.get(key);
                if (value != null) {
                    this.properties.put(key, value);
                }
            }
        }
        this.dynamicProperties.clear();
        this.enableDynamicProperties = false;
    }
    
    public static NiwsClientConfig getConfigWithDefaultProperties() {
        NiwsClientConfig config = new NiwsClientConfig();
        config.enableDynamicProperties = true;
        //Defaults
        config.putBooleanProperty(CommonClientConfigKey.UseHttpClient4, DEFAULT_USE_HTTP_CLIENT4);
        config.putIntegerProperty(CommonClientConfigKey.MaxHttpConnectionsPerHost, Integer.valueOf(DEFAULT_MAX_HTTP_CONNECTIONS_PER_HOST));
        config.putIntegerProperty(CommonClientConfigKey.MaxTotalHttpConnections, Integer.valueOf(DEFAULT_MAX_TOTAL_HTTP_CONNECTIONS));
        config.putIntegerProperty(CommonClientConfigKey.ConnectTimeout, Integer.valueOf(DEFAULT_CONNECT_TIMEOUT));
        config.putIntegerProperty(CommonClientConfigKey.ConnectionManagerTimeout, Integer.valueOf(DEFAULT_CONNECTION_MANAGER_TIMEOUT));
        config.putIntegerProperty(CommonClientConfigKey.ReadTimeout, Integer.valueOf(DEFAULT_READ_TIMEOUT));
        config.putIntegerProperty(CommonClientConfigKey.MaxAutoRetries, Integer.valueOf(DEFAULT_MAX_AUTO_RETRIES));
        config.putIntegerProperty(CommonClientConfigKey.MaxAutoRetriesNextServer, Integer.valueOf(DEFAULT_MAX_AUTO_RETRIES_NEXT_SERVER));
        config.putBooleanProperty(CommonClientConfigKey.OkToRetryOnAllOperations, DEFAULT_OK_TO_RETRY_ON_ALL_OPERATIONS);
        config.putBooleanProperty(CommonClientConfigKey.EnableNIWSEventLogging, DEFAULT_ENABLE_NIWS_EVENT_LOGGING);
        config.putFloatProperty(CommonClientConfigKey.PercentageNIWSEventLogged, Float.valueOf(DEFAULT_PERCENTAGE_NIWS_EVENT_LOGGED));  // 0=only log what the calling context suggests
        config.putBooleanProperty(CommonClientConfigKey.FollowRedirects, DEFAULT_FOLLOW_REDIRECTS);
        config.putBooleanProperty(CommonClientConfigKey.ConnectionPoolCleanerTaskEnabled, DEFAULT_CONNECTION_POOL_CLEANER_TASK_ENABLED); // default is true for RestClient
        config.putIntegerProperty(CommonClientConfigKey.ConnIdleEvictTimeMilliSeconds,
            Integer.valueOf(DEFAULT_CONNECTIONIDLE_TIME_IN_MSECS));
        config.putIntegerProperty(CommonClientConfigKey.ConnectionCleanerRepeatInterval,
            Integer.valueOf(DEFAULT_CONNECTION_IDLE_TIMERTASK_REPEAT_IN_MSECS));
        config.putBooleanProperty(CommonClientConfigKey.EnableGZIPContentEncodingFilter, DEFAULT_ENABLE_GZIP_CONTENT_ENCODING_FILTER);
        config.putBooleanProperty(CommonClientConfigKey.EnableRequestThrottling, DEFAULT_ENABLE_REQUEST_THROTTLING);
        config.putIntegerProperty(CommonClientConfigKey.RequestThrottlingWindowInMSecs, Integer.valueOf(DEFAULT_REQUEST_THROTTLING_WINDOW_IN_MILLIS));
        config.putIntegerProperty(CommonClientConfigKey.MaxRequestsAllowedPerWindow, Integer.valueOf(DEFAULT_MAX_REQUESTS_ALLOWED_PER_WINDOW));
        String proxyHost = ConfigurationManager.getConfigInstance().getString(getDefaultPropName(CommonClientConfigKey.ProxyHost.key()));
        if (proxyHost != null && proxyHost.length() > 0) {
            config.setProperty(CommonClientConfigKey.ProxyHost, proxyHost);
        }
        Integer proxyPort = ConfigurationManager
                .getConfigInstance()
                .getInteger(
                        getDefaultPropName(CommonClientConfigKey.ProxyPort),
                        (Integer.MIN_VALUE + 1)); // + 1 just to avoid potential clash with user setting
        if (proxyPort != (Integer.MIN_VALUE + 1)) {
            config.setProperty(CommonClientConfigKey.ProxyPort, proxyPort);
        }
        config.putIntegerProperty(CommonClientConfigKey.Port, Integer.valueOf(DEFAULT_PORT));
        config.putBooleanProperty(CommonClientConfigKey.EnablePrimeConnections, DEFAULT_ENABLE_PRIME_CONNECTIONS);
        config.putIntegerProperty(CommonClientConfigKey.MaxRetriesPerServerPrimeConnection, Integer.valueOf(DEFAULT_MAX_RETRIES_PER_SERVER_PRIME_CONNECTION));
        config.putIntegerProperty(CommonClientConfigKey.MaxTotalTimeToPrimeConnections, Integer.valueOf(DEFAULT_MAX_TOTAL_TIME_TO_PRIME_CONNECTIONS));
        config.putStringProperty(CommonClientConfigKey.PrimeConnectionsURI, DEFAULT_PRIME_CONNECTIONS_URI);
        config.putIntegerProperty(CommonClientConfigKey.PoolMinThreads, Integer.valueOf(DEFAULT_POOL_MIN_THREADS));
        config.putIntegerProperty(CommonClientConfigKey.PoolMaxThreads, Integer.valueOf(DEFAULT_POOL_MAX_THREADS));
        config.putLongProperty(CommonClientConfigKey.PoolKeepAliveTime, Long.valueOf(DEFAULT_POOL_KEEP_ALIVE_TIME));
        config.putTimeUnitProperty(CommonClientConfigKey.PoolKeepAliveTimeUnits,DEFAULT_POOL_KEEP_ALIVE_TIME_UNITS);
        config.putBooleanProperty(CommonClientConfigKey.EnableZoneAffinity, DEFAULT_ENABLE_ZONE_AFFINITY);
        config.putBooleanProperty(CommonClientConfigKey.EnableZoneExclusivity, DEFAULT_ENABLE_ZONE_EXCLUSIVITY);
        config.putStringProperty(CommonClientConfigKey.ClientClassName, DEFAULT_CLIENT_CLASSNAME);
        config.putStringProperty(CommonClientConfigKey.NFLoadBalancerClassName, DEFAULT_NFLOADBALANCER_CLASSNAME);
        config.putStringProperty(CommonClientConfigKey.NFLoadBalancerRuleClassName, DEFAULT_NFLOADBALANCER_RULE_CLASSNAME);
        config.putStringProperty(CommonClientConfigKey.NFLoadBalancerPingClassName, DEFAULT_NFLOADBALANCER_PING_CLASSNAME);
        config.putBooleanProperty(CommonClientConfigKey.PrioritizeVipAddressBasedServers, DEFAULT_PRIORITIZE_VIP_ADDRESS_BASED_SERVERS);
        config.putBooleanProperty(CommonClientConfigKey.EnableNIWSStats, DEFAULT_ENABLE_NIWSSTATS);
        config.putBooleanProperty(CommonClientConfigKey.EnableNIWSErrorStats, DEFAULT_ENABLE_NIWSERRORSTATS);
        config.putFloatProperty(CommonClientConfigKey.MinPrimeConnectionsRatio, DEFAULT_MIN_PRIME_CONNECTIONS_RATIO);
        config.putBooleanProperty(CommonClientConfigKey.UseTunnel, Boolean.FALSE);
        config.putStringProperty(CommonClientConfigKey.PrimeConnectionsClassName, DEFAULT_PRIME_CONNECTIONS_CLASS);
        // putBooleanProperty(CommonClientConfigKey.PrioritizeIntStack, Boolean.FALSE);
        config.putStringProperty(CommonClientConfigKey.VipAddressResolverClassName, DEFAULT_VIPADDRESS_RESOLVER_CLASSNAME);
        return config;
    }
    
    
    private void setPropertyInternal(IClientConfigKey propName, Object value) {
        setPropertyInternal(propName.key(), value);
    }

    private String getConfigKey(String propName) {
        return (clientName == null) ? getDefaultPropName(propName) : getInstancePropName(clientName, propName);
    }
    
    private void setPropertyInternal(final String propName, Object value) {
        String stringValue = (value == null) ? "" : String.valueOf(value);
        properties.put(propName, stringValue);
        if (!enableDynamicProperties) {
            return;
        }
        String configKey = getConfigKey(propName);
        final DynamicStringProperty prop = DynamicPropertyFactory.getInstance().getStringProperty(configKey, null);
        Runnable callback = new Runnable() {
            @Override
            public void run() {
                String value = prop.get();
                if (value != null) {
                    properties.put(propName, value);
                } else {
                    properties.remove(propName);
                }
            }
            
            // equals and hashcode needed 
            // since this is anonymous object is later used as a set key
            
            @Override 
            public boolean equals(Object other){
            	if (other == null) {
            		return false;
            	}
            	if (getClass() == other.getClass()) {
                    return toString().equals(other.toString());
                }
                return false;
            }
            
            @Override
            public String toString(){
            	return propName;
            }
            
            @Override
            public int hashCode(){
            	return propName.hashCode();
            }
            
            
        };
        prop.addCallback(callback);
        dynamicProperties.put(propName, prop);
    }
    
    
	// Helper methods which first check if a "default" (with rest client name)
	// property exists. If so, that value is used, else the default value
	// passed as argument is used to put into the properties member variable
    private void putIntegerProperty(IClientConfigKey propName, Integer defaultValue) {
        Integer value = ConfigurationManager.getConfigInstance().getInteger(
                getDefaultPropName(propName), defaultValue);
        setPropertyInternal(propName, value);
    }

    private void putLongProperty(IClientConfigKey propName, Long defaultValue) {
        Long value = ConfigurationManager.getConfigInstance().getLong(
                getDefaultPropName(propName), defaultValue);
        setPropertyInternal(propName, value);
    }
    
    private void putFloatProperty(IClientConfigKey propName, Float defaultValue) {
        Float value = ConfigurationManager.getConfigInstance().getFloat(
                getDefaultPropName(propName), defaultValue);
        setPropertyInternal(propName, value);
    }
    
    private void putTimeUnitProperty(IClientConfigKey propName, TimeUnit defaultValue) {
        TimeUnit value = defaultValue;
        String propValue = ConfigurationManager.getConfigInstance().getString(
                getDefaultPropName(propName));
        if(propValue != null && propValue.length() > 0) {
            value = TimeUnit.valueOf(propValue);
        }
        setPropertyInternal(propName, value);
    }
    
    static String getDefaultPropName(String propName) {
        return PROPERTY_NAMESPACE + "." + propName;
    }

    public static String getDefaultPropName(IClientConfigKey propName) {
        return getDefaultPropName(propName.key());
    }

    
    private void putStringProperty(IClientConfigKey propName, String defaultValue) {
        String value = ConfigurationManager.getConfigInstance().getString(
                getDefaultPropName(propName), defaultValue);
        setPropertyInternal(propName, value);
    }
    
    private void putBooleanProperty(IClientConfigKey propName, Boolean defaultValue) {
        Boolean value = ConfigurationManager.getConfigInstance().getBoolean(
                getDefaultPropName(propName), defaultValue);
        setPropertyInternal(propName, value);
    }
    
    /**
     * Enum Class to contain properties of the Client
     * @author stonse
     *
     */
    public enum NiwsClientConfigKey implements IClientConfigKey {

        AppName(CommonClientConfigKey.AppName.key()),
        Version("Version"),
        Port("Port"),
        SecurePort("SecurePort"),
        VipAddress("VipAddress"),
        DeploymentContextBasedVipAddresses("DeploymentContextBasedVipAddresses"),       
        MaxAutoRetries("MaxAutoRetries"),
        MaxAutoRetriesNextServer("MaxAutoRetriesNextServer"),
        OkToRetryOnAllOperations("OkToRetryOnAllOperations"),
        RequestSpecificRetryOn("RequestSpecificRetryOn"),
        ReceiveBuffferSize("ReceiveBuffferSize"),
        EnableNIWSEventLogging("EnableNIWSEventLogging"),
        EnnableVerboseErrorLogging("EnableVerboseErrorLogging"),
        PercentageNIWSEventLogged("PercentageNIWSEventLogged"),
        EnableRequestThrottling("EnableRequestThrottling"),
        RequestThrottlingWindowInMSecs("RequestThrottlingWindowInMSecs"),
        MaxRequestsAllowedPerWindow("MaxRequestsAllowedPerWindow"),        
        EnablePrimeConnections("EnablePrimeConnections"),
        PrimeConnectionsClassName("PrimeConnectionsClassName"),
        MaxRetriesPerServerPrimeConnection("MaxRetriesPerServerPrimeConnection"),
        MaxTotalTimeToPrimeConnections("MaxTotalTimeToPrimeConnections"),
        MinPrimeConnectionsRatio("MinPrimeConnectionsRatio"),
        PrimeConnectionsURI("PrimeConnectionsURI"),
        PoolMaxThreads("PoolMaxThreads"),
        PoolMinThreads("PoolMinThreads"),
        PoolKeepAliveTime("PoolKeepAliveTime"),
        PoolKeepAliveTimeUnits("PoolKeepAliveTimeUnits"),
        SLA("SLA"),
        SLAMinFailureResponseCode("SLAMinFailureResponseCode"),
        EnableNIWSStats("EnableNIWSStats"), // enable the feature of collecting request stats
        EnableNIWSErrorStats("EnableNIWSErrorStats"), // capture numErrors and other stats per Error Code
        NIWSStats("NIWSStats"), // The property key used per request stat alias

        //HTTP Client Related
        UseHttpClient4("UseHttpClient4"),
        MaxHttpConnectionsPerHost("MaxHttpConnectionsPerHost"),
        MaxTotalHttpConnections("MaxTotalHttpConnections"),
        IsSecure("IsSecure"),
        GZipPayload("GZipPayload"),
        ConnectTimeout("ConnectTimeout"),
        ReadTimeout("ReadTimeout"),
        SendBufferSize("SendBufferSize"),
        StaleCheckingEnabled("StaleCheckingEnabled"),
        Linger("Linger"),
        ConnectionManagerTimeout("ConnectionManagerTimeout"),
        FollowRedirects("FollowRedirects"),
        ConnectionPoolCleanerTaskEnabled("ConnectionPoolCleanerTaskEnabled"),
        ConnIdleEvictTimeMilliSeconds("ConnIdleEvictTimeMilliSeconds"),
        ConnectionCleanerRepeatInterval("ConnectionCleanerRepeatInterval"),
        EnableGZIPContentEncodingFilter("EnableGZIPContentEncodingFilter"),
        ProxyHost("ProxyHost"),
        ProxyPort("ProxyPort"),
        KeyStore("KeyStore"),
        KeyStorePassword("KeyStorePassword"),
        TrustStore("TrustStore"),
        TrustStorePassword("TrustStorePassword"),

        // Client implementation        
        ClientClassName("ClientClassName"),
        
        //LoadBalancer Related
        InitializeNFLoadBalancer("InitializeNFLoadBalancer"),
        NFLoadBalancerClassName("NFLoadBalancerClassName"),
        NFLoadBalancerRuleClassName("NFLoadBalancerRuleClassName"),
        NFLoadBalancerPingClassName("NFLoadBalancerPingClassName"),
        NFLoadBalancerPingInterval("NFLoadBalancerPingInterval"),
        NFLoadBalancerMaxTotalPingTime("NFLoadBalancerMaxTotalPingTime"),
        NIWSServerListClassName("NIWSServerListClassName"),
        NIWSServerListFilterClassName("NIWSServerListFilterClassName"),
        EnableMarkingServerDownOnReachingFailureLimit("EnableMarkingServerDownOnReachingFailureLimit"),
        ServerDownFailureLimit("ServerDownFailureLimit"),
        ServerDownStatWindowInMillis("ServerDownStatWindowInMillis"),
        EnableZoneAffinity("EnableZoneAffinity"),
        EnableZoneExclusivity("EnableZoneExclusivity"),
        PrioritizeVipAddressBasedServers("PrioritizeVipAddressBasedServers"),
        VipAddressResolverClassName("VipAddressResolverClassName"),

        //Tunnelling
        UseTunnel("UseTunnel");

        private final String configKey;

        NiwsClientConfigKey(String configKey) {
            this.configKey = configKey;
        }

        /* (non-Javadoc)
		 * @see com.netflix.niws.client.ClientConfig#key()
		 */
        @Override
		public String key() {
            return configKey;
        }
    }

    public void setClientName(String clientName){
        this.clientName  = clientName;
    }

    /* (non-Javadoc)
	 * @see com.netflix.niws.client.CliengConfig#getClientName()
	 */
    @Override
	public String getClientName() {
        return clientName;
    }

    /* (non-Javadoc)
	 * @see com.netflix.niws.client.CliengConfig#loadProperties(java.lang.String)
	 */
    @Override
	public void loadProperties(String restClientName){
        setClientName(restClientName);
        Configuration props = ConfigurationManager.getConfigInstance().subset(restClientName);        
        for (Iterator<String> keys = props.getKeys(); keys.hasNext(); ){
            String key = keys.next();
            String prop = key;
            if (prop.startsWith(PROPERTY_NAMESPACE)){
                prop = prop.substring(PROPERTY_NAMESPACE.length() + 1);
            }
            setPropertyInternal(prop, props.getProperty(key));
        }
        
        for (String dynamicPropPrefix: DYNAMIC_PROPERTY_PREFIX) {
            ConcurrentHashMap<String, Map<String, HttpVerbUriRegexPropertyValue>> map = new ConcurrentHashMap<String, Map<String, HttpVerbUriRegexPropertyValue>>();
            ConcurrentHashMap<String, Map<String, HttpVerbUriRegexPropertyValue>> previous = dynamicConfigMap.putIfAbsent(dynamicPropPrefix, map);
            if(previous != null) {
                map = previous;
            }
            initializeDynamicConfig(dynamicPropPrefix, map);
        }
    }
    
    @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "DC_DOUBLECHECK")
    private VipAddressResolver getVipAddressResolver() {
        if (resolver == null) {
            synchronized (this) {
                if (resolver == null) {
                    try {
                        resolver = (VipAddressResolver) Class.forName(
                                (String) getProperty(CommonClientConfigKey.VipAddressResolverClassName)).newInstance();
                    } catch (Throwable e) {
                        LOG.error("Cannot instantiate VipAddressResolver", e);
                    }
                }
            }
        }
        return resolver;
        
    }

    public String resolveDeploymentContextbasedVipAddresses(){
        
        String deploymentContextBasedVipAddressesMacro = (String) getProperty(CommonClientConfigKey.DeploymentContextBasedVipAddresses);
        return getVipAddressResolver().resolve(deploymentContextBasedVipAddressesMacro, this);
    }

    public String getAppName(){
        String appName = null;
        Object an = getProperty(CommonClientConfigKey.AppName);
        if (an!=null){
            appName = "" + an;
        }
        return appName;
    }

    public String getVersion(){
        String version = null;
        Object an = getProperty(CommonClientConfigKey.Version);
        if (an!=null){
            version = "" + an;
        }
        return version;
    }

    /* (non-Javadoc)
	 * @see com.netflix.niws.client.CliengConfig#getProperties()
	 */
    @Override
	public  Map<String, Object> getProperties() {
        return properties;
    }

    /**
     * Set the underlying properties cache. This may cause inconsistencies with dynamic properties.
     * Instead, use {@link #setProperty(NiwsClientConfigKey, Object)} to set property.
     * 
     * @param properties
     */
    @Deprecated 
    public void setProperties(Map properties) {
        this.properties = (Map<String, Object>) properties;
    }

    /* (non-Javadoc)
	 * @see com.netflix.niws.client.CliengConfig#setProperty(com.netflix.niws.client.ClientConfigKey, java.lang.Object)
	 */
    @Override
	public void setProperty(IClientConfigKey key, Object value){
        setPropertyInternal(key.key(), value);
    }

    public IClientConfig applyOverride(IClientConfig override) {
        if (override == null) {
            return this;
        }
        for (IClientConfigKey key: CommonClientConfigKey.values()) {
            Object value = override.getProperty(key);
            if (value != null) {
                setProperty(key, value);
            }
        }
        return this;
    }
    
    /**
     * Set a property. Should use {@link #setProperty(NiwsClientConfigKey, Object)} instead.    
     * 
     * @param key
     * @param value
     */
    @Deprecated
    public void setProperty(String key, Object value){
        setPropertyInternal(key, value);
    }

    /* (non-Javadoc)
	 * @see com.netflix.niws.client.CliengConfig#getProperty(com.netflix.niws.client.ClientConfigKey)
	 */
    @Override
	public Object getProperty(IClientConfigKey key){
        String propName = key.key();
        DynamicStringProperty dynamicProperty = dynamicProperties.get(propName);
        if (dynamicProperty != null) {
            String dynamicValue = dynamicProperty.get();
            if (dynamicValue != null) {
                return dynamicValue;
            }
        }
        return properties.get(propName);
    }

    /* (non-Javadoc)
	 * @see com.netflix.niws.client.CliengConfig#getProperty(com.netflix.niws.client.ClientConfigKey, java.lang.Object)
	 */
    @Override
	public Object getProperty(IClientConfigKey key, Object defaultVal){
        Object val = getProperty(key);
        if (val == null){
            return defaultVal;
        }
        return val;
    }

    public static Object getProperty(Map<String, Object> config, IClientConfigKey key, Object defaultVal) {
        Object val = config.get(key.key());
        if (val == null) {
            return defaultVal;
        }
        return val;
    }

    public static Object getProperty(Map<String, Object> config, IClientConfigKey key) {
        return getProperty(config, key, null);
    }

    public boolean isSecure() {
        Object oo = getProperty(CommonClientConfigKey.IsSecure);
        if (oo != null) {
            return Boolean.parseBoolean(oo.toString());
        } else {
        	return false;
        }
    }

    /* (non-Javadoc)
	 * @see com.netflix.niws.client.CliengConfig#containsProperty(com.netflix.niws.client.ClientConfigKey)
	 */
    @Override
	public boolean containsProperty(IClientConfigKey key){
        Object oo = getProperty(key);
        return oo!=null? true: false;
    }

    @Override
    public String toString(){
        final StringBuilder sb = new StringBuilder();
        String separator = "";

        sb.append("NiwsClientConfig:");
        for (IClientConfigKey key: CommonClientConfigKey.values()) {
            final Object value = getProperty(key);

            sb.append(separator);
            separator = ", ";
            sb.append(key).append(":");
            if (key.key().endsWith("Password") && value instanceof String) {
                sb.append(Strings.repeat("*", ((String) value).length()));
            } else {
                sb.append(value);
            }
        }
        return sb.toString();
    }

    
    private void initializeDynamicConfig(String prefix, 
            ConcurrentHashMap<String, Map<String, HttpVerbUriRegexPropertyValue>> configMapForPrefix) {
        AbstractConfiguration configInstance = ConfigurationManager.getConfigInstance();
        // load any pre-configured dynamic properties
        String niwsPropertyPrefix = getClientName() + "."
        + PROPERTY_NAMESPACE;
        String prefixWithNoDot = niwsPropertyPrefix + "." + prefix;
        String configPrefix = prefixWithNoDot + ".";

        if (configInstance != null) {
            Configuration c = configInstance.subset(prefixWithNoDot);
            if (c != null) {
                Iterator<?> it = c.getKeys();
                if (it != null) {
                    while (it.hasNext()) {
                        String alias = (String) it.next();
                        if (alias != null) {
                            // we have a property of interest - add it to
                            // our
                            // map
                            String value = configInstance.getString(configPrefix + alias);
                            if (value != null) {
                                Map<String, HttpVerbUriRegexPropertyValue> aliasMap = configMapForPrefix.get(getClientName());
                                if (aliasMap == null) {
                                    aliasMap = new ConcurrentHashMap<String, HttpVerbUriRegexPropertyValue>();
                                    Map<String, HttpVerbUriRegexPropertyValue> prev = configMapForPrefix.putIfAbsent(getClientName(),
                                            aliasMap);
                                    if (prev != null) {
                                    	aliasMap = prev;
                                    }
                                }
                                aliasMap.put(alias.trim(),
                                        HttpVerbUriRegexPropertyValue
                                                .getVerbUriRegex(value
                                                        .toString()));

                            }
                        }
                    }
                }
            }
        }

    }
    

    Map<String, HttpVerbUriRegexPropertyValue> getSlaAliasRuleMap() {
        return getDynamicPropMap("SLA");
    }

    private Map<String, HttpVerbUriRegexPropertyValue> getDynamicPropMap(String dynamicPropPrefix) {
        Map<String,HttpVerbUriRegexPropertyValue> map = new HashMap<String,HttpVerbUriRegexPropertyValue>();
        try{
            map = dynamicConfigMap.get(dynamicPropPrefix).get(getClientName());
        }catch(Exception e){
            LOG.warn("Unable to get config Map for <restClientName>.niws.client."
                            + dynamicPropPrefix + " prefix"); 
        }
        return map;
    }
    
    Map<String, HttpVerbUriRegexPropertyValue> getNIWSStatsConfigMap() {
            return getDynamicPropMap("NIWSStats");
    }

    Map<String, HttpVerbUriRegexPropertyValue> getCacheConfigMap() {
        return getDynamicPropMap("ResponseCache");
    }

    public Map<String, HttpVerbUriRegexPropertyValue> getMethodURIConfigMap() {
        return getDynamicPropMap("MethodURI");
    }
    /**
     * Listen to changes in properties for NIWS
     * @author stonse
     *
     */
    private static class NiwsConfigListener extends AbstractDynamicPropertyListener {
        
        private String getClientNameFromConfig(String name) {
            for (String prefix: DYNAMIC_PROPERTY_PREFIX) {
                if (name.contains(PROPERTY_NAMESPACE + "." + prefix)) {
                    return name.substring(0,name.indexOf(PROPERTY_NAMESPACE + "." + prefix) - 1);
                }
            }
            return null;
        }
        
        @Override
        public void handlePropertyEvent(String name, Object value,
                EventType eventType) {
            try {
                String clientName = getClientNameFromConfig(name);

                if (clientName != null) {
                    String niwsPropertyPrefix = clientName + "."
                            + PROPERTY_NAMESPACE;
                    for (String prefix : DYNAMIC_PROPERTY_PREFIX) {
                        String configPrefix = niwsPropertyPrefix + "." + prefix
                                + ".";
                        if (name != null && name.startsWith(configPrefix)) {
                            Map<String, HttpVerbUriRegexPropertyValue> aliasRuleMapForClient = dynamicConfigMap
                                    .get(prefix).get(clientName);
                            if (aliasRuleMapForClient == null) {
                                // no map exists so far, create one
                                aliasRuleMapForClient = new ConcurrentHashMap<String, HttpVerbUriRegexPropertyValue>();
                                Map<String, HttpVerbUriRegexPropertyValue> prev = dynamicConfigMap.get(prefix).putIfAbsent(clientName,
                                        aliasRuleMapForClient);
                                if (prev != null) {
                                	aliasRuleMapForClient = prev;
                                }
                            }

                            String alias = name.substring(configPrefix.length());
                            if (alias != null) {
                                alias = alias.trim();
                                switch (eventType) {
                                case CLEAR:
                                    aliasRuleMapForClient.remove(alias);
                                    break;
                                case ADD:
                                case SET:
                                    if (value != null) {
                                        aliasRuleMapForClient.put(alias,
                                                HttpVerbUriRegexPropertyValue
                                                        .getVerbUriRegex(value
                                                                .toString()));
                                    }
                                    break;
                                }
                            }
                        }
                    }
                }
            } catch (Throwable t) {
                LOG.warn("Unexpected error when checking for dynamic Rest "
                        + "Client property updates", t);
            }
        }
    }
    
    static void setProperty(Properties props, String restClientName, String key, String value){
        props.setProperty( getInstancePropName(restClientName, key), value);
    }

    public static String getInstancePropName(String restClientName,
            IClientConfigKey configKey) {
        return getInstancePropName(restClientName, configKey.key());
    }

    public static String getInstancePropName(String restClientName, String key) {
        return restClientName + "." + PROPERTY_NAMESPACE + "."
                + key;
    }
    
    public static IClientConfig getNamedConfig(String name) {
        NiwsClientConfig config = namedConfig.get(name);
        if (config != null) {
            return config;
        } else {
            config = getConfigWithDefaultProperties();
            config.loadProperties(name);
            NiwsClientConfig old = namedConfig.put(name, config);
            if (old != null) {
                config = old;
            }
            return config;
        }
    }
}=======
/*
*
* Copyright 2013 Netflix, Inc.
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*
*/
package com.netflix.niws.client;

import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.Properties;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.TimeUnit;

import org.apache.commons.configuration.AbstractConfiguration;
import org.apache.commons.configuration.Configuration;
import org.apache.commons.lang.StringUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.base.Strings;
import com.netflix.config.AbstractDynamicPropertyListener;
import com.netflix.config.ConfigurationManager;
import com.netflix.config.DynamicPropertyFactory;
import com.netflix.config.DynamicStringProperty;
import com.netflix.config.ExpandedConfigurationListenerAdapter;
import com.netflix.config.util.HttpVerbUriRegexPropertyValue;
import com.netflix.loadbalancer.DummyPing;
import com.netflix.niws.VipAddressResolver;

/**
 * Class that holds the NIWS Client Configuration to be used for <code>{@link RestClient}</code>)
 * @author Sudhir Tonse <stonse@netflix.com>
 *
 */
public class NiwsClientConfig implements IClientConfig {

    public static final Boolean DEFAULT_PRIORITIZE_VIP_ADDRESS_BASED_SERVERS = Boolean.TRUE;

    public static final String DEFAULT_NFLOADBALANCER_PING_CLASSNAME = DummyPing.class.getName();

    public static final String DEFAULT_NFLOADBALANCER_RULE_CLASSNAME = com.netflix.niws.client.AvailabilityFilteringRule.class.getName();

    public static final String DEFAULT_NFLOADBALANCER_CLASSNAME = com.netflix.niws.client.ZoneAwareNIWSDiscoveryLoadBalancer.class.getName();
    
    public static final String DEFAULT_CLIENT_CLASSNAME = "com.netflix.niws.client.http.RestClient";
    
    public static final String DEFAULT_VIPADDRESS_RESOLVER_CLASSNAME = com.netflix.niws.SimpleVipAddressResolver.class.getName();

    public static final String DEFAULT_PRIME_CONNECTIONS_URI = "/";

    public static final int DEFAULT_MAX_TOTAL_TIME_TO_PRIME_CONNECTIONS = 30*1000;

    public static final int DEFAULT_MAX_RETRIES_PER_SERVER_PRIME_CONNECTION = 2;

    public static final Boolean DEFAULT_ENABLE_PRIME_CONNECTIONS = Boolean.FALSE;

    public static final int DEFAULT_MAX_REQUESTS_ALLOWED_PER_WINDOW = Integer.MAX_VALUE;

    public static final int DEFAULT_REQUEST_THROTTLING_WINDOW_IN_MILLIS = 60*1000;

    public static final Boolean DEFAULT_ENABLE_REQUEST_THROTTLING = Boolean.FALSE;

    public static final Boolean DEFAULT_ENABLE_GZIP_CONTENT_ENCODING_FILTER = Boolean.FALSE;

    public static final Boolean DEFAULT_CONNECTION_POOL_CLEANER_TASK_ENABLED = Boolean.TRUE;

    public static final Boolean DEFAULT_FOLLOW_REDIRECTS = Boolean.TRUE;

    public static final float DEFAULT_PERCENTAGE_NIWS_EVENT_LOGGED = 0.0f;

    public static final int DEFAULT_MAX_AUTO_RETRIES_NEXT_SERVER = 0;

    public static final int DEFAULT_MAX_AUTO_RETRIES = 0;

    public static final int DEFAULT_READ_TIMEOUT = 5000;

    public static final int DEFAULT_CONNECTION_MANAGER_TIMEOUT = 2000;

    public static final int DEFAULT_CONNECT_TIMEOUT = 2000;

    public static final int DEFAULT_MAX_HTTP_CONNECTIONS_PER_HOST = 50;

    public static final int DEFAULT_MAX_TOTAL_HTTP_CONNECTIONS = 200;
    
    public static final Boolean DEFAULT_ENABLE_NIWSSTATS = Boolean.TRUE;
    
    public static final Boolean DEFAULT_ENABLE_NIWSERRORSTATS = Boolean.TRUE;
    
    public static final Boolean DEFAULT_USE_HTTP_CLIENT4 = Boolean.FALSE;
    
    public static final float DEFAULT_MIN_PRIME_CONNECTIONS_RATIO = 1.0f;
    
    public static final String DEFAULT_PRIME_CONNECTIONS_CLASS = "com.netflix.niws.client.HttpPrimeConnection";
    
    public static final String DEFAULT_SEVER_LIST_CLASS = "com.netflix.niws.client.DiscoveryEnabledNIWSServerList";
    
    public static final int DEFAULT_CONNECTION_IDLE_TIMERTASK_REPEAT_IN_MSECS = 30*1000; // every half minute (30 secs)
    
    public static final int DEFAULT_CONNECTIONIDLE_TIME_IN_MSECS = 30*1000; // all connections idle for 30 secs


    
    volatile Map<String, Object> properties = new ConcurrentHashMap<String, Object>();
    
    private static final Logger LOG = LoggerFactory.getLogger(NiwsClientConfig.class);

    private String clientName = null;
    
    private VipAddressResolver resolver = null;

    private boolean enableDynamicProperties = true;
    /**
     * Defaults for the parameters for the thread pool used by batchParallel
     * calls
     */
    public static final int DEFAULT_POOL_MAX_THREADS = DEFAULT_MAX_TOTAL_HTTP_CONNECTIONS;
    public static final int DEFAULT_POOL_MIN_THREADS = 1;
    public static final long DEFAULT_POOL_KEEP_ALIVE_TIME = 15 * 60L;
    public static final TimeUnit DEFAULT_POOL_KEEP_ALIVE_TIME_UNITS = TimeUnit.SECONDS;
    public static final Boolean DEFAULT_ENABLE_ZONE_AFFINITY = Boolean.FALSE;
    public static final Boolean DEFAULT_ENABLE_ZONE_EXCLUSIVITY = Boolean.FALSE;
    public static final int DEFAULT_PORT = 7001;
    public static final Boolean DEFAULT_ENABLE_LOADBALANCER = Boolean.TRUE;

    private static final String PROPERTY_NAMESPACE = "niws.client";

    public static final Boolean DEFAULT_OK_TO_RETRY_ON_ALL_OPERATIONS = Boolean.FALSE;
    
    private static ConcurrentHashMap<String, NiwsClientConfig> namedConfig = new ConcurrentHashMap<String, NiwsClientConfig>();

    public static final Boolean DEFAULT_ENABLE_NIWS_EVENT_LOGGING = Boolean.TRUE;

    private static ConcurrentHashMap<String, ConcurrentHashMap<String, Map<String, HttpVerbUriRegexPropertyValue>>> dynamicConfigMap =
        new ConcurrentHashMap<String, ConcurrentHashMap<String, Map<String, HttpVerbUriRegexPropertyValue>>>();
    
    private static final String[] DYNAMIC_PROPERTY_PREFIX = {"SLA", "NIWSStats", "ResponseCache", "MethodURI"};
    
    private Map<String, DynamicStringProperty> dynamicProperties = new ConcurrentHashMap<String, DynamicStringProperty>();
        
    static{
        ConfigurationManager.getConfigInstance().addConfigurationListener(
                new ExpandedConfigurationListenerAdapter(new NiwsConfigListener()));
    }    
    
    public NiwsClientConfig() {
        this.dynamicProperties.clear();
        this.enableDynamicProperties = false;
    }

    public NiwsClientConfig(Map<String, Object> properties) {
        if (properties != null) {
            for (IClientConfigKey niwsKey: CommonClientConfigKey.values()) {
                String key = niwsKey.key();
                Object value = properties.get(key);
                if (value != null) {
                    this.properties.put(key, value);
                }
            }
        }
        this.dynamicProperties.clear();
        this.enableDynamicProperties = false;
    }
    
    public static NiwsClientConfig getConfigWithDefaultProperties() {
        NiwsClientConfig config = new NiwsClientConfig();
        config.enableDynamicProperties = true;
        //Defaults
        config.putBooleanProperty(CommonClientConfigKey.UseHttpClient4, DEFAULT_USE_HTTP_CLIENT4);
        config.putIntegerProperty(CommonClientConfigKey.MaxHttpConnectionsPerHost, Integer.valueOf(DEFAULT_MAX_HTTP_CONNECTIONS_PER_HOST));
        config.putIntegerProperty(CommonClientConfigKey.MaxTotalHttpConnections, Integer.valueOf(DEFAULT_MAX_TOTAL_HTTP_CONNECTIONS));
        config.putIntegerProperty(CommonClientConfigKey.ConnectTimeout, Integer.valueOf(DEFAULT_CONNECT_TIMEOUT));
        config.putIntegerProperty(CommonClientConfigKey.ConnectionManagerTimeout, Integer.valueOf(DEFAULT_CONNECTION_MANAGER_TIMEOUT));
        config.putIntegerProperty(CommonClientConfigKey.ReadTimeout, Integer.valueOf(DEFAULT_READ_TIMEOUT));
        config.putIntegerProperty(CommonClientConfigKey.MaxAutoRetries, Integer.valueOf(DEFAULT_MAX_AUTO_RETRIES));
        config.putIntegerProperty(CommonClientConfigKey.MaxAutoRetriesNextServer, Integer.valueOf(DEFAULT_MAX_AUTO_RETRIES_NEXT_SERVER));
        config.putBooleanProperty(CommonClientConfigKey.OkToRetryOnAllOperations, DEFAULT_OK_TO_RETRY_ON_ALL_OPERATIONS);
        config.putBooleanProperty(CommonClientConfigKey.EnableNIWSEventLogging, DEFAULT_ENABLE_NIWS_EVENT_LOGGING);
        config.putFloatProperty(CommonClientConfigKey.PercentageNIWSEventLogged, Float.valueOf(DEFAULT_PERCENTAGE_NIWS_EVENT_LOGGED));  // 0=only log what the calling context suggests
        config.putBooleanProperty(CommonClientConfigKey.FollowRedirects, DEFAULT_FOLLOW_REDIRECTS);
        config.putBooleanProperty(CommonClientConfigKey.ConnectionPoolCleanerTaskEnabled, DEFAULT_CONNECTION_POOL_CLEANER_TASK_ENABLED); // default is true for RestClient
        config.putIntegerProperty(CommonClientConfigKey.ConnIdleEvictTimeMilliSeconds,
            Integer.valueOf(DEFAULT_CONNECTIONIDLE_TIME_IN_MSECS));
        config.putIntegerProperty(CommonClientConfigKey.ConnectionCleanerRepeatInterval,
            Integer.valueOf(DEFAULT_CONNECTION_IDLE_TIMERTASK_REPEAT_IN_MSECS));
        config.putBooleanProperty(CommonClientConfigKey.EnableGZIPContentEncodingFilter, DEFAULT_ENABLE_GZIP_CONTENT_ENCODING_FILTER);
        config.putBooleanProperty(CommonClientConfigKey.EnableRequestThrottling, DEFAULT_ENABLE_REQUEST_THROTTLING);
        config.putIntegerProperty(CommonClientConfigKey.RequestThrottlingWindowInMSecs, Integer.valueOf(DEFAULT_REQUEST_THROTTLING_WINDOW_IN_MILLIS));
        config.putIntegerProperty(CommonClientConfigKey.MaxRequestsAllowedPerWindow, Integer.valueOf(DEFAULT_MAX_REQUESTS_ALLOWED_PER_WINDOW));
        String proxyHost = ConfigurationManager.getConfigInstance().getString(getDefaultPropName(CommonClientConfigKey.ProxyHost.key()));
        if (proxyHost != null && proxyHost.length() > 0) {
            config.setProperty(CommonClientConfigKey.ProxyHost, proxyHost);
        }
        Integer proxyPort = ConfigurationManager
                .getConfigInstance()
                .getInteger(
                        getDefaultPropName(CommonClientConfigKey.ProxyPort),
                        (Integer.MIN_VALUE + 1)); // + 1 just to avoid potential clash with user setting
        if (proxyPort != (Integer.MIN_VALUE + 1)) {
            config.setProperty(CommonClientConfigKey.ProxyPort, proxyPort);
        }
        config.putIntegerProperty(CommonClientConfigKey.Port, Integer.valueOf(DEFAULT_PORT));
        config.putBooleanProperty(CommonClientConfigKey.EnablePrimeConnections, DEFAULT_ENABLE_PRIME_CONNECTIONS);
        config.putIntegerProperty(CommonClientConfigKey.MaxRetriesPerServerPrimeConnection, Integer.valueOf(DEFAULT_MAX_RETRIES_PER_SERVER_PRIME_CONNECTION));
        config.putIntegerProperty(CommonClientConfigKey.MaxTotalTimeToPrimeConnections, Integer.valueOf(DEFAULT_MAX_TOTAL_TIME_TO_PRIME_CONNECTIONS));
        config.putStringProperty(CommonClientConfigKey.PrimeConnectionsURI, DEFAULT_PRIME_CONNECTIONS_URI);
        config.putIntegerProperty(CommonClientConfigKey.PoolMinThreads, Integer.valueOf(DEFAULT_POOL_MIN_THREADS));
        config.putIntegerProperty(CommonClientConfigKey.PoolMaxThreads, Integer.valueOf(DEFAULT_POOL_MAX_THREADS));
        config.putLongProperty(CommonClientConfigKey.PoolKeepAliveTime, Long.valueOf(DEFAULT_POOL_KEEP_ALIVE_TIME));
        config.putTimeUnitProperty(CommonClientConfigKey.PoolKeepAliveTimeUnits,DEFAULT_POOL_KEEP_ALIVE_TIME_UNITS);
        config.putBooleanProperty(CommonClientConfigKey.EnableZoneAffinity, DEFAULT_ENABLE_ZONE_AFFINITY);
        config.putBooleanProperty(CommonClientConfigKey.EnableZoneExclusivity, DEFAULT_ENABLE_ZONE_EXCLUSIVITY);
        config.putStringProperty(CommonClientConfigKey.ClientClassName, DEFAULT_CLIENT_CLASSNAME);
        config.putStringProperty(CommonClientConfigKey.NFLoadBalancerClassName, DEFAULT_NFLOADBALANCER_CLASSNAME);
        config.putStringProperty(CommonClientConfigKey.NFLoadBalancerRuleClassName, DEFAULT_NFLOADBALANCER_RULE_CLASSNAME);
        config.putStringProperty(CommonClientConfigKey.NFLoadBalancerPingClassName, DEFAULT_NFLOADBALANCER_PING_CLASSNAME);
        config.putBooleanProperty(CommonClientConfigKey.PrioritizeVipAddressBasedServers, DEFAULT_PRIORITIZE_VIP_ADDRESS_BASED_SERVERS);
        config.putBooleanProperty(CommonClientConfigKey.EnableNIWSStats, DEFAULT_ENABLE_NIWSSTATS);
        config.putBooleanProperty(CommonClientConfigKey.EnableNIWSErrorStats, DEFAULT_ENABLE_NIWSERRORSTATS);
        config.putFloatProperty(CommonClientConfigKey.MinPrimeConnectionsRatio, DEFAULT_MIN_PRIME_CONNECTIONS_RATIO);
        config.putBooleanProperty(CommonClientConfigKey.UseTunnel, Boolean.FALSE);
        config.putStringProperty(CommonClientConfigKey.PrimeConnectionsClassName, DEFAULT_PRIME_CONNECTIONS_CLASS);
        // putBooleanProperty(CommonClientConfigKey.PrioritizeIntStack, Boolean.FALSE);
        config.putStringProperty(CommonClientConfigKey.VipAddressResolverClassName, DEFAULT_VIPADDRESS_RESOLVER_CLASSNAME);
        return config;
    }
    
    
    private void setPropertyInternal(IClientConfigKey propName, Object value) {
        setPropertyInternal(propName.key(), value);
    }

    private String getConfigKey(String propName) {
        return (clientName == null) ? getDefaultPropName(propName) : getInstancePropName(clientName, propName);
    }
    
    private void setPropertyInternal(final String propName, Object value) {
        String stringValue = (value == null) ? "" : String.valueOf(value);
        properties.put(propName, stringValue);
        if (!enableDynamicProperties) {
            return;
        }
        String configKey = getConfigKey(propName);
        final DynamicStringProperty prop = DynamicPropertyFactory.getInstance().getStringProperty(configKey, null);
        Runnable callback = new Runnable() {
            @Override
            public void run() {
                String value = prop.get();
                if (value != null) {
                    properties.put(propName, value);
                } else {
                    properties.remove(propName);
                }
            }
            
            // equals and hashcode needed 
            // since this is anonymous object is later used as a set key
            
            @Override 
            public boolean equals(Object other){
            	if (other == null) {
            		return false;
            	}
            	if (getClass() == other.getClass()) {
                    return toString().equals(other.toString());
                }
                return false;
            }
            
            @Override
            public String toString(){
            	return propName;
            }
            
            @Override
            public int hashCode(){
            	return propName.hashCode();
            }
            
            
        };
        prop.addCallback(callback);
        dynamicProperties.put(propName, prop);
    }
    
    
	// Helper methods which first check if a "default" (with rest client name)
	// property exists. If so, that value is used, else the default value
	// passed as argument is used to put into the properties member variable
    private void putIntegerProperty(IClientConfigKey propName, Integer defaultValue) {
        Integer value = ConfigurationManager.getConfigInstance().getInteger(
                getDefaultPropName(propName), defaultValue);
        setPropertyInternal(propName, value);
    }

    private void putLongProperty(IClientConfigKey propName, Long defaultValue) {
        Long value = ConfigurationManager.getConfigInstance().getLong(
                getDefaultPropName(propName), defaultValue);
        setPropertyInternal(propName, value);
    }
    
    private void putFloatProperty(IClientConfigKey propName, Float defaultValue) {
        Float value = ConfigurationManager.getConfigInstance().getFloat(
                getDefaultPropName(propName), defaultValue);
        setPropertyInternal(propName, value);
    }
    
    private void putTimeUnitProperty(IClientConfigKey propName, TimeUnit defaultValue) {
        TimeUnit value = defaultValue;
        String propValue = ConfigurationManager.getConfigInstance().getString(
                getDefaultPropName(propName));
        if(propValue != null && propValue.length() > 0) {
            value = TimeUnit.valueOf(propValue);
        }
        setPropertyInternal(propName, value);
    }
    
    static String getDefaultPropName(String propName) {
        return PROPERTY_NAMESPACE + "." + propName;
    }

    public static String getDefaultPropName(IClientConfigKey propName) {
        return getDefaultPropName(propName.key());
    }

    
    private void putStringProperty(IClientConfigKey propName, String defaultValue) {
        String value = ConfigurationManager.getConfigInstance().getString(
                getDefaultPropName(propName), defaultValue);
        setPropertyInternal(propName, value);
    }
    
    private void putBooleanProperty(IClientConfigKey propName, Boolean defaultValue) {
        Boolean value = ConfigurationManager.getConfigInstance().getBoolean(
                getDefaultPropName(propName), defaultValue);
        setPropertyInternal(propName, value);
    }
    
    /**
     * Enum Class to contain properties of the Client
     * @author stonse
     *
     */
    public enum NiwsClientConfigKey implements IClientConfigKey {

        AppName(CommonClientConfigKey.AppName.key()),
        Version("Version"),
        Port("Port"),
        SecurePort("SecurePort"),
        VipAddress("VipAddress"),
        DeploymentContextBasedVipAddresses("DeploymentContextBasedVipAddresses"),       
        MaxAutoRetries("MaxAutoRetries"),
        MaxAutoRetriesNextServer("MaxAutoRetriesNextServer"),
        OkToRetryOnAllOperations("OkToRetryOnAllOperations"),
        RequestSpecificRetryOn("RequestSpecificRetryOn"),
        ReceiveBuffferSize("ReceiveBuffferSize"),
        EnableNIWSEventLogging("EnableNIWSEventLogging"),
        EnnableVerboseErrorLogging("EnableVerboseErrorLogging"),
        PercentageNIWSEventLogged("PercentageNIWSEventLogged"),
        EnableRequestThrottling("EnableRequestThrottling"),
        RequestThrottlingWindowInMSecs("RequestThrottlingWindowInMSecs"),
        MaxRequestsAllowedPerWindow("MaxRequestsAllowedPerWindow"),        
        EnablePrimeConnections("EnablePrimeConnections"),
        PrimeConnectionsClassName("PrimeConnectionsClassName"),
        MaxRetriesPerServerPrimeConnection("MaxRetriesPerServerPrimeConnection"),
        MaxTotalTimeToPrimeConnections("MaxTotalTimeToPrimeConnections"),
        MinPrimeConnectionsRatio("MinPrimeConnectionsRatio"),
        PrimeConnectionsURI("PrimeConnectionsURI"),
        PoolMaxThreads("PoolMaxThreads"),
        PoolMinThreads("PoolMinThreads"),
        PoolKeepAliveTime("PoolKeepAliveTime"),
        PoolKeepAliveTimeUnits("PoolKeepAliveTimeUnits"),
        SLA("SLA"),
        SLAMinFailureResponseCode("SLAMinFailureResponseCode"),
        EnableNIWSStats("EnableNIWSStats"), // enable the feature of collecting request stats
        EnableNIWSErrorStats("EnableNIWSErrorStats"), // capture numErrors and other stats per Error Code
        NIWSStats("NIWSStats"), // The property key used per request stat alias

        //HTTP Client Related
        UseHttpClient4("UseHttpClient4"),
        MaxHttpConnectionsPerHost("MaxHttpConnectionsPerHost"),
        MaxTotalHttpConnections("MaxTotalHttpConnections"),
        IsSecure("IsSecure"),
        GZipPayload("GZipPayload"),
        ConnectTimeout("ConnectTimeout"),
        ReadTimeout("ReadTimeout"),
        SendBufferSize("SendBufferSize"),
        StaleCheckingEnabled("StaleCheckingEnabled"),
        Linger("Linger"),
        ConnectionManagerTimeout("ConnectionManagerTimeout"),
        FollowRedirects("FollowRedirects"),
        ConnectionPoolCleanerTaskEnabled("ConnectionPoolCleanerTaskEnabled"),
        ConnIdleEvictTimeMilliSeconds("ConnIdleEvictTimeMilliSeconds"),
        ConnectionCleanerRepeatInterval("ConnectionCleanerRepeatInterval"),
        EnableGZIPContentEncodingFilter("EnableGZIPContentEncodingFilter"),
        ProxyHost("ProxyHost"),
        ProxyPort("ProxyPort"),
        KeyStore("KeyStore"),
        KeyStorePassword("KeyStorePassword"),
        TrustStore("TrustStore"),
        TrustStorePassword("TrustStorePassword"),

        // Client implementation        
        ClientClassName("ClientClassName"),
        
        //LoadBalancer Related
        InitializeNFLoadBalancer("InitializeNFLoadBalancer"),
        NFLoadBalancerClassName("NFLoadBalancerClassName"),
        NFLoadBalancerRuleClassName("NFLoadBalancerRuleClassName"),
        NFLoadBalancerPingClassName("NFLoadBalancerPingClassName"),
        NFLoadBalancerPingInterval("NFLoadBalancerPingInterval"),
        NFLoadBalancerMaxTotalPingTime("NFLoadBalancerMaxTotalPingTime"),
        NIWSServerListClassName("NIWSServerListClassName"),
        NIWSServerListFilterClassName("NIWSServerListFilterClassName"),
        EnableMarkingServerDownOnReachingFailureLimit("EnableMarkingServerDownOnReachingFailureLimit"),
        ServerDownFailureLimit("ServerDownFailureLimit"),
        ServerDownStatWindowInMillis("ServerDownStatWindowInMillis"),
        EnableZoneAffinity("EnableZoneAffinity"),
        EnableZoneExclusivity("EnableZoneExclusivity"),
        PrioritizeVipAddressBasedServers("PrioritizeVipAddressBasedServers"),
        VipAddressResolverClassName("VipAddressResolverClassName"),

        //Tunnelling
        UseTunnel("UseTunnel");

        private final String configKey;

        NiwsClientConfigKey(String configKey) {
            this.configKey = configKey;
        }

        /* (non-Javadoc)
		 * @see com.netflix.niws.client.ClientConfig#key()
		 */
        @Override
		public String key() {
            return configKey;
        }
    }

    public void setClientName(String clientName){
        this.clientName  = clientName;
    }

    /* (non-Javadoc)
	 * @see com.netflix.niws.client.CliengConfig#getClientName()
	 */
    @Override
	public String getClientName() {
        return clientName;
    }

    /* (non-Javadoc)
	 * @see com.netflix.niws.client.CliengConfig#loadProperties(java.lang.String)
	 */
    @Override
	public void loadProperties(String restClientName){
        setClientName(restClientName);
        Configuration props = ConfigurationManager.getConfigInstance().subset(restClientName);        
        for (Iterator<String> keys = props.getKeys(); keys.hasNext(); ){
            String key = keys.next();
            String prop = key;
            if (prop.startsWith(PROPERTY_NAMESPACE)){
                prop = prop.substring(PROPERTY_NAMESPACE.length() + 1);
            }
            setPropertyInternal(prop, props.getProperty(key));
        }
        
        for (String dynamicPropPrefix: DYNAMIC_PROPERTY_PREFIX) {
            ConcurrentHashMap<String, Map<String, HttpVerbUriRegexPropertyValue>> map = new ConcurrentHashMap<String, Map<String, HttpVerbUriRegexPropertyValue>>();
            ConcurrentHashMap<String, Map<String, HttpVerbUriRegexPropertyValue>> previous = dynamicConfigMap.putIfAbsent(dynamicPropPrefix, map);
            if(previous != null) {
                map = previous;
            }
            initializeDynamicConfig(dynamicPropPrefix, map);
        }
    }
    
    @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "DC_DOUBLECHECK")
    private VipAddressResolver getVipAddressResolver() {
        if (resolver == null) {
            synchronized (this) {
                if (resolver == null) {
                    try {
                        resolver = (VipAddressResolver) Class.forName(
                                (String) getProperty(CommonClientConfigKey.VipAddressResolverClassName)).newInstance();
                    } catch (Throwable e) {
                        LOG.error("Cannot instantiate VipAddressResolver", e);
                    }
                }
            }
        }
        return resolver;
        
    }

    public String resolveDeploymentContextbasedVipAddresses(){
        
        String deploymentContextBasedVipAddressesMacro = (String) getProperty(CommonClientConfigKey.DeploymentContextBasedVipAddresses);
        return getVipAddressResolver().resolve(deploymentContextBasedVipAddressesMacro, this);
    }

    public String getAppName(){
        String appName = null;
        Object an = getProperty(CommonClientConfigKey.AppName);
        if (an!=null){
            appName = "" + an;
        }
        return appName;
    }

    public String getVersion(){
        String version = null;
        Object an = getProperty(CommonClientConfigKey.Version);
        if (an!=null){
            version = "" + an;
        }
        return version;
    }

    /* (non-Javadoc)
	 * @see com.netflix.niws.client.CliengConfig#getProperties()
	 */
    @Override
	public  Map<String, Object> getProperties() {
        return properties;
    }

    /**
     * Set the underlying properties cache. This may cause inconsistencies with dynamic properties.
     * Instead, use {@link #setProperty(NiwsClientConfigKey, Object)} to set property.
     * 
     * @param properties
     */
    @Deprecated 
    public void setProperties(Map properties) {
        this.properties = (Map<String, Object>) properties;
    }

    /* (non-Javadoc)
	 * @see com.netflix.niws.client.CliengConfig#setProperty(com.netflix.niws.client.ClientConfigKey, java.lang.Object)
	 */
    @Override
	public void setProperty(IClientConfigKey key, Object value){
        setPropertyInternal(key.key(), value);
    }

    public IClientConfig applyOverride(IClientConfig override) {
        if (override == null) {
            return this;
        }
        for (IClientConfigKey key: CommonClientConfigKey.values()) {
            Object value = override.getProperty(key);
            if (value != null) {
                setProperty(key, value);
            }
        }
        return this;
    }
    
    /**
     * Set a property. Should use {@link #setProperty(NiwsClientConfigKey, Object)} instead.    
     * 
     * @param key
     * @param value
     */
    @Deprecated
    public void setProperty(String key, Object value){
        setPropertyInternal(key, value);
    }

    /* (non-Javadoc)
	 * @see com.netflix.niws.client.CliengConfig#getProperty(com.netflix.niws.client.ClientConfigKey)
	 */
    @Override
	public Object getProperty(IClientConfigKey key){
        String propName = key.key();
        DynamicStringProperty dynamicProperty = dynamicProperties.get(propName);
        if (dynamicProperty != null) {
            String dynamicValue = dynamicProperty.get();
            if (dynamicValue != null) {
                return dynamicValue;
            }
        }
        return properties.get(propName);
    }

    /* (non-Javadoc)
	 * @see com.netflix.niws.client.CliengConfig#getProperty(com.netflix.niws.client.ClientConfigKey, java.lang.Object)
	 */
    @Override
	public Object getProperty(IClientConfigKey key, Object defaultVal){
        Object val = getProperty(key);
        if (val == null){
            return defaultVal;
        }
        return val;
    }

    public static Object getProperty(Map<String, Object> config, IClientConfigKey key, Object defaultVal) {
        Object val = config.get(key.key());
        if (val == null) {
            return defaultVal;
        }
        return val;
    }

    public static Object getProperty(Map<String, Object> config, IClientConfigKey key) {
        return getProperty(config, key, null);
    }

    public boolean isSecure() {
        Object oo = getProperty(CommonClientConfigKey.IsSecure);
        if (oo != null) {
            return Boolean.parseBoolean(oo.toString());
        } else {
        	return false;
        }
    }

    /* (non-Javadoc)
	 * @see com.netflix.niws.client.CliengConfig#containsProperty(com.netflix.niws.client.ClientConfigKey)
	 */
    @Override
	public boolean containsProperty(IClientConfigKey key){
        Object oo = getProperty(key);
        return oo!=null? true: false;
    }

    @Override
    public String toString(){
        final StringBuilder sb = new StringBuilder();
        String separator = "";

        sb.append("NiwsClientConfig:");
        for (IClientConfigKey key: CommonClientConfigKey.values()) {
            final Object value = getProperty(key);

            sb.append(separator);
            separator = ", ";
            sb.append(key).append(":");
            if (key.key().endsWith("Password") && value instanceof String) {
                sb.append(Strings.repeat("*", ((String) value).length()));
            } else {
                sb.append(value);
            }
        }
        return sb.toString();
    }

    
    private void initializeDynamicConfig(String prefix, 
            ConcurrentHashMap<String, Map<String, HttpVerbUriRegexPropertyValue>> configMapForPrefix) {
        AbstractConfiguration configInstance = ConfigurationManager.getConfigInstance();
        // load any pre-configured dynamic properties
        String niwsPropertyPrefix = getClientName() + "."
        + PROPERTY_NAMESPACE;
        String prefixWithNoDot = niwsPropertyPrefix + "." + prefix;
        String configPrefix = prefixWithNoDot + ".";

        if (configInstance != null) {
            Configuration c = configInstance.subset(prefixWithNoDot);
            if (c != null) {
                Iterator<?> it = c.getKeys();
                if (it != null) {
                    while (it.hasNext()) {
                        String alias = (String) it.next();
                        if (alias != null) {
                            // we have a property of interest - add it to
                            // our
                            // map
                            String value = configInstance.getString(configPrefix + alias);
                            if (value != null) {
                                Map<String, HttpVerbUriRegexPropertyValue> aliasMap = configMapForPrefix.get(getClientName());
                                if (aliasMap == null) {
                                    aliasMap = new ConcurrentHashMap<String, HttpVerbUriRegexPropertyValue>();
                                    Map<String, HttpVerbUriRegexPropertyValue> prev = configMapForPrefix.putIfAbsent(getClientName(),
                                            aliasMap);
                                    if (prev != null) {
                                    	aliasMap = prev;
                                    }
                                }
                                aliasMap.put(alias.trim(),
                                        HttpVerbUriRegexPropertyValue
                                                .getVerbUriRegex(value
                                                        .toString()));

                            }
                        }
                    }
                }
            }
        }

    }
    

    Map<String, HttpVerbUriRegexPropertyValue> getSlaAliasRuleMap() {
        return getDynamicPropMap("SLA");
    }

    private Map<String, HttpVerbUriRegexPropertyValue> getDynamicPropMap(String dynamicPropPrefix) {
        Map<String,HttpVerbUriRegexPropertyValue> map = new HashMap<String,HttpVerbUriRegexPropertyValue>();
        try{
            map = dynamicConfigMap.get(dynamicPropPrefix).get(getClientName());
        }catch(Exception e){
            LOG.warn("Unable to get config Map for <restClientName>.niws.client."
                            + dynamicPropPrefix + " prefix"); 
        }
        return map;
    }
    
    Map<String, HttpVerbUriRegexPropertyValue> getNIWSStatsConfigMap() {
            return getDynamicPropMap("NIWSStats");
    }

    Map<String, HttpVerbUriRegexPropertyValue> getCacheConfigMap() {
        return getDynamicPropMap("ResponseCache");
    }

    public Map<String, HttpVerbUriRegexPropertyValue> getMethodURIConfigMap() {
        return getDynamicPropMap("MethodURI");
    }
    /**
     * Listen to changes in properties for NIWS
     * @author stonse
     *
     */
    private static class NiwsConfigListener extends AbstractDynamicPropertyListener {
        
        private String getClientNameFromConfig(String name) {
            for (String prefix: DYNAMIC_PROPERTY_PREFIX) {
                if (name.contains(PROPERTY_NAMESPACE + "." + prefix)) {
                    return name.substring(0,name.indexOf(PROPERTY_NAMESPACE + "." + prefix) - 1);
                }
            }
            return null;
        }
        
        @Override
        public void handlePropertyEvent(String name, Object value,
                EventType eventType) {
            try {
                String clientName = getClientNameFromConfig(name);

                if (clientName != null) {
                    String niwsPropertyPrefix = clientName + "."
                            + PROPERTY_NAMESPACE;
                    for (String prefix : DYNAMIC_PROPERTY_PREFIX) {
                        String configPrefix = niwsPropertyPrefix + "." + prefix
                                + ".";
                        if (name != null && name.startsWith(configPrefix)) {
                            Map<String, HttpVerbUriRegexPropertyValue> aliasRuleMapForClient = dynamicConfigMap
                                    .get(prefix).get(clientName);
                            if (aliasRuleMapForClient == null) {
                                // no map exists so far, create one
                                aliasRuleMapForClient = new ConcurrentHashMap<String, HttpVerbUriRegexPropertyValue>();
                                Map<String, HttpVerbUriRegexPropertyValue> prev = dynamicConfigMap.get(prefix).putIfAbsent(clientName,
                                        aliasRuleMapForClient);
                                if (prev != null) {
                                	aliasRuleMapForClient = prev;
                                }
                            }

                            String alias = name.substring(configPrefix.length());
                            if (alias != null) {
                                alias = alias.trim();
                                switch (eventType) {
                                case CLEAR:
                                    aliasRuleMapForClient.remove(alias);
                                    break;
                                case ADD:
                                case SET:
                                    if (value != null) {
                                        aliasRuleMapForClient.put(alias,
                                                HttpVerbUriRegexPropertyValue
                                                        .getVerbUriRegex(value
                                                                .toString()));
                                    }
                                    break;
                                }
                            }
                        }
                    }
                }
            } catch (Throwable t) {
                LOG.warn("Unexpected error when checking for dynamic Rest "
                        + "Client property updates", t);
            }
        }
    }
    
    static void setProperty(Properties props, String restClientName, String key, String value){
        props.setProperty( getInstancePropName(restClientName, key), value);
    }

    public static String getInstancePropName(String restClientName,
            IClientConfigKey configKey) {
        return getInstancePropName(restClientName, configKey.key());
    }

    public static String getInstancePropName(String restClientName, String key) {
        return restClientName + "." + PROPERTY_NAMESPACE + "."
                + key;
    }
    
    public static IClientConfig getNamedConfig(String name) {
        NiwsClientConfig config = namedConfig.get(name);
        if (config != null) {
            return config;
        } else {
            config = getConfigWithDefaultProperties();
            config.loadProperties(name);
            NiwsClientConfig old = namedConfig.put(name, config);
            if (old != null) {
                config = old;
            }
            return config;
        }
    }
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_f1160f3_1e333eb/rev_f1160f3-1e333eb/ribbon-core/src/main/java/com/netflix/niws/client/ClientFactory.java;null
/home/taes/taes/projects/ribbon/revisions/rev_f1160f3_1e333eb/rev_f1160f3-1e333eb/ribbon-core/src/main/java/com/netflix/niws/client/PrimeConnections.java;null
/home/taes/taes/projects/ribbon/revisions/rev_f1160f3_1e333eb/rev_f1160f3-1e333eb/ribbon-core/src/main/java/com/netflix/niws/client/DynamicServerListLoadBalancer.java;<<<<<<< MINE
import com.netflix.client.config.DefaultClientConfigImpl;
||||||| BASE
=======
import com.google.common.util.concurrent.ThreadFactoryBuilder;
import com.netflix.loadbalancer.BaseLoadBalancer;
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_f1160f3_1e333eb/rev_f1160f3-1e333eb/ribbon-core/src/main/java/com/netflix/niws/client/DynamicServerListLoadBalancer.java;<<<<<<< MINE
import com.google.common.util.concurrent.ThreadFactoryBuilder; 
||||||| BASE
import com.netflix.niws.client.NiwsClientConfig.NiwsClientConfigKey;
import com.google.common.util.concurrent.ThreadFactoryBuilder; 
=======
>>>>>>> YOURS
/home/taes/taes/projects/liferay-plugins/revisions/rev_fc5104c_379a202/rev_fc5104c-379a202/portlets/opensocial-portlet/docroot/WEB-INF/src/com/liferay/opensocial/gadget/portlet/BaseGadgetPortlet.java;null
/home/taes/taes/projects/liferay-plugins/revisions/rev_fc5104c_379a202/rev_fc5104c-379a202/portlets/opensocial-portlet/docroot/WEB-INF/src/com/liferay/opensocial/gadget/portlet/BaseGadgetPortlet.java;null
/home/taes/taes/projects/liferay-plugins/revisions/rev_fc5104c_379a202/rev_fc5104c-379a202/portlets/opensocial-portlet/docroot/WEB-INF/src/com/liferay/opensocial/gadget/portlet/BaseGadgetPortlet.java;null
/home/taes/taes/projects/liferay-plugins/revisions/rev_fc5104c_379a202/rev_fc5104c-379a202/portlets/opensocial-portlet/docroot/WEB-INF/src/com/liferay/opensocial/gadget/portlet/BaseGadgetPortlet.java;null
/home/taes/taes/projects/liferay-plugins/revisions/rev_fc5104c_379a202/rev_fc5104c-379a202/portlets/opensocial-portlet/docroot/WEB-INF/src/com/liferay/opensocial/gadget/portlet/BaseGadgetPortlet.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_2eb053b_b8dae6e/rev_2eb053b-b8dae6e/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_2eb053b_b8dae6e/rev_2eb053b-b8dae6e/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/liferay-plugins/revisions/rev_c6d377e_010bf7b/rev_c6d377e-010bf7b/portlets/opensocial-portlet/docroot/WEB-INF/src/com/liferay/opensocial/editor/portlet/EditorPortlet.java;null
/home/taes/taes/projects/liferay-plugins/revisions/rev_c6d377e_010bf7b/rev_c6d377e-010bf7b/portlets/opensocial-portlet/docroot/WEB-INF/src/com/liferay/opensocial/editor/portlet/EditorPortlet.java;null
/home/taes/taes/projects/atlas/revisions/rev_9fc1590_1fa2d2a/rev_9fc1590-1fa2d2a/atlas-gradle-plugin/dexpatch/src/main/java/com/taobao/android/TPatchTool.java;<<<<<<< MINE
import com.taobao.android.tpatch.manifest.AndroidManifestDiffFactory;
||||||| BASE
=======
import com.taobao.android.task.ExecutorServicesHelper;
>>>>>>> YOURS
/home/taes/taes/projects/jodd/revisions/rev_7d88093_1943bcf/rev_7d88093-1943bcf/jodd-madvoc/src/main/java/jodd/madvoc/config/RouteMadvocConfigurator.java;null
/home/taes/taes/projects/jodd/revisions/rev_7d88093_1943bcf/rev_7d88093-1943bcf/jodd-madvoc/src/main/java/jodd/madvoc/config/RouteMadvocConfigurator.java;null
/home/taes/taes/projects/jodd/revisions/rev_7d88093_1943bcf/rev_7d88093-1943bcf/jodd-madvoc/src/main/java/jodd/madvoc/config/RouteMadvocConfigurator.java;null
/home/taes/taes/projects/jodd/revisions/rev_7d88093_1943bcf/rev_7d88093-1943bcf/jodd-madvoc/src/testInt/java/jodd/madvoc/MyWebApplication2.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_dd64dd0_8ff804b/rev_dd64dd0-8ff804b/src/java/com/twitter/elephantbird/pig/load/HBaseSlice.java;<<<<<<< MINE
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with this
 * work for additional information regarding copyright ownership. The ASF
 * licenses this file to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * 
 * http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.math.BigDecimal;
import java.math.BigInteger;
import java.util.ArrayList;
import java.util.Map;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.UnknownScannerException;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.filter.BinaryComparator;
import org.apache.hadoop.hbase.filter.CompareFilter;
import org.apache.hadoop.hbase.filter.FilterList;
import org.apache.hadoop.hbase.filter.RowFilter;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.util.StringUtils;
import org.apache.pig.Slice;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;

import com.google.common.collect.Maps;
import com.twitter.elephantbird.pig.util.PigCounterHelper;

/**
 * HBase Slice to load a portion of range of a table. The key range will be
 * [start, end) Modeled from org.apache.hadoop.hbase.mapred.TableSplit.
 */
public class HBaseSlice implements Slice {

  /** A Generated Serial Version UID **/
  private static final long serialVersionUID = 9035916017187148965L;
  private static final Log LOG = LogFactory.getLog(HBaseSlice.class);
  private transient PigCounterHelper counterHelper_;

  // assigned during construction
  /** Table Name **/
  private final byte[] tableName_;
  /** Table Start Row **/
  private final byte[] startRow_;
  /** Table End Row **/
  private final byte[] endRow_;
  /** Table Region Location **/
  private final String regionLocation_;
  /** Input Columns **/
  private final byte[][] inputColumns_;
  /** Whether the row should be loaded **/
  private final boolean loadRowKey_;

  /** BigInteger representations of row range */
  private final BigInteger bigStart_;
  private final BigInteger bigEnd_;
  private final BigDecimal bigRange_;


  private Map<CompareFilter.CompareOp, String> innerFilters_ = Maps.newHashMap();
  private long limit_ = -1;


  // created as part of init
  /** The connection to the table in Hbase **/
  private transient HTable m_table;
  /** The scanner over the table **/
  private transient ResultScanner m_scanner;
  private transient long seenRows_ = 0;

  private transient ArrayList<Object> mProtoTuple;

  /**
   * Record the last processed row, so that we can restart the scanner when an
   * exception happened during scanning a table
   */
  private transient byte[] m_lastRow_;

  /**
   * Constructor
   * 
   * @param tableName
   *            table name
   * @param startRow
   *            start now, inclusive
   * @param endRow
   *            end row, exclusive
   * @param inputColumns
   *            input columns
   * @param location
   *            region location
   */
  public HBaseSlice(byte[] tableName, byte[] startRow, byte[] endRow,
      byte[][] inputColumns, boolean loadRowKey, final String location) {
    tableName_ = tableName;
    startRow_ = startRow;
    endRow_ = endRow;
    inputColumns_ = inputColumns;
    regionLocation_ = location;
    loadRowKey_ = loadRowKey;

    // We have to deal with different byte lengths of keys producing very different
    // BigIntegers (bigendianness is great this way). The code is mostly cribbed
    // from HBase's Bytes class.
    byte [] startPadded;
    byte [] endPadded;
    if (startRow.length < endRow.length) {
      startPadded = Bytes.padTail(startRow, endRow.length - startRow.length);
      endPadded = endRow;
    } else if (endRow.length < startRow.length) {
      startPadded = startRow;
      endPadded = Bytes.padTail(endRow, startRow.length - endRow.length);
    } else {
      startPadded = startRow;
      endPadded = endRow;
    }
    byte [] prependHeader = {1, 0};
    bigStart_ = new BigInteger(Bytes.add(prependHeader, startPadded));
    bigEnd_ = new BigInteger(Bytes.add(prependHeader, endPadded));
    bigRange_ = new BigDecimal(bigEnd_.subtract(bigStart_));
  }

  public void addFilter(CompareFilter.CompareOp compareOp, String filterValue) {
    innerFilters_.put(compareOp, filterValue);
  }

  /** @return table name */
  public byte[] getTableName() {
    return this.tableName_;
  }

  /** @return starting row key */
  public byte[] getStartRow() {
    return this.startRow_;
  }

  /** @return end row key */
  public byte[] getEndRow() {
    return this.endRow_;
  }

  /** @return input columns */
  public byte[][] getInputColumns() {
    return this.inputColumns_;
  }

  /** @return the region's hostname */
  public String getRegionLocation() {
    return this.regionLocation_;
  }

  @Override
  public long getStart() {
    // Not clear how to obtain this in a table...
    return 0;
  }

  @Override
  public long getLength() {
    // Not clear how to obtain this in a table...
    // it seems to be used only for sorting splits
    return 0;
  }

  @Override
  public String[] getLocations() {
    return new String[] { regionLocation_ };
  }

  @Override
  public long getPos() throws IOException {
    // This should be the ordinal tuple in the range;
    // not clear how to calculate...
    return 0;
  }

  @Override
  public float getProgress() throws IOException {

    // No way to know max.. just return 0. Sorry, reporting on the last slice is janky.
    // So is reporting on the first slice, by the way -- it will start out too high, possibly at 100%.
    if (endRow_.length==0) return 0;
    byte[] lastPadded = m_lastRow_;
    if (m_lastRow_.length < endRow_.length) {
      lastPadded = Bytes.padTail(m_lastRow_, endRow_.length - m_lastRow_.length);
    }
    if (m_lastRow_.length < startRow_.length) {
      lastPadded = Bytes.padTail(m_lastRow_, startRow_.length - m_lastRow_.length);
    }
    byte [] prependHeader = {1, 0};
    BigInteger bigLastRow = new BigInteger(Bytes.add(prependHeader, lastPadded));
    BigDecimal processed = new BigDecimal(bigLastRow.subtract(bigStart_));
    try {
      BigDecimal progress = processed.setScale(3).divide(bigRange_, BigDecimal.ROUND_HALF_DOWN);
      return progress.floatValue();
    } catch (java.lang.ArithmeticException e) {
      return 0;
    }
  }

  @Override
  public void init(DataStorage store) throws IOException {
    HBaseConfiguration conf = new HBaseConfiguration();
    // connect to the given table
    m_table = new HTable(conf, tableName_);
    // init the scanner
    initScanner();
  }

  /**
   * Init the table scanner
   * 
   * @throws IOException
   */
  private void initScanner() throws IOException {
    restart(startRow_);
    m_lastRow_ = startRow_;
  }

  /**
   * Restart scanning from survivable exceptions by creating a new scanner.
   * 
   * @param startRow
   *            the start row
   * @throws IOException
   */
  private void restart(byte[] startRow) throws IOException {
    Scan scan;
    if ((endRow_ != null) && (endRow_.length > 0)) {
      scan = new Scan(startRow, endRow_);
    } else {
      scan = new Scan(startRow);
    }

    // Set filters, if any.
    FilterList scanFilter = null;
    if (!innerFilters_.isEmpty()) {
      scanFilter = new FilterList();
      for (Map.Entry<CompareFilter.CompareOp, String>entry  : innerFilters_.entrySet()) {
        scanFilter.addFilter(new RowFilter(entry.getKey(), new BinaryComparator(Bytes.toBytesBinary(entry.getValue()) )));
      }
      scan.setFilter(scanFilter);
    }

    scan.addColumns(inputColumns_);
    this.m_scanner = this.m_table.getScanner(scan);
  }

  @Override
  public boolean next(Tuple value) throws IOException {
    Result result;
    try {
      result = m_scanner.next();
    } catch (UnknownScannerException e) {
      LOG.info("recovered from " + StringUtils.stringifyException(e));
      restart(m_lastRow_);
      if (m_lastRow_ != startRow_) {
        m_scanner.next(); // skip presumed already mapped row
      }
      result = this.m_scanner.next();
    }
    boolean hasMore = result != null && result.size() > 0 && (limit_ < 0 || limit_ > seenRows_);
    if (hasMore) {
      if (counterHelper_ == null) counterHelper_ = new PigCounterHelper();
      counterHelper_.incrCounter(HBaseSlice.class.getName(), Bytes.toString(tableName_) + " rows read", 1);
      m_lastRow_ = result.getRow();
      convertResultToTuple(result, value);
      seenRows_ += 1;
    }
    return hasMore;
  }

  /**
   * Convert a row result to a tuple
   * 
   * @param result
   *            row result
   * @param tuple
   *            tuple
   */
  private void convertResultToTuple(Result result, Tuple tuple) {
    if (mProtoTuple == null)
      mProtoTuple = new ArrayList<Object>(inputColumns_.length + (loadRowKey_ ? 1 : 0));

    if (loadRowKey_) {
      mProtoTuple.add(new DataByteArray(result.getRow()));
    }

    for (byte[] column : inputColumns_) {
      byte[] value = result.getValue(column);
      if (value == null) {
        mProtoTuple.add(null);
      } else {
        mProtoTuple.add(new DataByteArray(value));
      }
    }

    Tuple newT = TupleFactory.getInstance().newTuple(mProtoTuple);
    mProtoTuple.clear();
    tuple.reference(newT);
  }

  @Override
  public void close() throws IOException {
    if (m_scanner != null) {
      m_scanner.close();
      m_scanner = null;
    }
  }

  @Override
  public String toString() {
    return regionLocation_ + ":" + Bytes.toString(startRow_) + ","
    + Bytes.toString(endRow_);
  }

  public void setLimit(String limit) {
    LOG.info("Setting Slice limit to "+Long.valueOf(limit));
    limit_ = Long.valueOf(limit);
  }

}||||||| BASE
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with this
 * work for additional information regarding copyright ownership. The ASF
 * licenses this file to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * 
 * http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.math.BigDecimal;
import java.math.BigInteger;
import java.util.ArrayList;
import java.util.Map;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.UnknownScannerException;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.filter.BinaryComparator;
import org.apache.hadoop.hbase.filter.CompareFilter;
import org.apache.hadoop.hbase.filter.FilterList;
import org.apache.hadoop.hbase.filter.RowFilter;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.util.StringUtils;
import org.apache.pig.Slice;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;

import com.google.common.collect.Maps;
import com.twitter.elephantbird.pig.util.PigCounterHelper;

/**
 * HBase Slice to load a portion of range of a table. The key range will be
 * [start, end) Modeled from org.apache.hadoop.hbase.mapred.TableSplit.
 */
public class HBaseSlice implements Slice {

  /** A Generated Serial Version UID **/
  private static final long serialVersionUID = 9035916017187148965L;
  private static final Log LOG = LogFactory.getLog(HBaseSlice.class);
  private transient PigCounterHelper counterHelper_;

  // assigned during construction
  /** Table Name **/
  private final byte[] tableName_;
  /** Table Start Row **/
  private final byte[] startRow_;
  /** Table End Row **/
  private final byte[] endRow_;
  /** Table Region Location **/
  private final String regionLocation_;
  /** Input Columns **/
  private final byte[][] inputColumns_;
  /** Whether the row should be loaded **/
  private final boolean loadRowKey_;

  /** BigInteger representations of row range */
  private final BigInteger bigStart_;
  private final BigInteger bigEnd_;
  private final BigDecimal bigRange_;


  private Map<CompareFilter.CompareOp, String> innerFilters_ = Maps.newHashMap();
  private long limit_ = -1;


  // created as part of init
  /** The connection to the table in Hbase **/
  private transient HTable m_table;
  /** The scanner over the table **/
  private transient ResultScanner m_scanner;
  private transient long seenRows_ = 0;

  private transient ArrayList<Object> mProtoTuple;

  /**
   * Record the last processed row, so that we can restart the scanner when an
   * exception happened during scanning a table
   */
  private transient byte[] m_lastRow_;

  /**
   * Constructor
   * 
   * @param tableName
   *            table name
   * @param startRow
   *            start now, inclusive
   * @param endRow
   *            end row, exclusive
   * @param inputColumns
   *            input columns
   * @param location
   *            region location
   */
  public HBaseSlice(byte[] tableName, byte[] startRow, byte[] endRow,
      byte[][] inputColumns, boolean loadRowKey, final String location) {
    tableName_ = tableName;
    startRow_ = startRow;
    endRow_ = endRow;
    inputColumns_ = inputColumns;
    regionLocation_ = location;
    loadRowKey_ = loadRowKey;
    bigStart_ = bytesToPositiveBigInt(startRow);
    bigEnd_ = bytesToPositiveBigInt(endRow);
    bigRange_ = new BigDecimal(bigEnd_.subtract(bigStart_));
  }

  public void addFilter(CompareFilter.CompareOp compareOp, String filterValue) {
    innerFilters_.put(compareOp, filterValue);
  }

  /** @return table name */
  public byte[] getTableName() {
    return this.tableName_;
  }

  /** @return starting row key */
  public byte[] getStartRow() {
    return this.startRow_;
  }

  /** @return end row key */
  public byte[] getEndRow() {
    return this.endRow_;
  }

  /** @return input columns */
  public byte[][] getInputColumns() {
    return this.inputColumns_;
  }

  /** @return the region's hostname */
  public String getRegionLocation() {
    return this.regionLocation_;
  }

  @Override
  public long getStart() {
    // Not clear how to obtain this in a table...
    return 0;
  }

  @Override
  public long getLength() {
    // Not clear how to obtain this in a table...
    // it seems to be used only for sorting splits
    return 0;
  }

  @Override
  public String[] getLocations() {
    return new String[] { regionLocation_ };
  }

  @Override
  public long getPos() throws IOException {
    // This should be the ordinal tuple in the range;
    // not clear how to calculate...
    return 0;
  }

  @Override
  public float getProgress() throws IOException {

    // No way to know max.. just return 0. Sorry, reporting on the last slice is janky.
    // So is reporting on the first slice, by the way -- it will start out too high, possibly at 100%.
    if (endRow_.length==0) return 0;

    BigInteger bigLastRow = bytesToPositiveBigInt(m_lastRow_);
    BigDecimal processed = new BigDecimal(bigLastRow.subtract(bigStart_));

    try {
      BigDecimal progress = processed.setScale(3).divide(bigRange_, BigDecimal.ROUND_HALF_DOWN);
      return progress.floatValue();
    } catch (java.lang.ArithmeticException e) {
      return 0;
    }
  }

  @Override
  public void init(DataStorage store) throws IOException {
    HBaseConfiguration conf = new HBaseConfiguration();
    // connect to the given table
    m_table = new HTable(conf, tableName_);
    // init the scanner
    initScanner();
  }

  /**
   * Init the table scanner
   * 
   * @throws IOException
   */
  private void initScanner() throws IOException {
    restart(startRow_);
    m_lastRow_ = startRow_;
  }

  /**
   * Restart scanning from survivable exceptions by creating a new scanner.
   * 
   * @param startRow
   *            the start row
   * @throws IOException
   */
  private void restart(byte[] startRow) throws IOException {
    Scan scan;
    if ((endRow_ != null) && (endRow_.length > 0)) {
      scan = new Scan(startRow, endRow_);
    } else {
      scan = new Scan(startRow);
    }

    // Set filters, if any.
    FilterList scanFilter = null;
    if (!innerFilters_.isEmpty()) {
      scanFilter = new FilterList();
      for (Map.Entry<CompareFilter.CompareOp, String>entry  : innerFilters_.entrySet()) {
        scanFilter.addFilter(new RowFilter(entry.getKey(), new BinaryComparator(Bytes.toBytesBinary(entry.getValue()) )));
      }
      scan.setFilter(scanFilter);
    }

    scan.addColumns(inputColumns_);
    this.m_scanner = this.m_table.getScanner(scan);
  }

  @Override
  public boolean next(Tuple value) throws IOException {
    Result result;
    try {
      result = m_scanner.next();
    } catch (UnknownScannerException e) {
      LOG.info("recovered from " + StringUtils.stringifyException(e));
      restart(m_lastRow_);
      if (m_lastRow_ != startRow_) {
        m_scanner.next(); // skip presumed already mapped row
      }
      result = this.m_scanner.next();
    }
    boolean hasMore = result != null && result.size() > 0 && (limit_ < 0 || limit_ > seenRows_);
    if (hasMore) {
      if (counterHelper_ == null) counterHelper_ = new PigCounterHelper();
      counterHelper_.incrCounter(HBaseSlice.class.getName(), Bytes.toString(tableName_) + " rows read", 1);
      m_lastRow_ = result.getRow();
      convertResultToTuple(result, value);
      seenRows_ += 1;
    }
    return hasMore;
  }

  /**
   * Convert a row result to a tuple
   * 
   * @param result
   *            row result
   * @param tuple
   *            tuple
   */
  private void convertResultToTuple(Result result, Tuple tuple) {
    if (mProtoTuple == null)
      mProtoTuple = new ArrayList<Object>(inputColumns_.length + (loadRowKey_ ? 1 : 0));

    if (loadRowKey_) {
      mProtoTuple.add(new DataByteArray(result.getRow()));
    }

    for (byte[] column : inputColumns_) {
      byte[] value = result.getValue(column);
      if (value == null) {
        mProtoTuple.add(null);
      } else {
        mProtoTuple.add(new DataByteArray(value));
      }
    }

    Tuple newT = TupleFactory.getInstance().newTuple(mProtoTuple);
    mProtoTuple.clear();
    tuple.reference(newT);
  }

  @Override
  public void close() throws IOException {
    if (m_scanner != null) {
      m_scanner.close();
      m_scanner = null;
    }
  }

  @Override
  public String toString() {
    return regionLocation_ + ":" + Bytes.toString(startRow_) + ","
    + Bytes.toString(endRow_);
  }

  private BigInteger bytesToPositiveBigInt(byte[] bytes) {
    return (bytes.length == 0) ? new BigInteger(new byte[] {0}) :
      new BigInteger(bytes).shiftRight(1).clearBit(bytes.length-1);
  }

  public void setLimit(String limit) {
    LOG.info("Setting Slice limit to "+Long.valueOf(limit));
    limit_ = Long.valueOf(limit);
  }

}=======
>>>>>>> YOURS
/home/taes/taes/projects/atlas/revisions/rev_c2e9822_9b24b0d/rev_c2e9822-9b24b0d/atlas-core/src/main/java/android/taobao/atlas/startup/AtlasBridgeApplication.java;null
/home/taes/taes/projects/jodd/revisions/rev_b308375_38b60c8/rev_b308375-38b60c8/jodd-http/src/main/java/jodd/http/HttpBrowser.java;null
/home/taes/taes/projects/jodd/revisions/rev_b308375_38b60c8/rev_b308375-38b60c8/jodd-http/src/main/java/jodd/http/HttpBrowser.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_c01b27b_cebd3d0/rev_c01b27b-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;<<<<<<< MINE
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
||||||| BASE
import com.google.protobuf.Message;
import com.hadoop.compression.lzo.LzopCodec;
import com.twitter.elephantbird.mapreduce.io.ProtobufWritable;
import com.twitter.elephantbird.util.TypeRef;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
=======
import com.google.protobuf.Message;
import com.twitter.elephantbird.mapreduce.io.ProtobufConverter;
import com.twitter.elephantbird.mapreduce.io.ProtobufWritable;
import com.twitter.elephantbird.util.TypeRef;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_c01b27b_cebd3d0/rev_c01b27b-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineRecordWriter.java;<<<<<<< MINE
package com.twitter.elephantbird.mapreduce.output;

import java.io.DataOutputStream;
import java.io.IOException;

import com.twitter.elephantbird.mapreduce.io.ThriftB64LineWritable;
import com.twitter.elephantbird.util.TypeRef;

import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.thrift.TBase;

/**
 * A RecordWriter-derived class for use with the LzoThriftB64LineOutputFormat.
 * Writes data as base64 encoded serialized thrift objects, one per line.
 */

public class LzoThriftB64LineRecordWriter<T extends TBase>
    extends RecordWriter<NullWritable, ThriftB64LineWritable<T>> {
  private static final Logger LOG = LogManager.getLogger(LzoThriftB64LineRecordWriter.class);

  protected final TypeRef typeRef_;
  protected final DataOutputStream out_;

  public LzoThriftB64LineRecordWriter(TypeRef<T> typeRef, DataOutputStream out) {
    typeRef_ = typeRef;
    out_ = out;
  }

  public void write(NullWritable nullWritable, ThriftB64LineWritable<T> thriftWritable)
      throws IOException, InterruptedException {
    thriftWritable.write(out_);
  }

  public void close(TaskAttemptContext taskAttemptContext)
      throws IOException, InterruptedException {
    out_.close();
  }
}||||||| BASE
package com.twitter.elephantbird.mapreduce.output;

import java.io.DataOutputStream;
import java.io.IOException;

import com.twitter.elephantbird.util.TypeRef;
import com.twitter.elephantbird.mapreduce.io.ThriftB64LineWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.thrift.TBase;

/**
 * A RecordWriter-derived class for use with the LzoThriftB64LineOutputFormat.
 * Writes data as base64 encoded serialized thrift objects, one per line.
 */

public class LzoThriftB64LineRecordWriter<T extends TBase>
    extends RecordWriter<NullWritable, ThriftB64LineWritable<T>> {
  private static final Logger LOG = LogManager.getLogger(LzoThriftB64LineRecordWriter.class);

  protected final TypeRef typeRef_;
  protected final DataOutputStream out_;

  public LzoThriftB64LineRecordWriter(TypeRef<T> typeRef, DataOutputStream out) {
    typeRef_ = typeRef;
    out_ = out;
  }

  public void write(NullWritable nullWritable, ThriftB64LineWritable<T> thriftWritable)
      throws IOException, InterruptedException {
    thriftWritable.write(out_);
  }

  public void close(TaskAttemptContext taskAttemptContext)
      throws IOException, InterruptedException {
    out_.close();
  }
}=======
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_c01b27b_cebd3d0/rev_c01b27b-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;<<<<<<< MINE
import com.hadoop.compression.lzo.LzopCodec;
import com.twitter.elephantbird.mapreduce.io.ThriftB64LineWritable;
||||||| BASE
import com.hadoop.compression.lzo.LzopCodec;
=======
import com.twitter.elephantbird.mapreduce.io.ThriftConverter;
import com.twitter.elephantbird.mapreduce.io.ThriftWritable;
import com.twitter.elephantbird.util.ThriftUtils;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_c01b27b_cebd3d0/rev_c01b27b-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_c01b27b_cebd3d0/rev_c01b27b-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;<<<<<<< MINE
import com.twitter.elephantbird.mapreduce.input.LzoInputFormat;
import com.twitter.elephantbird.mapreduce.io.ThriftB64LineWritable;
||||||| BASE
import com.twitter.elephantbird.mapreduce.input.LzoInputFormat;
=======
import com.twitter.elephantbird.mapreduce.io.ThriftWritable;
import com.twitter.elephantbird.util.ThriftUtils;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_c01b27b_cebd3d0/rev_c01b27b-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;<<<<<<< MINE

||||||| BASE
import com.twitter.elephantbird.mapreduce.io.ThriftB64LineWritable;
=======

import org.apache.hadoop.conf.Configuration;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_c01b27b_cebd3d0/rev_c01b27b-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineRecordReader.java;<<<<<<< MINE
import java.io.IOException;
import java.io.InputStream;

import com.twitter.elephantbird.mapreduce.input.LzoRecordReader;
import com.twitter.elephantbird.mapreduce.io.ThriftB64LineWritable;
import com.twitter.elephantbird.util.TypeRef;

import org.apache.commons.codec.binary.Base64;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.util.LineReader;
||||||| BASE
import java.io.IOException;
import java.io.InputStream;

import com.twitter.elephantbird.mapreduce.input.LzoRecordReader;
import com.twitter.elephantbird.util.TypeRef;
import com.twitter.elephantbird.mapreduce.io.ThriftB64LineWritable;
import org.apache.commons.codec.binary.Base64;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.util.LineReader;
=======
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_c01b27b_cebd3d0/rev_c01b27b-cebd3d0/src/java/com/twitter/elephantbird/pig/store/LzoProtobufB64LinePigStorage.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.store;

import java.io.IOException;
import java.util.List;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.data.Tuple;

import com.google.protobuf.Message;
import com.google.protobuf.Descriptors.FieldDescriptor;
import com.google.protobuf.Message.Builder;
import com.twitter.elephantbird.pig.util.PigToProtobuf;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * Serializes Pig Tuples into Base-64 encoded, line-delimited protocol buffers.
 * The fields in the pig tuple must correspond exactly to the fields in the protobuf, as
 * no name-matching is performed (names of the tuple fields are not currently accessible to
 * a StoreFunc. It will be in 0.7, so something more flexible will be possible)
 *
 * @param <M> Protocol Buffer Message class being serialized
 */
public abstract class LzoProtobufB64LinePigStorage<M extends Message> extends LzoBaseStoreFunc {

  private TypeRef<M> typeRef_;
  private Base64 base64_ = new Base64();
  private final PigToProtobuf pigToProto_ = new PigToProtobuf();

  protected void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
  }

  public void putNext(Tuple f) throws IOException {
    if (f == null) return;
	Builder builder = Protobufs.getMessageBuilder(typeRef_.getRawClass());
    os_.write(base64_.encode(pigToProto_.tupleToMessage(builder, f).toByteArray()));
    os_.write("\n".getBytes("UTF-8"));
  }

}=======
package com.twitter.elephantbird.pig.store;

import java.io.IOException;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.data.Tuple;

import com.google.protobuf.Message;
import com.google.protobuf.Message.Builder;
import com.twitter.elephantbird.pig.util.PigToProtobuf;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * Serializes Pig Tuples into Base-64 encoded, line-delimited protocol buffers.
 * The fields in the pig tuple must correspond exactly to the fields in the protobuf, as
 * no name-matching is performed (names of the tuple fields are not currently accessible to
 * a StoreFunc. It will be in 0.7, so something more flexible will be possible)
 *
 * @param <M> Protocol Buffer Message class being serialized
 */
public abstract class LzoProtobufB64LinePigStorage<M extends Message> extends LzoBaseStoreFunc {

  private TypeRef<M> typeRef_;
  private Base64 base64_ = new Base64();
  private final PigToProtobuf pigToProto_ = new PigToProtobuf();

  protected void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
  }

  public void putNext(Tuple f) throws IOException {
    if (f == null) return;
	Builder builder = Protobufs.getMessageBuilder(typeRef_.getRawClass());
    os_.write(base64_.encode(pigToProto_.tupleToMessage(builder, f).toByteArray()));
    os_.write("\n".getBytes("UTF-8"));
  }

}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_c01b27b_cebd3d0/rev_c01b27b-cebd3d0/src/java/com/twitter/elephantbird/pig/util/ProtobufToPig.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.util;

import java.util.List;
import java.util.Map;

import com.google.common.collect.Lists;
import com.google.common.collect.Maps;
import com.google.protobuf.Descriptors.Descriptor;
import com.google.protobuf.Descriptors.EnumValueDescriptor;
import com.google.protobuf.Descriptors.FieldDescriptor;
import com.google.protobuf.ByteString;
import com.google.protobuf.Message;
import com.twitter.data.proto.Misc.CountedMap;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * A class for turning codegen'd protos into Pig Tuples and Schemas
 * for custom Pig LoadFuncs.
 * @author Kevin Weil
 */
public class ProtobufToPig {
  private static final Logger LOG = LoggerFactory.getLogger(ProtobufToPig.class);

  private static final TupleFactory tupleFactory_ = TupleFactory.getInstance();
  private static BagFactory bagFactory_ = BagFactory.getInstance();

  public enum CoercionLevel { kNoCoercion, kAllowCoercionToPigMaps }

  private final CoercionLevel coercionLevel_;

  public ProtobufToPig() {
    this(CoercionLevel.kAllowCoercionToPigMaps);
  }

  public ProtobufToPig(CoercionLevel coercionLevel) {
    coercionLevel_ = coercionLevel;
  }
  /**
   * Turn a generic message into a Tuple.  Individual fields that are enums
   * are converted into their string equivalents.  Fields that are not filled
   * out in the protobuf are set to null, unless there is a default field value in
   * which case that is used instead.
   * @param msg the protobuf message
   * @return a pig tuple representing the message.
   */
  public Tuple toTuple(Message msg) {
    if (msg == null) {
      // Pig tuples deal gracefully with nulls.
      // Also, we can be called with null here in recursive calls.
      return null;
    }

    Descriptor msgDescriptor = msg.getDescriptorForType();
    Tuple tuple = tupleFactory_.newTuple(msgDescriptor.getFields().size());
    int curField = 0;
    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        // Get the set value, or the default value, or null.
        Object fieldValue = getFieldValue(msg, fieldDescriptor);

        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          tuple.set(curField++, messageToTuple(fieldDescriptor, fieldValue));
        } else {
          tuple.set(curField++, singleFieldToTuple(fieldDescriptor, fieldValue));
        }
      }
    } catch (ExecException e) {
      LOG.warn("Could not convert msg " + msg + " to tuple", e);
    }

    return tuple;
  }

  /**
   * Translate a nested message to a tuple.  If the field is repeated, it walks the list and adds each to a bag.
   * Otherwise, it just adds the given one.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object representing fieldValue in Pig -- either a bag or a tuple.
   */
  @SuppressWarnings("unchecked")
  protected Object messageToTuple(FieldDescriptor fieldDescriptor, Object fieldValue) {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToTuple called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      // The protobuf contract is that if the field is repeated, then the object returned is actually a List
      // of the underlying datatype, which in this case is a nested message.
      List<Message> messageList = (List<Message>) (fieldValue != null ? fieldValue : Lists.newArrayList());

      // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
      // we can force the type into a pig map type.
      if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
          fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName())) {
        Map<Object, Long> map = Maps.newHashMap();
        for (Message m : messageList) {
          CountedMap cm = (CountedMap) m;
          final Long curCount = map.get(cm.getKey());
          map.put(cm.getKey(), (curCount == null ? 0L : curCount) + cm.getValue());
        }
        return map;
      } else {
        DataBag bag = bagFactory_.newDefaultBag();
        for (Message m : messageList) {
          bag.add(new ProtobufTuple(m));
        }
        return bag;
      }
    } else {
      return new ProtobufTuple((Message)fieldValue);
    }
  }

  /**
   * Translate a single field to a tuple.  If the field is repeated, it walks the list and adds each to a bag.
   * Otherwise, it just adds the given one.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object representing fieldValue in Pig -- either a bag or a single field.
   * @throws ExecException if Pig decides to.  Shouldn't happen because we won't walk off the end of a tuple's field set.
   */
  @SuppressWarnings("unchecked")
  protected Object singleFieldToTuple(FieldDescriptor fieldDescriptor, Object fieldValue) throws ExecException {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "messageToFieldSchema called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      // The protobuf contract is that if the field is repeated, then the object returned is actually a List
      // of the underlying datatype, which in this case is a "primitive" like int, float, String, etc.
      // We have to make a single-item tuple out of it to put it in the bag.
      DataBag bag = bagFactory_.newDefaultBag();
      List<Object> fieldValueList = (List<Object>) (fieldValue != null ? fieldValue : Lists.newArrayList());
      for (Object singleFieldValue : fieldValueList) {
        Object nonEnumFieldValue = coerceToPigTypes(fieldDescriptor, singleFieldValue);
        Tuple innerTuple = tupleFactory_.newTuple(1);
        innerTuple.set(0, nonEnumFieldValue);
        bag.add(innerTuple);
      }
      return bag;
    } else {
      return coerceToPigTypes(fieldDescriptor, fieldValue);
    }
  }

  /**
   * If the given field value is an enum, translate it to the enum's name as a string, since Pig cannot handle enums.
   * Also, if the given field value is a bool, translate it to 0 or 1 to avoid Pig bools, which can be sketchy.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object, unless it was from an enum field, in which case we return the name of the enum field.
   */
  private Object coerceToPigTypes(FieldDescriptor fieldDescriptor, Object fieldValue) {
    if (fieldDescriptor.getType() == FieldDescriptor.Type.ENUM && fieldValue != null) {
      EnumValueDescriptor enumValueDescriptor = (EnumValueDescriptor)fieldValue;
      return enumValueDescriptor.getName();
    } else if (fieldDescriptor.getType() == FieldDescriptor.Type.BOOL && fieldValue != null) {
      Boolean boolValue = (Boolean)fieldValue;
      return new Integer(boolValue ? 1 : 0);
    } else if (fieldDescriptor.getType() == FieldDescriptor.Type.BYTES && fieldValue != null) {
      ByteString bsValue = (ByteString)fieldValue;
      return new DataByteArray(bsValue.toByteArray());
    }
    return fieldValue;
  }

  /**
   * A utility function for getting the value of a field in a protobuf message.  It first tries the
   * literal set value in the protobuf's field list.  If the value isn't set, and the field has a default
   * value, it uses that.  Otherwise, it returns null.
   * @param msg the protobuf message
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the value of the field, or null if none can be assigned.
   */
  protected Object getFieldValue(Message msg, FieldDescriptor fieldDescriptor) {
    Object o = null;
    Map<FieldDescriptor, Object> setFields = msg.getAllFields();
    if (setFields.containsKey(fieldDescriptor)) {
      o = setFields.get(fieldDescriptor);
    } else if (fieldDescriptor.hasDefaultValue()) {
      o = fieldDescriptor.getDefaultValue();
    }

    return o;
  }

  /**
   * Turn a generic message descriptor into a Schema.  Individual fields that are enums
   * are converted into their string equivalents.
   * @param msgDescriptor the descriptor for the given message type.
   * @return a pig schema representing the message.
   */
  public Schema toSchema(Descriptor msgDescriptor) {
    Schema schema = new Schema();

    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          schema.add(messageToFieldSchema(fieldDescriptor));
        } else {
          schema.add(singleFieldToFieldSchema(fieldDescriptor));
        }
      }
    } catch (FrontendException e) {
      LOG.warn("Could not convert descriptor " + msgDescriptor + " to schema", e);
    }

    return schema;
  }

  /**
   * Turn a nested message into a Schema object.  For repeated nested messages, it generates a schema for a bag of
   * tuples.  For non-repeated nested messages, it just generates a schema for the tuple itself.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the Schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private FieldSchema messageToFieldSchema(FieldDescriptor fieldDescriptor) throws FrontendException {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToFieldSchema called with field of type " + fieldDescriptor.getType();

    // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
    // we can force the type into a pig map type.
    if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
        fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName()) && fieldDescriptor.isRepeated()) {
      return new FieldSchema(fieldDescriptor.getName(), null, DataType.MAP);
    }

    Schema innerSchema = toSchema(fieldDescriptor.getMessageType());

    if (fieldDescriptor.isRepeated()) {
      Schema tupleSchema = new Schema();
      tupleSchema.add(new FieldSchema(fieldDescriptor.getName() + "_tuple", innerSchema, DataType.TUPLE));
      return new FieldSchema(fieldDescriptor.getName(), tupleSchema, DataType.BAG);
    } else {
      return new FieldSchema(fieldDescriptor.getName(), innerSchema, DataType.TUPLE);
    }
  }

  /**
   * Turn a single field into a Schema object.  For repeated single fields, it generates a schema for a bag of single-item tuples.
   * For non-repeated fields, it just generates a standard field schema.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the Schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private FieldSchema singleFieldToFieldSchema(FieldDescriptor fieldDescriptor) throws FrontendException {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "singleFieldToFieldSchema called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      Schema itemSchema = new Schema();
      itemSchema.add(new FieldSchema(fieldDescriptor.getName(), null, getPigDataType(fieldDescriptor)));
      Schema itemTupleSchema = new Schema();
      itemTupleSchema.add(new FieldSchema(fieldDescriptor.getName() + "_tuple", itemSchema, DataType.TUPLE));

      return new FieldSchema(fieldDescriptor.getName() + "_bag", itemTupleSchema, DataType.BAG);
    } else {
      return new FieldSchema(fieldDescriptor.getName(), null, getPigDataType(fieldDescriptor));
    }
  }

  /**
   * Translate between protobuf's datatype representation and Pig's datatype representation.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the byte representing the pig datatype for the given field type.
   */
  private byte getPigDataType(FieldDescriptor fieldDescriptor) {
    switch (fieldDescriptor.getType()) {
      case INT32:
      case UINT32:
      case SINT32:
      case FIXED32:
      case SFIXED32:
      case BOOL: // We convert booleans to ints for pig output.
        return DataType.INTEGER;
      case INT64:
      case UINT64:
      case SINT64:
      case FIXED64:
      case SFIXED64:
        return DataType.LONG;
      case FLOAT:
        return DataType.FLOAT;
      case DOUBLE:
        return DataType.DOUBLE;
      case STRING:
      case ENUM: // We convert enums to strings for pig output.
        return DataType.CHARARRAY;
      case BYTES:
        return DataType.BYTEARRAY;
      case MESSAGE:
        throw new IllegalArgumentException("getPigDataType called on field " + fieldDescriptor.getFullName() + " of type message.");
      default:
        throw new IllegalArgumentException("Unexpected field type. " + fieldDescriptor.toString() + " " + fieldDescriptor.getFullName() + " " + fieldDescriptor.getType());
    }
  }

  /**
   * Turn a generic message descriptor into a loading schema for a pig script.
   * @param msgDescriptor the descriptor for the given message type.
   * @param loaderClassName the fully qualified classname of the pig loader to use.  Not
   * passed a <code>Class<? extends LoadFunc></code> because in many situations that class
   * is being generated as well, and so doesn't exist in compiled form.
   * @return a pig schema representing the message.
   */
  public String toPigScript(Descriptor msgDescriptor, String loaderClassName) {
    StringBuffer sb = new StringBuffer();
    final int initialTabOffset = 3;

    sb.append("raw_data = load '$INPUT_FILES' using " + loaderClassName + "()").append("\n");
    sb.append(tabs(initialTabOffset)).append("as (").append("\n");
    sb.append(toPigScriptInternal(msgDescriptor, initialTabOffset));
    sb.append(tabs(initialTabOffset)).append(");").append("\n").append("\n");

    return sb.toString();
  }

  /**
   * Turn a generic message descriptor into a loading schema for a pig script.  Individual fields that are enums
   * are converted into their string equivalents.
   * @param msgDescriptor the descriptor for the given message type.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return a pig schema representing the message.
   */
  private StringBuffer toPigScriptInternal(Descriptor msgDescriptor, int numTabs) {
    StringBuffer sb = new StringBuffer();
    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        // We have to add a comma after every line EXCEPT for the last, or Pig gets mad.
        boolean isLast = (fieldDescriptor == msgDescriptor.getFields().get(msgDescriptor.getFields().size() - 1));
        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          sb.append(messageToPigScript(fieldDescriptor, numTabs + 1, isLast));
        } else {
          sb.append(singleFieldToPigScript(fieldDescriptor, numTabs + 1, isLast));
        }
      }
    } catch (FrontendException e) {
      LOG.warn("Could not convert descriptor " + msgDescriptor + " to pig script", e);
    }

    return sb;
  }

  /**
   * Turn a nested message into a pig script load string.  For repeated nested messages, it generates a string for a bag of
   * tuples.  For non-repeated nested messages, it just generates a string for the tuple itself.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return the pig script load schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private StringBuffer messageToPigScript(FieldDescriptor fieldDescriptor, int numTabs, boolean isLast) throws FrontendException {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToPigScript called with field of type " + fieldDescriptor.getType();

    // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
    // we force the type into a pig map type.
    if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
        fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName()) && fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName())
          .append(": map[]").append(isLast ? "" : ",").append("\n");
    }

    if (fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": bag {").append("\n")
          .append(tabs(numTabs + 1)).append(fieldDescriptor.getName()).append("_tuple: tuple (").append("\n")
          .append(toPigScriptInternal(fieldDescriptor.getMessageType(), numTabs + 2))
          .append(tabs(numTabs + 1)).append(")").append("\n")
          .append(tabs(numTabs)).append("}").append(isLast ? "" : ",").append("\n");
    } else {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": tuple (").append("\n")
          .append(toPigScriptInternal(fieldDescriptor.getMessageType(), numTabs + 1))
          .append(tabs(numTabs)).append(")").append(isLast ? "" : ",").append("\n");
    }
  }

  /**
   * Turn a single field into a pig script load string.  For repeated single fields, it generates a string for a bag of single-item tuples.
   * For non-repeated fields, it just generates a standard single-element string.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return the pig script load string for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private StringBuffer singleFieldToPigScript(FieldDescriptor fieldDescriptor, int numTabs, boolean isLast) throws FrontendException {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "singleFieldToPigScript called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append("_bag: bag {").append("\n")
          .append(tabs(numTabs + 1)).append(fieldDescriptor.getName()).append("_tuple: tuple (").append("\n")
          .append(tabs(numTabs + 2)).append(fieldDescriptor.getName()).append(": ").append(getPigScriptDataType(fieldDescriptor)).append("\n")
          .append(tabs(numTabs + 1)).append(")").append("\n")
          .append(tabs(numTabs)).append("}").append(isLast ? "" : ",").append("\n");
    } else {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": ")
          .append(getPigScriptDataType(fieldDescriptor)).append(isLast ? "" : ",").append("\n");
    }
  }

  /**
   * Translate between protobuf's datatype representation and Pig's datatype representation.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the byte representing the pig datatype for the given field type.
   */
  private String getPigScriptDataType(FieldDescriptor fieldDescriptor) {
    switch (fieldDescriptor.getType()) {
      case INT32:
      case UINT32:
      case SINT32:
      case FIXED32:
      case SFIXED32:
      case BOOL: // We convert booleans to ints for pig output.
        return "int";
      case INT64:
      case UINT64:
      case SINT64:
      case FIXED64:
      case SFIXED64:
        return "long";
      case FLOAT:
        return "float";
      case DOUBLE:
        return "double";
      case STRING:
      case ENUM: // We convert enums to strings for pig output.
        return "chararray";
      case BYTES:
        return "bytearray";
      case MESSAGE:
        throw new IllegalArgumentException("getPigScriptDataType called on field " + fieldDescriptor.getFullName() + " of type message.");
      default:
        throw new IllegalArgumentException("Unexpected field type. " + fieldDescriptor.toString() + " " + fieldDescriptor.getFullName() + " " + fieldDescriptor.getType());
    }
  }

  private StringBuffer tabs(int numTabs) {
    StringBuffer sb = new StringBuffer();
    for (int i = 0; i < numTabs; i++) {
      sb.append("  ");
    }
    return sb;
  }
}=======
package com.twitter.elephantbird.pig.util;

import java.util.List;
import java.util.Map;

import com.google.common.collect.Lists;
import com.google.common.collect.Maps;
import com.google.protobuf.Descriptors.Descriptor;
import com.google.protobuf.Descriptors.EnumValueDescriptor;
import com.google.protobuf.Descriptors.FieldDescriptor;
import com.google.protobuf.ByteString;
import com.google.protobuf.Message;
import com.sun.org.apache.xerces.internal.impl.dv.xs.SchemaDateTimeException;
import com.twitter.data.proto.Misc.CountedMap;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * A class for turning codegen'd protos into Pig Tuples and Schemas
 * for custom Pig LoadFuncs.
 * @author Kevin Weil
 */
public class ProtobufToPig {
  private static final Logger LOG = LoggerFactory.getLogger(ProtobufToPig.class);

  private static final TupleFactory tupleFactory_ = TupleFactory.getInstance();
  private static BagFactory bagFactory_ = BagFactory.getInstance();

  public enum CoercionLevel { kNoCoercion, kAllowCoercionToPigMaps }

  private final CoercionLevel coercionLevel_;

  public ProtobufToPig() {
    this(CoercionLevel.kAllowCoercionToPigMaps);
  }

  public ProtobufToPig(CoercionLevel coercionLevel) {
    coercionLevel_ = coercionLevel;
  }
  /**
   * Turn a generic message into a Tuple.  Individual fields that are enums
   * are converted into their string equivalents.  Fields that are not filled
   * out in the protobuf are set to null, unless there is a default field value in
   * which case that is used instead.
   * @param msg the protobuf message
   * @return a pig tuple representing the message.
   */
  public Tuple toTuple(Message msg) {
    if (msg == null) {
      // Pig tuples deal gracefully with nulls.
      // Also, we can be called with null here in recursive calls.
      return null;
    }

    Descriptor msgDescriptor = msg.getDescriptorForType();
    Tuple tuple = tupleFactory_.newTuple(msgDescriptor.getFields().size());
    int curField = 0;
    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        // Get the set value, or the default value, or null.
        Object fieldValue = getFieldValue(msg, fieldDescriptor);

        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          tuple.set(curField++, messageToTuple(fieldDescriptor, fieldValue));
        } else {
          tuple.set(curField++, singleFieldToTuple(fieldDescriptor, fieldValue));
        }
      }
    } catch (ExecException e) {
      LOG.warn("Could not convert msg " + msg + " to tuple", e);
    }

    return tuple;
  }

  /**
   * Translate a nested message to a tuple.  If the field is repeated, it walks the list and adds each to a bag.
   * Otherwise, it just adds the given one.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object representing fieldValue in Pig -- either a bag or a tuple.
   */
  @SuppressWarnings("unchecked")
  protected Object messageToTuple(FieldDescriptor fieldDescriptor, Object fieldValue) {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToTuple called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      // The protobuf contract is that if the field is repeated, then the object returned is actually a List
      // of the underlying datatype, which in this case is a nested message.
      List<Message> messageList = (List<Message>) (fieldValue != null ? fieldValue : Lists.newArrayList());

      // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
      // we can force the type into a pig map type.
      if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
          fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName())) {
        Map<Object, Long> map = Maps.newHashMap();
        for (Message m : messageList) {
          CountedMap cm = (CountedMap) m;
          final Long curCount = map.get(cm.getKey());
          map.put(cm.getKey(), (curCount == null ? 0L : curCount) + cm.getValue());
        }
        return map;
      } else {
        DataBag bag = bagFactory_.newDefaultBag();
        for (Message m : messageList) {
          bag.add(new ProtobufTuple(m));
        }
        return bag;
      }
    } else {
      return new ProtobufTuple((Message)fieldValue);
    }
  }

  /**
   * Translate a single field to a tuple.  If the field is repeated, it walks the list and adds each to a bag.
   * Otherwise, it just adds the given one.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object representing fieldValue in Pig -- either a bag or a single field.
   * @throws ExecException if Pig decides to.  Shouldn't happen because we won't walk off the end of a tuple's field set.
   */
  @SuppressWarnings("unchecked")
  protected Object singleFieldToTuple(FieldDescriptor fieldDescriptor, Object fieldValue) throws ExecException {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "messageToFieldSchema called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      // The protobuf contract is that if the field is repeated, then the object returned is actually a List
      // of the underlying datatype, which in this case is a "primitive" like int, float, String, etc.
      // We have to make a single-item tuple out of it to put it in the bag.
      DataBag bag = bagFactory_.newDefaultBag();
      List<Object> fieldValueList = (List<Object>) (fieldValue != null ? fieldValue : Lists.newArrayList());
      for (Object singleFieldValue : fieldValueList) {
        Object nonEnumFieldValue = coerceToPigTypes(fieldDescriptor, singleFieldValue);
        Tuple innerTuple = tupleFactory_.newTuple(1);
        innerTuple.set(0, nonEnumFieldValue);
        bag.add(innerTuple);
      }
      return bag;
    } else {
      return coerceToPigTypes(fieldDescriptor, fieldValue);
    }
  }

  /**
   * If the given field value is an enum, translate it to the enum's name as a string, since Pig cannot handle enums.
   * Also, if the given field value is a bool, translate it to 0 or 1 to avoid Pig bools, which can be sketchy.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object, unless it was from an enum field, in which case we return the name of the enum field.
   */
  private Object coerceToPigTypes(FieldDescriptor fieldDescriptor, Object fieldValue) {
    if (fieldDescriptor.getType() == FieldDescriptor.Type.ENUM && fieldValue != null) {
      EnumValueDescriptor enumValueDescriptor = (EnumValueDescriptor)fieldValue;
      return enumValueDescriptor.getName();
    } else if (fieldDescriptor.getType() == FieldDescriptor.Type.BOOL && fieldValue != null) {
      Boolean boolValue = (Boolean)fieldValue;
      return new Integer(boolValue ? 1 : 0);
    } else if (fieldDescriptor.getType() == FieldDescriptor.Type.BYTES && fieldValue != null) {
      ByteString bsValue = (ByteString)fieldValue;
      return new DataByteArray(bsValue.toByteArray());
    }
    return fieldValue;
  }

  /**
   * A utility function for getting the value of a field in a protobuf message.  It first tries the
   * literal set value in the protobuf's field list.  If the value isn't set, and the field has a default
   * value, it uses that.  Otherwise, it returns null.
   * @param msg the protobuf message
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the value of the field, or null if none can be assigned.
   */
  protected Object getFieldValue(Message msg, FieldDescriptor fieldDescriptor) {
    Object o = null;
    Map<FieldDescriptor, Object> setFields = msg.getAllFields();
    if (setFields.containsKey(fieldDescriptor)) {
      o = setFields.get(fieldDescriptor);
    } else if (fieldDescriptor.hasDefaultValue()) {
      o = fieldDescriptor.getDefaultValue();
    }

    return o;
  }

  /**
   * Turn a generic message descriptor into a Schema.  Individual fields that are enums
   * are converted into their string equivalents.
   * @param msgDescriptor the descriptor for the given message type.
   * @return a pig schema representing the message.
   */
  public Schema toSchema(Descriptor msgDescriptor) {
    Schema schema = new Schema();

    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          schema.add(messageToFieldSchema(fieldDescriptor));
        } else {
          schema.add(singleFieldToFieldSchema(fieldDescriptor));
        }
      }
    } catch (FrontendException e) {
      LOG.warn("Could not convert descriptor " + msgDescriptor + " to schema", e);
    }

    return schema;
  }

  /**
   * Turn a nested message into a Schema object.  For repeated nested messages, it generates a schema for a bag of
   * tuples.  For non-repeated nested messages, it just generates a schema for the tuple itself.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the Schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private FieldSchema messageToFieldSchema(FieldDescriptor fieldDescriptor) throws FrontendException {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToFieldSchema called with field of type " + fieldDescriptor.getType();

    // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
    // we can force the type into a pig map type.
    if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
        fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName()) && fieldDescriptor.isRepeated()) {
      return new FieldSchema(fieldDescriptor.getName(), null, DataType.MAP);
    }

    Schema innerSchema = toSchema(fieldDescriptor.getMessageType());

    if (fieldDescriptor.isRepeated()) {
      Schema tupleSchema = new Schema();
      tupleSchema.add(new FieldSchema(fieldDescriptor.getName() + "_tuple", innerSchema, DataType.TUPLE));
      return new FieldSchema(fieldDescriptor.getName(), tupleSchema, DataType.BAG);
    } else {
      return new FieldSchema(fieldDescriptor.getName(), innerSchema, DataType.TUPLE);
    }
  }

  /**
   * Turn a single field into a Schema object.  For repeated single fields, it generates a schema for a bag of single-item tuples.
   * For non-repeated fields, it just generates a standard field schema.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the Schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private FieldSchema singleFieldToFieldSchema(FieldDescriptor fieldDescriptor) throws FrontendException {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "singleFieldToFieldSchema called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      Schema itemSchema = new Schema();
      itemSchema.add(new FieldSchema(fieldDescriptor.getName(), null, getPigDataType(fieldDescriptor)));
      Schema itemTupleSchema = new Schema();
      itemTupleSchema.add(new FieldSchema(fieldDescriptor.getName() + "_tuple", itemSchema, DataType.TUPLE));

      return new FieldSchema(fieldDescriptor.getName() + "_bag", itemTupleSchema, DataType.BAG);
    } else {
      return new FieldSchema(fieldDescriptor.getName(), null, getPigDataType(fieldDescriptor));
    }
  }

  /**
   * Translate between protobuf's datatype representation and Pig's datatype representation.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the byte representing the pig datatype for the given field type.
   */
  private byte getPigDataType(FieldDescriptor fieldDescriptor) {
    switch (fieldDescriptor.getType()) {
      case INT32:
      case UINT32:
      case SINT32:
      case FIXED32:
      case SFIXED32:
      case BOOL: // We convert booleans to ints for pig output.
        return DataType.INTEGER;
      case INT64:
      case UINT64:
      case SINT64:
      case FIXED64:
      case SFIXED64:
        return DataType.LONG;
      case FLOAT:
        return DataType.FLOAT;
      case DOUBLE:
        return DataType.DOUBLE;
      case STRING:
      case ENUM: // We convert enums to strings for pig output.
        return DataType.CHARARRAY;
      case BYTES:
        return DataType.BYTEARRAY;
      case MESSAGE:
        throw new IllegalArgumentException("getPigDataType called on field " + fieldDescriptor.getFullName() + " of type message.");
      default:
        throw new IllegalArgumentException("Unexpected field type. " + fieldDescriptor.toString() + " " + fieldDescriptor.getFullName() + " " + fieldDescriptor.getType());
    }
  }

  /**
   * Turn a generic message descriptor into a loading schema for a pig script.
   * @param msgDescriptor the descriptor for the given message type.
   * @param loaderClassName the fully qualified classname of the pig loader to use.  Not
   * passed a <code>Class<? extends LoadFunc></code> because in many situations that class
   * is being generated as well, and so doesn't exist in compiled form.
   * @return a pig schema representing the message.
   */
  public String toPigScript(Descriptor msgDescriptor, String loaderClassName) {
    StringBuffer sb = new StringBuffer();
    final int initialTabOffset = 3;

    sb.append("raw_data = load '$INPUT_FILES' using " + loaderClassName + "()").append("\n");
    sb.append(tabs(initialTabOffset)).append("as (").append("\n");
    sb.append(toPigScriptInternal(msgDescriptor, initialTabOffset));
    sb.append(tabs(initialTabOffset)).append(");").append("\n").append("\n");

    return sb.toString();
  }

  /**
   * Turn a generic message descriptor into a loading schema for a pig script.  Individual fields that are enums
   * are converted into their string equivalents.
   * @param msgDescriptor the descriptor for the given message type.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return a pig schema representing the message.
   */
  private StringBuffer toPigScriptInternal(Descriptor msgDescriptor, int numTabs) {
    StringBuffer sb = new StringBuffer();
    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        // We have to add a comma after every line EXCEPT for the last, or Pig gets mad.
        boolean isLast = (fieldDescriptor == msgDescriptor.getFields().get(msgDescriptor.getFields().size() - 1));
        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          sb.append(messageToPigScript(fieldDescriptor, numTabs + 1, isLast));
        } else {
          sb.append(singleFieldToPigScript(fieldDescriptor, numTabs + 1, isLast));
        }
      }
    } catch (FrontendException e) {
      LOG.warn("Could not convert descriptor " + msgDescriptor + " to pig script", e);
    }

    return sb;
  }

  /**
   * Turn a nested message into a pig script load string.  For repeated nested messages, it generates a string for a bag of
   * tuples.  For non-repeated nested messages, it just generates a string for the tuple itself.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return the pig script load schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private StringBuffer messageToPigScript(FieldDescriptor fieldDescriptor, int numTabs, boolean isLast) throws FrontendException {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToPigScript called with field of type " + fieldDescriptor.getType();

    // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
    // we force the type into a pig map type.
    if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
        fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName()) && fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName())
          .append(": map[]").append(isLast ? "" : ",").append("\n");
    }

    if (fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": bag {").append("\n")
          .append(tabs(numTabs + 1)).append(fieldDescriptor.getName()).append("_tuple: tuple (").append("\n")
          .append(toPigScriptInternal(fieldDescriptor.getMessageType(), numTabs + 2))
          .append(tabs(numTabs + 1)).append(")").append("\n")
          .append(tabs(numTabs)).append("}").append(isLast ? "" : ",").append("\n");
    } else {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": tuple (").append("\n")
          .append(toPigScriptInternal(fieldDescriptor.getMessageType(), numTabs + 1))
          .append(tabs(numTabs)).append(")").append(isLast ? "" : ",").append("\n");
    }
  }

  /**
   * Turn a single field into a pig script load string.  For repeated single fields, it generates a string for a bag of single-item tuples.
   * For non-repeated fields, it just generates a standard single-element string.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return the pig script load string for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private StringBuffer singleFieldToPigScript(FieldDescriptor fieldDescriptor, int numTabs, boolean isLast) throws FrontendException {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "singleFieldToPigScript called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append("_bag: bag {").append("\n")
          .append(tabs(numTabs + 1)).append(fieldDescriptor.getName()).append("_tuple: tuple (").append("\n")
          .append(tabs(numTabs + 2)).append(fieldDescriptor.getName()).append(": ").append(getPigScriptDataType(fieldDescriptor)).append("\n")
          .append(tabs(numTabs + 1)).append(")").append("\n")
          .append(tabs(numTabs)).append("}").append(isLast ? "" : ",").append("\n");
    } else {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": ")
          .append(getPigScriptDataType(fieldDescriptor)).append(isLast ? "" : ",").append("\n");
    }
  }

  /**
   * Translate between protobuf's datatype representation and Pig's datatype representation.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the byte representing the pig datatype for the given field type.
   */
  private String getPigScriptDataType(FieldDescriptor fieldDescriptor) {
    switch (fieldDescriptor.getType()) {
      case INT32:
      case UINT32:
      case SINT32:
      case FIXED32:
      case SFIXED32:
      case BOOL: // We convert booleans to ints for pig output.
        return "int";
      case INT64:
      case UINT64:
      case SINT64:
      case FIXED64:
      case SFIXED64:
        return "long";
      case FLOAT:
        return "float";
      case DOUBLE:
        return "double";
      case STRING:
      case ENUM: // We convert enums to strings for pig output.
        return "chararray";
      case BYTES:
        return "bytearray";
      case MESSAGE:
        throw new IllegalArgumentException("getPigScriptDataType called on field " + fieldDescriptor.getFullName() + " of type message.");
      default:
        throw new IllegalArgumentException("Unexpected field type. " + fieldDescriptor.toString() + " " + fieldDescriptor.getFullName() + " " + fieldDescriptor.getType());
    }
  }

  private StringBuffer tabs(int numTabs) {
    StringBuffer sb = new StringBuffer();
    for (int i = 0; i < numTabs; i++) {
      sb.append("  ");
    }
    return sb;
  }  
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_c01b27b_cebd3d0/rev_c01b27b-cebd3d0/src/java/com/twitter/elephantbird/pig/util/PigCounterHelper.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.util;

import java.util.Map;

import com.google.common.collect.Maps;
import org.apache.hadoop.mapred.Reporter;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger;
import org.apache.pig.impl.util.Pair;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * A helper class to deal with Hadoop counters in Pig.  They are stored within the singleton
 * PigHadoopLogger instance, but are null for some period of time at job startup, even after
 * Pig has been invoked.  This class buffers counters, trying each time to get a valid Reporter and flushing
 * stored counters each time it does.
 */
public class PigCounterHelper {
  private static final Logger LOG = LoggerFactory.getLogger(PigCounterHelper.class);

  private Map<Pair<String, String>, Long> counterStringMap_ = Maps.newHashMap();
  private Map<Enum<?>, Long> counterEnumMap_ = Maps.newHashMap();
  private Reporter reporter_ = null;

  /**
   * Mocks the Reporter.incrCounter, but adds buffering.
   * See org.apache.hadoop.mapred.Reporter's incrCounter.
   */
  public void incrCounter(String group, String counter, long incr) {
    Pair<String, String> key = new Pair<String, String>(group, counter);
    Long currentValue = counterStringMap_.get(key);
    counterStringMap_.put(key, (currentValue == null ? 0 : currentValue) + incr);

    if (getReporter() != null) {
      for (Map.Entry<Pair<String, String>, Long> entry : counterStringMap_.entrySet()) {
        getReporter().incrCounter(entry.getKey().first, entry.getKey().second, entry.getValue());
      }
      counterStringMap_.clear();
    }
  }

  /**
   * Mocks the Reporter.incrCounter, but adds buffering.
   * See org.apache.hadoop.mapred.Reporter's incrCounter.
   */
  public void incrCounter(Enum<?> key, long incr) {
    Long currentValue = counterEnumMap_.get(key);
    counterEnumMap_.put(key, (currentValue == null ? 0 : currentValue) + incr);

    if (getReporter() != null) {
      for (Map.Entry<Enum<?>, Long> entry : counterEnumMap_.entrySet()) {
        getReporter().incrCounter(entry.getKey(), entry.getValue());
      }
      counterEnumMap_.clear();
    }
  }

  /**
   * Try for the Reporter object if it hasn't been initialized yet, otherwise just return it.
   * @return the job's reporter object, or null if it isn't retrievable yet.
   */
  private Reporter getReporter() {
    if (reporter_ == null) {
      reporter_ = PigHadoopLogger.getInstance().getReporter();
    }
    return reporter_;
  }
}=======
package com.twitter.elephantbird.pig.util;

import java.util.Map;

import com.google.common.collect.Maps;
import org.apache.hadoop.mapred.Reporter;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger;
import org.apache.pig.impl.util.Pair;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * A helper class to deal with Hadoop counters in Pig.  They are stored within the singleton
 * PigHadoopLogger instance, but are null for some period of time at job startup, even after
 * Pig has been invoked.  This class buffers counters, trying each time to get a valid Reporter and flushing
 * stored counters each time it does.
 */
public class PigCounterHelper {
  private Map<Pair<String, String>, Long> counterStringMap_ = Maps.newHashMap();
  private Map<Enum<?>, Long> counterEnumMap_ = Maps.newHashMap();
  private Reporter reporter_ = null;

  /**
   * Mocks the Reporter.incrCounter, but adds buffering.
   * See org.apache.hadoop.mapred.Reporter's incrCounter.
   */
  public void incrCounter(String group, String counter, long incr) {
    if (getReporter() != null) { // common case
      getReporter().incrCounter(group, counter, incr);
      if (counterStringMap_.size() > 0) {
        for (Map.Entry<Pair<String, String>, Long> entry : counterStringMap_.entrySet()) {
          getReporter().incrCounter(entry.getKey().first, entry.getKey().second, entry.getValue());
        }
        counterStringMap_.clear();
      }
    } else { // buffer the increments.
      Pair<String, String> key = new Pair<String, String>(group, counter);
      Long currentValue = counterStringMap_.get(key);
      counterStringMap_.put(key, (currentValue == null ? 0 : currentValue) + incr);
    }
  }

  /**
   * Mocks the Reporter.incrCounter, but adds buffering.
   * See org.apache.hadoop.mapred.Reporter's incrCounter.
   */
  public void incrCounter(Enum<?> key, long incr) {
    if (getReporter() != null) {
      getReporter().incrCounter(key, incr);
      if (counterEnumMap_.size() > 0) {
        for (Map.Entry<Enum<?>, Long> entry : counterEnumMap_.entrySet()) {
          getReporter().incrCounter(entry.getKey(), entry.getValue());
        }
        counterEnumMap_.clear();
      }
    } else { // buffer the increments
      Long currentValue = counterEnumMap_.get(key);
      counterEnumMap_.put(key, (currentValue == null ? 0 : currentValue) + incr);
    }
  }

  /**
   * Try for the Reporter object if it hasn't been initialized yet, otherwise just return it.
   * @return the job's reporter object, or null if it isn't retrievable yet.
   */
  private Reporter getReporter() {
    if (reporter_ == null) {
      reporter_ = PigHadoopLogger.getInstance().getReporter();
    }
    return reporter_;
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_c01b27b_cebd3d0/rev_c01b27b-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoBaseLoadFunc.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.net.URI;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import org.apache.pig.ExecType;
import org.apache.pig.FuncSpec;
import org.apache.pig.LoadFunc;
import org.apache.pig.PigException;
import org.apache.pig.SamplableLoader;
import org.apache.pig.Slice;
import org.apache.pig.Slicer;
import org.apache.pig.backend.datastorage.ContainerDescriptor;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.backend.datastorage.ElementDescriptor;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.builtin.Utf8StorageConverter;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.io.BufferedPositionedInputStream;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.collect.Lists;
import com.hadoop.compression.lzo.LzoIndex;
import com.hadoop.compression.lzo.LzopCodec;
import com.twitter.elephantbird.pig.util.PigCounterHelper;

/**
 * This class handles LZO-decoding and slicing input LZO files.  It expects the
 * filenames to end in .lzo, otherwise it assumes they are not compressed and skips them.
 * TODO: Improve the logic to accept a mixture of lzo and non-lzo files.
 */
public abstract class LzoBaseLoadFunc extends Utf8StorageConverter implements LoadFunc, Slicer, SamplableLoader {
  private static final Logger LOG = LoggerFactory.getLogger(LzoBaseLoadFunc.class);

  protected final String LZO_EXTENSION = new LzopCodec().getDefaultExtension();

  // This input stream will be our wrapped LZO-decoding stream.
  protected BufferedPositionedInputStream is_;
  protected long end_;
  // The load func spec is the load function name (with classpath) plus the arguments.
  protected FuncSpec loadFuncSpec_;
  // Whether our split begins at the beginning of the data in the file.  Generally one can check
  // for offset == 0, but not with LZO's variable-length file header.
  protected boolean beginsAtHeader_ = false;

  // Making accessing Hadoop counters from Pig slightly more convenient.
  private final PigCounterHelper counterHelper_ = new PigCounterHelper();

  /**
   * Construct a new load func.
   */
  public LzoBaseLoadFunc() {
    // By default, the spec is the class being loaded with no arguments.
    setLoaderSpec(getClass(), new String[] {});
  }

  /**
   * Set whether this chunk begins at the beginning of the entire file,
   * just after the LZO file header.
   * @param beginsAtHeader whether this is the first chunk of the file, and so
   *        begins at the LZO file header offset.
   */
  public void setBeginsAtHeader(boolean beginsAtHeader) {
    beginsAtHeader_ = beginsAtHeader;
  }

  /**
   * The important part of the loader -- given a storage object and a location to load, actually
   * compute the splits.  Walks through each LZO file under the given path and attempts to use the
   * .lzo.index file to slice it.
   *
   * @param store the data storage object.
   * @param location the given location to load, e.g. '/tables/statuses/20090815.lzo' when invoked as
   * a = LOAD '/tables/statuses/20090815.lzo' USING ... AS ...;
   */
  public Slice[] slice(DataStorage store, String location) throws IOException {
    LOG.info("LzoBaseLoadFunc::slice, location = " + location);
    List<LzoSlice> slices = Lists.newArrayList();
    // Compute the set of LZO files matching the given pattern.
    List<ElementDescriptor> globbedFiles = globFiles(store, location);

    for (ElementDescriptor file : globbedFiles) {
      // Make sure to slice according to the per-file split characteristics.
      Map<String, Object> fileStats = file.getStatistics();
      long blockSize = (Long)fileStats.get(ElementDescriptor.BLOCK_SIZE_KEY);
      long fileSize = (Long)fileStats.get(ElementDescriptor.LENGTH_KEY);

      LOG.debug("Slicing LZO file at path " + file + ": block size " + blockSize + " and file size " + fileSize);
      slices.addAll(sliceFile(file.toString(), blockSize, fileSize));
    }
    if (slices.size() == 0) {
      throw new PigException("no files found a path "+location);
    }
    LOG.info("Got " + slices.size() + " LZO slices in total.");
    return slices.toArray(new Slice[slices.size()]);
  }

  /**
   * Nothing to do here, since location can be an unexpanded glob.
   * Also, the idea that validate returns void and instead throws an exception to fail validation is ridiculous.
   */
  public void validate(DataStorage store, String location) throws IOException {
  }

  /**
   * Called on the datanodes during the data loading process, this connects a Map job with an input split.
   * @param filename the name of the file whose split is being loaded.
   * @param is the input stream to the file.
   * @param offset the offset within the file (the input stream is pre-positioned here)
   * @param end the end offset of the input split.
   */
  public void bindTo(String filename, BufferedPositionedInputStream is, long offset, long end) throws IOException {
    LOG.info("LzoBaseLoadFunc::bindTo, filename = " + filename + ", offset = " + offset + ", and end = " + end);
    LOG.debug("InputStream position is: "+is.getPosition());
    is_ = is;
    end_ = end;

    // Override this to do anything loader-specific to the input stream, etc.
    postBind();
    // Override to do any special syncing for moving to the right point of a new input split.
    skipToNextSyncPoint(beginsAtHeader_);

    LOG.debug("InputStream position after skip is: "+is.getPosition());
  }

  /**
   * Override to do anything special after the bindTo function has been called, and before the sync happens.
   */
  public void postBind() throws IOException {
  }

  /**
   * Override to do any special syncing for moving to the right point of a new input split.
   *
   * @param atFirstRecord whether or not this is the first record in the file.  Typically for line-based
   * readers, for example, we want to skip to the next new line at the beginning of an input split because
   * the arbitrary byte offset we're at generally puts us in the middle of a line.  We count on the previous
   * input split to read slightly beyond its offset to the end of the next line to account for this.
   * However, this doesn't hold for the very first record in the file.
   */
  public abstract void skipToNextSyncPoint(boolean atFirstRecord) throws IOException;

  /**
   * Give hints to pig about the output schema -- there are none needed.
   */
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return null;
  }

  /**
   * This seems to always be unimplemented.
   */
  public void fieldsToRead(Schema schema) {
  }

  /**
   * Set the loader spec so any arguments given in the script are tracked, to be reinstantiated by the mappers.
   * @param clazz the class of the load function to use.
   * @param args an array of strings that are fed to the class's constructor.
   */
  protected void setLoaderSpec(Class <? extends LzoBaseLoadFunc> clazz, String[] args) {
    loadFuncSpec_ = new FuncSpec(clazz.getName(), args);
  }

  /**
   * A convenience function for working with Hadoop counter objects from load functions.  The Hadoop
   * reporter object isn't always set up at first, so this class provides brief buffering to ensure
   * that counters are always recorded.
   */
  protected void incrCounter(String group, String counter, long incr) {
    counterHelper_.incrCounter(group, counter, incr);
  }

  /**
   * A convenience function for working with Hadoop counter objects from load functions.  The Hadoop
   * reporter object isn't always set up at first, so this class provides brief buffering to ensure
   * that counters are always recorded.
   */
  protected void incrCounter(Enum<?> key, long incr) {
    counterHelper_.incrCounter(key, incr);
  }

  /**
   * Called to verify that the stream is readable, i.e. not null and not past the byte offset
   * of the next split.
   * @return true if the input stream is valid and has not yet read past the last byte of the current split.
   */
  protected boolean verifyStream() throws IOException {
    return is_ != null && is_.getPosition() <= end_;
  }

  /**
   * Given a path, glob all the files underneath it, and return all the LZO files.
   * @param store the data store object
   * @param location the input location glob from the pig script
   * @return the set of LZO files matching the location glob
   */
  private List<ElementDescriptor> globFiles(DataStorage store, String location) throws IOException {
    List<ElementDescriptor> files = Lists.newArrayList();
    List<ElementDescriptor> paths = Lists.newArrayList();
    paths.addAll(Arrays.asList(store.asCollection(location)));

    // Note that paths.size increases through the loop as directories are encountered.
    for (int j = 0; j < paths.size(); j++) {
      ElementDescriptor path = paths.get(j);
      ElementDescriptor fullPath = store.asElement(store.getActiveContainer(), path);
      if (fullPath.systemElement()) {
        // Skip Hadoop's private/meta files.
        continue;
      }

      // If it's a directory, add it to the path and go back to the top.
      try {
        if (fullPath instanceof ContainerDescriptor) {
          for (ElementDescriptor child : (ContainerDescriptor)fullPath) {
            paths.add(child);
          }
          continue;
        }
      } catch (Exception e) {
        // See the corresponding part of PigSlicer.java
        int errCode = 2099;
        String msg = "Problem in constructing LZO slices: " + e.getMessage();
        throw new ExecException(msg, errCode, PigException.BUG, e);
      }

      // It's a file.
      // TODO: make this able to read non-LZO data too.
      if (!fullPath.toString().endsWith(LZO_EXTENSION)) {
        continue;
      }
      files.add(fullPath);
    }

    return files;
  }

  /**
   * Given an individual LZO file, break it into input splits according to its block size,
   * then use the index (if it exists) to massage the offsets to LZO block boundaries
   * to allow the file to be split across mappers.
   * @param filename the name of the file being read
   * @param blockSize the Hadoop block size of the file (usually 64 or 128 MB)
   * @param fileSize the total length of the file.
   * @return a list of LzoSlice objects corresponding to the massaged splits.
   */
  private List<LzoSlice> sliceFile(String filename, long blockSize, long fileSize) throws IOException {
    List<LzoSlice> slices = new ArrayList<LzoSlice>();
    Path filePath = new Path(filename);

    LzoIndex index = LzoIndex.readIndex(FileSystem.get(URI.create(filename), new Configuration()), filePath);
    if (index == null || index.isEmpty()) {
      LOG.info("LzoLoadFunc::sliceFile, file " + filename + " and index is empty or nonexistant");
      // If there is no index, or it couldn't be read, don't split.
      slices.add(new LzoSlice(filename, 0, fileSize, loadFuncSpec_));
    } else if (fileSize == 0) {
      LOG.info("LzoLoadFunc::sliceFile, file " + filename + " and fileSize == 0");
      // Add fileSize empty slice.  This is a total hack to deal with the
      // case where hadoop isn't starting maps for empty arrays of
      // InputSplits.  See PIG-619.  This should be removed
      // once we determine why this is.
      slices.add(new LzoSlice(filename, 0, blockSize, loadFuncSpec_));
    } else {
      // There is an index file.  First create the default file splits based on the blocksize.
      List<FileSplit> splits = new ArrayList<FileSplit>();
      for (long pos = 0; pos < fileSize; pos += blockSize) {
        splits.add(new FileSplit(filePath, pos, Math.min(blockSize, fileSize - pos), null));
      }

      LOG.info("LzoLoadFunc::sliceFile, file " + filename + " with size " + fileSize + " into " + splits.size() + " chunks.");
      for (FileSplit split : splits) {
        // Now massage the default splits to LZO block boundaries.
        long start = split.getStart();
        long end = start + split.getLength();

        long lzoStart = index.alignSliceStartToIndex(start, end);
        long lzoEnd = index.alignSliceEndToIndex(end, fileSize);

        if (lzoStart != LzoIndex.NOT_FOUND  && lzoEnd != LzoIndex.NOT_FOUND) {
          slices.add(new LzoSlice(filename, lzoStart, lzoEnd - lzoStart, loadFuncSpec_));
        }
      }
    }

    return slices;
  }

  public long getPosition() throws IOException {
    return is_.getPosition();
  }

  public Tuple getSampledTuple() throws IOException {
    if (getPosition() > end_ ) {
      return null;
    }
    return getNext();
  }

  public long skip(long bytesToSkip) throws IOException {
    long startPos = getPosition();
    is_.skip(bytesToSkip);
    skipToNextSyncPoint(getPosition() == 0 && this.beginsAtHeader_);
    return getPosition() - startPos;
  }

  @Override
  public LoadFunc.RequiredFieldResponse fieldsToRead(LoadFunc.RequiredFieldList requiredFieldList) throws FrontendException {
      return new LoadFunc.RequiredFieldResponse(false);
  }

}=======
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.net.URI;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import org.apache.pig.ExecType;
import org.apache.pig.FuncSpec;
import org.apache.pig.LoadFunc;
import org.apache.pig.PigException;
import org.apache.pig.SamplableLoader;
import org.apache.pig.Slice;
import org.apache.pig.Slicer;
import org.apache.pig.backend.datastorage.ContainerDescriptor;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.backend.datastorage.ElementDescriptor;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.builtin.Utf8StorageConverter;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.io.BufferedPositionedInputStream;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.collect.Lists;
import com.hadoop.compression.lzo.LzoIndex;
import com.hadoop.compression.lzo.LzopCodec;
import com.twitter.elephantbird.pig.util.PigCounterHelper;

/**
 * This class handles LZO-decoding and slicing input LZO files.  It expects the
 * filenames to end in .lzo, otherwise it assumes they are not compressed and skips them.
 * TODO: Improve the logic to accept a mixture of lzo and non-lzo files.
 */
public abstract class LzoBaseLoadFunc extends Utf8StorageConverter implements LoadFunc, Slicer, SamplableLoader {
  private static final Logger LOG = LoggerFactory.getLogger(LzoBaseLoadFunc.class);

  protected final String LZO_EXTENSION = new LzopCodec().getDefaultExtension();

  // This input stream will be our wrapped LZO-decoding stream.
  protected BufferedPositionedInputStream is_;
  protected long end_;
  // The load func spec is the load function name (with classpath) plus the arguments.
  protected FuncSpec loadFuncSpec_;
  // Whether our split begins at the beginning of the data in the file.  Generally one can check
  // for offset == 0, but not with LZO's variable-length file header.
  protected boolean beginsAtHeader_ = false;

  // Making accessing Hadoop counters from Pig slightly more convenient.
  private final PigCounterHelper counterHelper_ = new PigCounterHelper();

  /**
   * Construct a new load func.
   */
  public LzoBaseLoadFunc() {
    // By default, the spec is the class being loaded with no arguments.
    setLoaderSpec(getClass(), new String[] {});
  }

  /**
   * Set whether this chunk begins at the beginning of the entire file,
   * just after the LZO file header.
   * @param beginsAtHeader whether this is the first chunk of the file, and so
   *        begins at the LZO file header offset.
   */
  public void setBeginsAtHeader(boolean beginsAtHeader) {
    beginsAtHeader_ = beginsAtHeader;
  }

  /**
   * The important part of the loader -- given a storage object and a location to load, actually
   * compute the splits.  Walks through each LZO file under the given path and attempts to use the
   * .lzo.index file to slice it.
   *
   * @param store the data storage object.
   * @param location the given location to load, e.g. '/tables/statuses/20090815.lzo' when invoked as
   * a = LOAD '/tables/statuses/20090815.lzo' USING ... AS ...;
   */
  public Slice[] slice(DataStorage store, String location) throws IOException {
    LOG.info("LzoBaseLoadFunc::slice, location = " + location);
    List<LzoSlice> slices = Lists.newArrayList();
    // Compute the set of LZO files matching the given pattern.
    List<ElementDescriptor> globbedFiles = globFiles(store, location);

    for (ElementDescriptor file : globbedFiles) {
      // Make sure to slice according to the per-file split characteristics.
      Map<String, Object> fileStats = file.getStatistics();
      long blockSize = (Long)fileStats.get(ElementDescriptor.BLOCK_SIZE_KEY);
      long fileSize = (Long)fileStats.get(ElementDescriptor.LENGTH_KEY);

      LOG.debug("Slicing LZO file at path " + file + ": block size " + blockSize + " and file size " + fileSize);
      slices.addAll(sliceFile(file.toString(), blockSize, fileSize));
    }
    if (slices.size() == 0) {
      throw new PigException("no files found a path "+location);
    }
    LOG.info("Got " + slices.size() + " LZO slices in total.");
    return slices.toArray(new Slice[slices.size()]);
  }

  /**
   * Nothing to do here, since location can be an unexpanded glob.
   * Also, the idea that validate returns void and instead throws an exception to fail validation is ridiculous.
   */
  public void validate(DataStorage store, String location) throws IOException {
  }

  /**
   * Called on the datanodes during the data loading process, this connects a Map job with an input split.
   * @param filename the name of the file whose split is being loaded.
   * @param is the input stream to the file.
   * @param offset the offset within the file (the input stream is pre-positioned here)
   * @param end the end offset of the input split.
   */
  public void bindTo(String filename, BufferedPositionedInputStream is, long offset, long end) throws IOException {
    LOG.info("LzoBaseLoadFunc::bindTo, filename = " + filename + ", offset = " + offset + ", and end = " + end);
    LOG.debug("InputStream position is: "+is.getPosition());
    is_ = is;
    end_ = end;

    // Override this to do anything loader-specific to the input stream, etc.
    postBind();
    // Override to do any special syncing for moving to the right point of a new input split.
    skipToNextSyncPoint(beginsAtHeader_);

    LOG.debug("InputStream position after skip is: "+is.getPosition());
  }

  /**
   * Override to do anything special after the bindTo function has been called, and before the sync happens.
   */
  public void postBind() throws IOException {
  }

  /**
   * Override to do any special syncing for moving to the right point of a new input split.
   *
   * @param atFirstRecord whether or not this is the first record in the file.  Typically for line-based
   * readers, for example, we want to skip to the next new line at the beginning of an input split because
   * the arbitrary byte offset we're at generally puts us in the middle of a line.  We count on the previous
   * input split to read slightly beyond its offset to the end of the next line to account for this.
   * However, this doesn't hold for the very first record in the file.
   */
  public abstract void skipToNextSyncPoint(boolean atFirstRecord) throws IOException;

  /**
   * Give hints to pig about the output schema -- there are none needed.
   */
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return null;
  }

  /**
   * This seems to always be unimplemented.
   */
  public void fieldsToRead(Schema schema) {
  }

  /**
   * Set the loader spec so any arguments given in the script are tracked, to be reinstantiated by the mappers.
   * @param clazz the class of the load function to use.
   * @param args an array of strings that are fed to the class's constructor.
   */
  protected void setLoaderSpec(Class <? extends LzoBaseLoadFunc> clazz, String[] args) {
    loadFuncSpec_ = new FuncSpec(clazz.getName(), args);
  }

  /**
   * A convenience function for working with Hadoop counter objects from load functions.  The Hadoop
   * reporter object isn't always set up at first, so this class provides brief buffering to ensure
   * that counters are always recorded.
   */
  protected void incrCounter(String group, String counter, long incr) {
    counterHelper_.incrCounter(group, counter, incr);
  }
  
  /** same as incrCounter(pair.first, pair.second, incr). */
  protected void incrCounter(Pair<String, String> groupCounterPair, long incr) {
    counterHelper_.incrCounter(groupCounterPair.first, groupCounterPair.second, incr);
  }
  
  /**
   * A convenience function for working with Hadoop counter objects from load functions.  The Hadoop
   * reporter object isn't always set up at first, so this class provides brief buffering to ensure
   * that counters are always recorded.
   */
  protected void incrCounter(Enum<?> key, long incr) {
    counterHelper_.incrCounter(key, incr);
  }

  /**
   * Called to verify that the stream is readable, i.e. not null and not past the byte offset
   * of the next split.
   * @return true if the input stream is valid and has not yet read past the last byte of the current split.
   */
  protected boolean verifyStream() throws IOException {
    return is_ != null && is_.getPosition() <= end_;
  }

  /**
   * Given a path, glob all the files underneath it, and return all the LZO files.
   * @param store the data store object
   * @param location the input location glob from the pig script
   * @return the set of LZO files matching the location glob
   */
  private List<ElementDescriptor> globFiles(DataStorage store, String location) throws IOException {
    List<ElementDescriptor> files = Lists.newArrayList();
    List<ElementDescriptor> paths = Lists.newArrayList();
    paths.addAll(Arrays.asList(store.asCollection(location)));

    // Note that paths.size increases through the loop as directories are encountered.
    for (int j = 0; j < paths.size(); j++) {
      ElementDescriptor path = paths.get(j);
      ElementDescriptor fullPath = store.asElement(store.getActiveContainer(), path);
      if (fullPath.systemElement()) {
        // Skip Hadoop's private/meta files.
        continue;
      }

      // If it's a directory, add it to the path and go back to the top.
      try {
        if (fullPath instanceof ContainerDescriptor) {
          for (ElementDescriptor child : (ContainerDescriptor)fullPath) {
            paths.add(child);
          }
          continue;
        }
      } catch (Exception e) {
        // See the corresponding part of PigSlicer.java
        int errCode = 2099;
        String msg = "Problem in constructing LZO slices: " + e.getMessage();
        throw new ExecException(msg, errCode, PigException.BUG, e);
      }

      // It's a file.
      // TODO: make this able to read non-LZO data too.
      if (!fullPath.toString().endsWith(LZO_EXTENSION)) {
        continue;
      }
      files.add(fullPath);
    }

    return files;
  }

  /**
   * Given an individual LZO file, break it into input splits according to its block size,
   * then use the index (if it exists) to massage the offsets to LZO block boundaries
   * to allow the file to be split across mappers.
   * @param filename the name of the file being read
   * @param blockSize the Hadoop block size of the file (usually 64 or 128 MB)
   * @param fileSize the total length of the file.
   * @return a list of LzoSlice objects corresponding to the massaged splits.
   */
  private List<LzoSlice> sliceFile(String filename, long blockSize, long fileSize) throws IOException {
    List<LzoSlice> slices = new ArrayList<LzoSlice>();
    Path filePath = new Path(filename);

    LzoIndex index = LzoIndex.readIndex(FileSystem.get(URI.create(filename), new Configuration()), filePath);
    if (index == null || index.isEmpty()) {
      LOG.info("LzoLoadFunc::sliceFile, file " + filename + " and index is empty or nonexistant");
      // If there is no index, or it couldn't be read, don't split.
      slices.add(new LzoSlice(filename, 0, fileSize, loadFuncSpec_));
    } else if (fileSize == 0) {
      LOG.info("LzoLoadFunc::sliceFile, file " + filename + " and fileSize == 0");
      // Add fileSize empty slice.  This is a total hack to deal with the
      // case where hadoop isn't starting maps for empty arrays of
      // InputSplits.  See PIG-619.  This should be removed
      // once we determine why this is.
      slices.add(new LzoSlice(filename, 0, blockSize, loadFuncSpec_));
    } else {
      // There is an index file.  First create the default file splits based on the blocksize.
      List<FileSplit> splits = new ArrayList<FileSplit>();
      for (long pos = 0; pos < fileSize; pos += blockSize) {
        splits.add(new FileSplit(filePath, pos, Math.min(blockSize, fileSize - pos), null));
      }

      LOG.info("LzoLoadFunc::sliceFile, file " + filename + " with size " + fileSize + " into " + splits.size() + " chunks.");
      for (FileSplit split : splits) {
        // Now massage the default splits to LZO block boundaries.
        long start = split.getStart();
        long end = start + split.getLength();

        long lzoStart = index.alignSliceStartToIndex(start, end);
        long lzoEnd = index.alignSliceEndToIndex(end, fileSize);

        if (lzoStart != LzoIndex.NOT_FOUND  && lzoEnd != LzoIndex.NOT_FOUND) {
          slices.add(new LzoSlice(filename, lzoStart, lzoEnd - lzoStart, loadFuncSpec_));
        }
      }
    }

    return slices;
  }

  public long getPosition() throws IOException {
    return is_.getPosition();
  }

  public Tuple getSampledTuple() throws IOException {
    if (getPosition() > end_ ) {
      return null;
    }
    return getNext();
  }

  public long skip(long bytesToSkip) throws IOException {
    long startPos = getPosition();
    is_.skip(bytesToSkip);
    skipToNextSyncPoint(getPosition() == 0 && this.beginsAtHeader_);
    return getPosition() - startPos;
  }

  @Override
  public LoadFunc.RequiredFieldResponse fieldsToRead(LoadFunc.RequiredFieldList requiredFieldList) throws FrontendException {
      return new LoadFunc.RequiredFieldResponse(false);
  }

}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_c01b27b_cebd3d0/rev_c01b27b-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.load;

import java.io.IOException;

import org.apache.pig.ExecType;
import org.apache.pig.LoadFunc;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.Message;
import com.twitter.elephantbird.mapreduce.io.ProtobufBlockReader;
import com.twitter.elephantbird.mapreduce.io.ProtobufWritable;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;


public class LzoProtobufBlockPigLoader<M extends Message> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoProtobufBlockPigLoader.class);

  private ProtobufBlockReader<M> reader_ = null;
  private ProtobufWritable<M> value_ = null;
  private TypeRef<M> typeRef_ = null;
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  protected enum LzoProtobufBlockPigLoaderCounters { ProtobufsRead }

  public LzoProtobufBlockPigLoader() {
    LOG.info("LzoProtobufBlockLoader zero-parameter creation");
  }

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called before getNext!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    value_ = new ProtobufWritable<M>(typeRef_);
  }

  @Override
  public void postBind() throws IOException {
    reader_ = new ProtobufBlockReader<M>(is_, typeRef_);
  }

  @Override
  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // We want to explicitly not do any special syncing here, because the reader_
    // handles this automatically.
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }


  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    // If we are past the end of the file split, tell the reader not to read any more new blocks.
    // Then continue reading until the last of the reader's already-parsed values are used up.
    // The next split will start at the next sync point and no records will be missed.
    if (is_.getPosition() > end_) {
      reader_.markNoMoreNewBlocks();
    }

    Tuple t = null;
    if (reader_.readProtobuf(value_)) {
      t = new ProtobufTuple(value_.get());
      incrCounter(LzoProtobufBlockPigLoaderCounters.ProtobufsRead, 1L);
    }
    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}=======
package com.twitter.elephantbird.pig.load;

import java.io.IOException;

import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.Message;
import com.twitter.elephantbird.mapreduce.io.ProtobufBlockReader;
import com.twitter.elephantbird.mapreduce.io.ProtobufWritable;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;


public class LzoProtobufBlockPigLoader<M extends Message> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoProtobufBlockPigLoader.class);

  private ProtobufBlockReader<M> reader_ = null;
  private ProtobufWritable<M> value_ = null;
  private TypeRef<M> typeRef_ = null;
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  private Pair<String, String> protobufsRead;
  private Pair<String, String> protobufErrors;

  public LzoProtobufBlockPigLoader() {
    LOG.info("LzoProtobufBlockLoader zero-parameter creation");
  }

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called before getNext!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    value_ = new ProtobufWritable<M>(typeRef_);
    String group = "LzoBlocks of " + typeRef_.getRawClass().getName();
    protobufsRead = new Pair<String, String>(group, "Protobufs Read");
    protobufErrors = new Pair<String, String>(group, "Errors");
  }

  @Override
  public void postBind() throws IOException {
    reader_ = new ProtobufBlockReader<M>(is_, typeRef_);
  }

  @Override
  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // We want to explicitly not do any special syncing here, because the reader_
    // handles this automatically.
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }


  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    // If we are past the end of the file split, tell the reader not to read any more new blocks.
    // Then continue reading until the last of the reader's already-parsed values are used up.
    // The next split will start at the next sync point and no records will be missed.
    if (is_.getPosition() > end_) {
      reader_.markNoMoreNewBlocks();
    }

    Tuple t = null;
    if (reader_.readProtobuf(value_)) {
      if (value_.get() == null) {
        incrCounter(protobufErrors, 1);
      }
      t = new ProtobufTuple(value_.get());
      incrCounter(protobufsRead, 1L);
    }
    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_c01b27b_cebd3d0/rev_c01b27b-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.nio.charset.Charset;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.ExecType;
import org.apache.pig.LoadFunc;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.base.Function;
import com.google.protobuf.Message;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * This is the base class for all base64 encoded, line-oriented protocol buffer based pig loaders.
 * Data is expected to be one base64 encoded serialized protocol buffer per line. The specific
 * protocol buffer is a template parameter, generally specified by a codegen'd derived class.
 * See com.twitter.elephantbird.proto.HadoopProtoCodeGenerator.
 */

public abstract class LzoProtobufB64LinePigLoader<M extends Message> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoProtobufB64LinePigLoader.class);

  private TypeRef<M> typeRef_ = null;
  private Function<byte[], M> protoConverter_ = null;
  private final Base64 base64_ = new Base64();
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  private static final Charset UTF8 = Charset.forName("UTF-8");
  private static final byte RECORD_DELIMITER = (byte)'\n';

  protected enum LzoProtobufB64LinePigLoaderCounts { LinesRead, ProtobufsRead }

  public LzoProtobufB64LinePigLoader() {
    LOG.info("LzoProtobufB64LineLoader zero-parameter creation");
  }

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called before getNext!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    protoConverter_ = Protobufs.getProtoConverter(typeRef.getRawClass());
  }

  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // Since we are not block aligned we throw away the first record of each split and count on a different
    // instance to read it.  The only split this doesn't work for is the first.
    if (!atFirstRecord) {
      getNext();
    }
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    String line;
    Tuple t = null;
    while ((line = is_.readLine(UTF8, RECORD_DELIMITER)) != null) {
      incrCounter(LzoProtobufB64LinePigLoaderCounts.LinesRead, 1L);
      M protoValue = protoConverter_.apply(base64_.decode(line.getBytes("UTF-8")));
      if (protoValue != null) {
        t = new ProtobufTuple(protoValue);
        incrCounter(LzoProtobufB64LinePigLoaderCounts.ProtobufsRead, 1L);
        break;
      }
    }

    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}=======
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.nio.charset.Charset;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.base.Function;
import com.google.protobuf.Message;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * This is the base class for all base64 encoded, line-oriented protocol buffer based pig loaders.
 * Data is expected to be one base64 encoded serialized protocol buffer per line. The specific
 * protocol buffer is a template parameter, generally specified by a codegen'd derived class.
 * See com.twitter.elephantbird.proto.HadoopProtoCodeGenerator.
 */

public abstract class LzoProtobufB64LinePigLoader<M extends Message> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoProtobufB64LinePigLoader.class);

  private TypeRef<M> typeRef_ = null;
  private Function<byte[], M> protoConverter_ = null;
  private final Base64 base64_ = new Base64();
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  private static final Charset UTF8 = Charset.forName("UTF-8");
  private static final byte RECORD_DELIMITER = (byte)'\n';

  private Pair<String, String> linesRead;
  private Pair<String, String> protobufsRead;
  private Pair<String, String> protobufErrors;

  public LzoProtobufB64LinePigLoader() {
    LOG.info("LzoProtobufB64LineLoader zero-parameter creation");
  }

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called before getNext!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    protoConverter_ = Protobufs.getProtoConverter(typeRef.getRawClass());
    String group = "LzoB64Lines of " + typeRef_.getRawClass().getName();
    linesRead = new Pair<String, String>(group, "Lines Read");
    protobufsRead = new Pair<String, String>(group, "Protobufs Read");
    protobufErrors = new Pair<String, String>(group, "Errors");
  }

  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // Since we are not block aligned we throw away the first record of each split and count on a different
    // instance to read it.  The only split this doesn't work for is the first.
    if (!atFirstRecord) {
      getNext();
    }
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    String line;
    Tuple t = null;
    while ((line = is_.readLine(UTF8, RECORD_DELIMITER)) != null) {
      incrCounter(linesRead, 1L);
      M protoValue = protoConverter_.apply(base64_.decode(line.getBytes("UTF-8")));
      if (protoValue != null) {
        t = new ProtobufTuple(protoValue);
        incrCounter(protobufsRead, 1L);
        break;
      } else {
        incrCounter(protobufErrors, 1L);
      }
    }

    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_c01b27b_cebd3d0/rev_c01b27b-cebd3d0/src/java/com/twitter/elephantbird/pig/piggybank/BytesToThriftTuple.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.piggybank;

import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.thrift.TBase;
import org.apache.thrift.TDeserializer;
import org.apache.thrift.TException;
import org.apache.thrift.protocol.TBinaryProtocol;
import org.apache.thrift.transport.TMemoryBuffer;

import com.twitter.elephantbird.util.TypeRef;

/**
 * This is an abstract UDF for converting serialized Thrift objects into Pig tuples.
 * To create a converter for your Thrift class <code>MyThriftClass</code>, you simply need to extend
 * <code>BytesToThriftTuple</code> with something like this:
 *<pre>
 * {@code
 * public class BytesToSimpleLocation extends BytesToThriftTuple<MyThriftClass> {
 *
 *   public BytesToSimpleLocation() {
 *     setTypeRef(new TypeRef<MyThriftClass>() {});
 *   }
 * }}
 *</pre>
 */
public abstract class BytesToThriftTuple<T extends TBase<?>> extends EvalFunc<Tuple> {

  private final TDeserializer deserializer_ = new TDeserializer(new TBinaryProtocol.Factory());
private final ThriftToTuple<T> thriftToTuple_ = new ThriftToTuple<T>();
  private TypeRef<T> typeRef_;
  private T thriftObj_ = null;

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called by the constructor!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<T> typeRef) {
    typeRef_ = typeRef;
  }


  @Override
  public Tuple exec(org.apache.pig.data.Tuple input) throws IOException {
    if (input == null || input.size() < 1) return null;
    try {
      if (thriftObj_ == null) {
        thriftObj_ = typeRef_.safeNewInstance();
      }
      DataByteArray dbarr = (DataByteArray) input.get(0);
      deserializer_.deserialize(thriftObj_, dbarr.get());
      return thriftToTuple_.convert(thriftObj_);
    } catch (IOException e) {
      log.warn("Caught exception "+e.getMessage());
      return null;
    } catch (TException e) {
      log.warn("Unable to deserialize Thrift object: "+e);
      return null;
    }
  }
}=======
package com.twitter.elephantbird.pig.piggybank;

import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.thrift.TBase;
import org.apache.thrift.TDeserializer;
import org.apache.thrift.TException;
import org.apache.thrift.protocol.TBinaryProtocol;
import org.apache.thrift.transport.TMemoryBuffer;

import com.twitter.elephantbird.util.TypeRef;

/**
 * This is an abstract UDF for converting serialized Thrift objects into Pig tuples.
 * To create a converter for your Thrift class <code>MyThriftClass</code>, you simply need to extend
 * <code>BytesToThriftTuple</code> with something like this:
 *<pre>
 * {@code
 * public class BytesToSimpleLocation extends BytesToThriftTuple<MyThriftClass> {
 *
 *   public BytesToSimpleLocation() {
 *     setTypeRef(new TypeRef<MyThriftClass>() {});
 *   }
 * }}
 *</pre>
 */
public abstract class BytesToThriftTuple<T extends TBase<?>> extends EvalFunc<Tuple> {

  private final TDeserializer deserializer_ = new TDeserializer(new TBinaryProtocol.Factory());
  private ThriftToPig<T> thriftToTuple_;
  private TypeRef<T> typeRef_;

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called by the constructor!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<T> typeRef) {
    typeRef_ = typeRef;
    thriftToTuple_ = ThriftToPig.newInstance(typeRef);
  }


  @Override
  public Tuple exec(org.apache.pig.data.Tuple input) throws IOException {
    if (input == null || input.size() < 1) return null;
    try {
      T tObj = typeRef_.safeNewInstance();
      DataByteArray dbarr = (DataByteArray) input.get(0);
      deserializer_.deserialize(tObj, dbarr.get());
      return thriftToTuple_.getPigTuple(tObj);
    } catch (IOException e) {
      log.warn("Caught exception "+e.getMessage());
      return null;
    } catch (TException e) {
      log.warn("Unable to deserialize Thrift object: "+e);
      return null;
    }
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/examples/src/java/com/twitter/elephantbird/examples/ThriftMRExample.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/io/ProtobufPersonWritable.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/io/ProtobufAddressBookWritable.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/examples/src/gen-java/com/twitter/elephantbird/examples/thrift/AddressBook.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/examples/src/gen-java/com/twitter/elephantbird/examples/thrift/PhoneType.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/examples/src/gen-java/com/twitter/elephantbird/examples/thrift/Age.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/examples/src/gen-java/com/twitter/elephantbird/examples/thrift/Person.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/examples/src/gen-java/com/twitter/elephantbird/examples/thrift/PhoneNumber.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/util/HadoopUtils.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/util/ThriftUtils.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/util/Protobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoBinaryBlockRecordWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoBinaryB64LineRecordWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/output/LzoOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoBinaryBlockRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoGenericProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoBinaryB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftBlockRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ThriftConverter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ThriftBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/BinaryWritable.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/BinaryBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ThriftBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/BinaryBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufWritable.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufWritable.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufWritable.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufWritable.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufWritable.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufWritable.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ThriftWritable.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/BinaryConverter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufConverter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/store/LzoProtobufB64LinePigStorage.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/store/LzoProtobufB64LinePigStorage.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/util/ThiftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/util/ProtobufToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/util/ProtobufToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/util/PigCounterHelper.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/util/PigCounterHelper.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/util/PigCounterHelper.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/util/PigCounterHelper.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/util/PigCounterHelper.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/util/PigCounterHelper.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoThriftB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoThriftBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoBaseLoadFunc.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoBaseLoadFunc.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/piggybank/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/piggybank/BytesToThriftTuple.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/piggybank/BytesToThriftTuple.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/piggybank/BytesToThriftTuple.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/piggybank/BytesToThriftTuple.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/java/com/twitter/elephantbird/pig/piggybank/BytesToThriftTuple.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8eed34e_cebd3d0/rev_8eed34e-cebd3d0/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/java/com/twitter/elephantbird/examples/ProtobufMRExample.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoAddressBookProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoAddressBookProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoAddressBookProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoAddressBookProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoPersonProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoPersonProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoPersonProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoPersonProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoPersonProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoPersonProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoPersonProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoAddressBookProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoAddressBookProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoAddressBookProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoPersonProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoPersonProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoPersonProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoAddressBookProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoAddressBookProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoAddressBookProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/examples/src/gen-java/com/twitter/elephantbird/examples/proto/Examples.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/util/Protobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/util/Protobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/util/Protobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/util/Protobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufWritable.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufConverter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufConverter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufConverter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufBlockOutputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufBlockOutputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufBlockInputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufBlockInputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufBlockInputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufB64LineOutputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufB64LineOutputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufB64LineInputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufB64LineInputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufB64LineInputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/pig/piggybank/ProtobufBytesToTuple.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/pig/piggybank/ProtobufBytesToTuple.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/pig/piggybank/ProtobufBytesToTuple.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/java/com/twitter/elephantbird/pig/piggybank/ProtobufBytesToTuple.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/test/com/twitter/elephantbird/util/TestProtobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/test/com/twitter/elephantbird/util/TestProtobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/test/com/twitter/elephantbird/util/TestProtobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/test/com/twitter/elephantbird/util/TestProtobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/test/com/twitter/elephantbird/util/TestProtobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b0d7a3e_6abbab9/rev_b0d7a3e-6abbab9/src/test/com/twitter/elephantbird/util/TestProtobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_d5876bb_b813096/rev_d5876bb-b813096/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;<<<<<<< MINE

||||||| BASE
=======

import org.apache.hadoop.conf.Configuration;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_d5876bb_b813096/rev_d5876bb-b813096/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;<<<<<<< MINE
  @Override
  public RecordWriter<NullWritable, W> getRecordWriter(TaskAttemptContext job)
||||||| BASE
  public RecordWriter<NullWritable, W> getRecordWriter(TaskAttemptContext job)
=======
  public LzoProtobufBlockOutputFormat() {}

  /**
   * Returns {@link LzoProtobufB64LineOutputFormat} class.
   * Sets an internal configuration in jobConf so that remote Tasks
   * instantiate appropriate object for this generic class based on protoClass
   */
  @SuppressWarnings("unchecked")
  public static <M extends Message> Class<LzoProtobufBlockOutputFormat>
     getOutputFormatClass(Class<M> protoClass, Configuration jobConf) {

    Protobufs.setClassConf(jobConf, LzoProtobufBlockOutputFormat.class, protoClass);
    return LzoProtobufBlockOutputFormat.class;
  }

  public RecordWriter<NullWritable, ProtobufWritable<M>> getRecordWriter(TaskAttemptContext job)
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_d5876bb_b813096/rev_d5876bb-b813096/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;<<<<<<< MINE
||||||| BASE
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
=======

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_d5876bb_b813096/rev_d5876bb-b813096/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;<<<<<<< MINE
  @Override
  public RecordWriter<NullWritable, W> getRecordWriter(TaskAttemptContext job)
||||||| BASE
  public RecordWriter<NullWritable, W> getRecordWriter(TaskAttemptContext job)
=======
  public LzoProtobufB64LineOutputFormat() {}

  /**
   * Returns {@link LzoProtobufBlockOutputFormat} class.
   * Sets an internal configuration in jobConf so that remote Tasks
   * instantiate appropriate object for this generic class based on protoClass
   */
  @SuppressWarnings("unchecked")
  public static <M extends Message> Class<LzoProtobufB64LineOutputFormat>
     getOutputFormatClass(Class<M> protoClass, Configuration jobConf) {

    Protobufs.setClassConf(jobConf, LzoProtobufB64LineOutputFormat.class, protoClass);
    return LzoProtobufB64LineOutputFormat.class;
  }


  public RecordWriter<NullWritable, ProtobufWritable<M>> getRecordWriter(TaskAttemptContext job)
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_d5876bb_b813096/rev_d5876bb-b813096/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.load;

import java.io.IOException;

import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.Message;
import com.twitter.elephantbird.mapreduce.io.ProtobufBlockReader;
import com.twitter.elephantbird.mapreduce.io.ProtobufWritable;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;


public class LzoProtobufBlockPigLoader<M extends Message> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoProtobufBlockPigLoader.class);

  private ProtobufBlockReader<M> reader_ = null;
  private ProtobufWritable<M> value_ = null;
  private TypeRef<M> typeRef_ = null;
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  private Pair<String, String> protobufsRead;
  private Pair<String, String> protobufErrors;

  public LzoProtobufBlockPigLoader() {
    LOG.info("LzoProtobufBlockLoader zero-parameter creation");
  }

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called before getNext!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    value_ = new ProtobufWritable<M>(typeRef_);
    String group = "LzoBlocks of " + typeRef_.getRawClass().getName();
    protobufsRead = new Pair<String, String>(group, "Protobufs Read");
    protobufErrors = new Pair<String, String>(group, "Errors");
  }

  @Override
  public void postBind() throws IOException {
    reader_ = new ProtobufBlockReader<M>(is_, typeRef_);
  }

  @Override
  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // We want to explicitly not do any special syncing here, because the reader_
    // handles this automatically.
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }


  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    // If we are past the end of the file split, tell the reader not to read any more new blocks.
    // Then continue reading until the last of the reader's already-parsed values are used up.
    // The next split will start at the next sync point and no records will be missed.
    if (is_.getPosition() > end_) {
      reader_.markNoMoreNewBlocks();
    }

    Tuple t = null;
    if (reader_.readProtobuf(value_)) {
      if (value_.get() == null) {
        incrCounter(protobufErrors, 1);
      }
      t = new ProtobufTuple(value_.get());
      incrCounter(protobufsRead, 1L);
    }
    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}=======
package com.twitter.elephantbird.pig.load;

import java.io.IOException;

import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.Message;
import com.twitter.elephantbird.mapreduce.io.ProtobufBlockReader;
import com.twitter.elephantbird.mapreduce.io.ProtobufWritable;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;


public class LzoProtobufBlockPigLoader<M extends Message> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoProtobufBlockPigLoader.class);

  private ProtobufBlockReader<M> reader_ = null;
  private ProtobufWritable<M> value_ = null;
  private TypeRef<M> typeRef_ = null;
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  private Pair<String, String> protobufsRead;
  private Pair<String, String> protobufErrors;

  public LzoProtobufBlockPigLoader() {
    LOG.info("LzoProtobufBlockLoader zero-parameter creation");
  }

  public LzoProtobufBlockPigLoader(String protoClassName) {
    TypeRef<M> typeRef = Protobufs.getTypeRef(protoClassName);
    setTypeRef(typeRef);
    setLoaderSpec(getClass(), new String[]{protoClassName});
  }

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called before getNext!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    value_ = new ProtobufWritable<M>(typeRef_);
    String group = "LzoBlocks of " + typeRef_.getRawClass().getName();
    protobufsRead = new Pair<String, String>(group, "Protobufs Read");
    protobufErrors = new Pair<String, String>(group, "Errors");
  }

  @Override
  public void postBind() throws IOException {
    reader_ = new ProtobufBlockReader<M>(is_, typeRef_);
  }

  @Override
  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // We want to explicitly not do any special syncing here, because the reader_
    // handles this automatically.
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }


  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    // If we are past the end of the file split, tell the reader not to read any more new blocks.
    // Then continue reading until the last of the reader's already-parsed values are used up.
    // The next split will start at the next sync point and no records will be missed.
    if (is_.getPosition() > end_) {
      reader_.markNoMoreNewBlocks();
    }

    Tuple t = null;
    if (reader_.readProtobuf(value_)) {
      if (value_.get() == null) {
        incrCounter(protobufErrors, 1);
      }
      t = new ProtobufTuple(value_.get());
      incrCounter(protobufsRead, 1L);
    }
    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_d5876bb_b813096/rev_d5876bb-b813096/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.nio.charset.Charset;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.base.Function;
import com.google.protobuf.Message;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * This is the base class for all base64 encoded, line-oriented protocol buffer based pig loaders.
 * Data is expected to be one base64 encoded serialized protocol buffer per line. The specific
 * protocol buffer is a template parameter, generally specified by a codegen'd derived class.
 * See com.twitter.elephantbird.proto.HadoopProtoCodeGenerator.
 */

public abstract class LzoProtobufB64LinePigLoader<M extends Message> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoProtobufB64LinePigLoader.class);

  private TypeRef<M> typeRef_ = null;
  private Function<byte[], M> protoConverter_ = null;
  private final Base64 base64_ = new Base64();
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  private static final Charset UTF8 = Charset.forName("UTF-8");
  private static final byte RECORD_DELIMITER = (byte)'\n';

  private Pair<String, String> linesRead;
  private Pair<String, String> protobufsRead;
  private Pair<String, String> protobufErrors;

  public LzoProtobufB64LinePigLoader() {
    LOG.info("LzoProtobufB64LineLoader zero-parameter creation");
  }

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called before getNext!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    protoConverter_ = Protobufs.getProtoConverter(typeRef.getRawClass());
    String group = "LzoB64Lines of " + typeRef_.getRawClass().getName();
    linesRead = new Pair<String, String>(group, "Lines Read");
    protobufsRead = new Pair<String, String>(group, "Protobufs Read");
    protobufErrors = new Pair<String, String>(group, "Errors");
  }

  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // Since we are not block aligned we throw away the first record of each split and count on a different
    // instance to read it.  The only split this doesn't work for is the first.
    if (!atFirstRecord) {
      getNext();
    }
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    String line;
    Tuple t = null;
    while ((line = is_.readLine(UTF8, RECORD_DELIMITER)) != null) {
      incrCounter(linesRead, 1L);
      M protoValue = protoConverter_.apply(base64_.decode(line.getBytes("UTF-8")));
      if (protoValue != null) {
        t = new ProtobufTuple(protoValue);
        incrCounter(protobufsRead, 1L);
        break;
      } else {
        incrCounter(protobufErrors, 1L);
      }
    }

    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}=======
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.nio.charset.Charset;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.base.Function;
import com.google.protobuf.Message;
import com.twitter.elephantbird.mapreduce.io.ProtobufConverter;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * This is the base class for all base64 encoded, line-oriented protocol buffer based pig loaders.
 * Data is expected to be one base64 encoded serialized protocol buffer per line. The specific
 * protocol buffer is a template parameter, generally specified by a codegen'd derived class.
 * See com.twitter.elephantbird.proto.HadoopProtoCodeGenerator.
 */

public class LzoProtobufB64LinePigLoader<M extends Message> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoProtobufB64LinePigLoader.class);

  private TypeRef<M> typeRef_ = null;
  private ProtobufConverter<M> protoConverter_ = null;
  private final Base64 base64_ = new Base64();
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  private static final Charset UTF8 = Charset.forName("UTF-8");
  private static final byte RECORD_DELIMITER = (byte)'\n';

  private Pair<String, String> linesRead;
  private Pair<String, String> protobufsRead;
  private Pair<String, String> protobufErrors;

  public LzoProtobufB64LinePigLoader() {
    LOG.info("LzoProtobufB64LineLoader zero-parameter creation");
  }

  public LzoProtobufB64LinePigLoader(String protoClassName) {
    TypeRef<M> typeRef = Protobufs.getTypeRef(protoClassName);
    setTypeRef(typeRef);
    setLoaderSpec(getClass(), new String[]{protoClassName});
  }

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called before getNext!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    protoConverter_ = ProtobufConverter.newInstance(typeRef);
    String group = "LzoB64Lines of " + typeRef_.getRawClass().getName();
    linesRead = new Pair<String, String>(group, "Lines Read");
    protobufsRead = new Pair<String, String>(group, "Protobufs Read");
    protobufErrors = new Pair<String, String>(group, "Errors");
  }

  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // Since we are not block aligned we throw away the first record of each split and count on a different
    // instance to read it.  The only split this doesn't work for is the first.
    if (!atFirstRecord) {
      getNext();
    }
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    String line;
    Tuple t = null;
    while ((line = is_.readLine(UTF8, RECORD_DELIMITER)) != null) {
      incrCounter(linesRead, 1L);
      M protoValue = protoConverter_.fromBytes(base64_.decode(line.getBytes("UTF-8")));
      if (protoValue != null) {
        t = new ProtobufTuple(protoValue);
        incrCounter(protobufsRead, 1L);
        break;
      } else {
        incrCounter(protobufErrors, 1L);
      }
    }

    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_d5876bb_b813096/rev_d5876bb-b813096/src/java/com/twitter/elephantbird/pig/piggybank/ProtobufBytesToTuple.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.piggybank;

import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;

import com.google.common.base.Function;
import com.google.protobuf.Message;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * The base class for a Pig UDF that takes as input a tuple containing a single element, the
 * bytes of a serialized protocol buffer as a DataByteArray.  It outputs the protobuf in
 * expanded form.  The specific protocol buffer is a template parameter, generally specified by a
 * codegen'd derived class. See com.twitter.elephantbird.proto.HadoopProtoCodeGenerator.
 */
public abstract class ProtobufBytesToTuple<M extends Message> extends EvalFunc<Tuple> {
  private TypeRef<M> typeRef_ = null;
  private Function<byte[], M> protoConverter_ = null;
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  /**
   * Set the type parameter so it doesn't get erased by Java. Must be called during
   * initialization.
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    protoConverter_ = Protobufs.getProtoConverter(typeRef.getRawClass());
  }

  @Override
  public Tuple exec(Tuple input) throws IOException {
    if (input == null || input.size() < 1) return null;
    try {
      DataByteArray bytes = (DataByteArray) input.get(0);
      M value_ = protoConverter_.apply(bytes.get());
      return new ProtobufTuple(value_);
    } catch (IOException e) {
      return null;
    }
  }

  @Override
  public Schema outputSchema(Schema input) {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}=======
package com.twitter.elephantbird.pig.piggybank;

import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;

import com.google.common.base.Function;
import com.google.protobuf.Message;
import com.twitter.elephantbird.mapreduce.io.ProtobufConverter;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * The base class for a Pig UDF that takes as input a tuple containing a single element, the
 * bytes of a serialized protocol buffer as a DataByteArray.  It outputs the protobuf in
 * expanded form.  The specific protocol buffer is a template parameter, generally specified by a
 * codegen'd derived class. See com.twitter.elephantbird.proto.HadoopProtoCodeGenerator.
 */
public abstract class ProtobufBytesToTuple<M extends Message> extends EvalFunc<Tuple> {
  private TypeRef<M> typeRef_ = null;
  private ProtobufConverter<M> protoConverter_ = null;
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  /**
   * Set the type parameter so it doesn't get erased by Java. Must be called during
   * initialization.
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    protoConverter_ = ProtobufConverter.newInstance(typeRef);
  }

  @Override
  public Tuple exec(Tuple input) throws IOException {
    if (input == null || input.size() < 1) return null;
    try {
      DataByteArray bytes = (DataByteArray) input.get(0);
      M value_ = protoConverter_.fromBytes(bytes.get());
      return new ProtobufTuple(value_);
    } catch (IOException e) {
      return null;
    }
  }

  @Override
  public Schema outputSchema(Schema input) {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}>>>>>>> YOURS
/home/taes/taes/projects/jodd/revisions/rev_fa133ef_2e3ee1e/rev_fa133ef-2e3ee1e/jodd-core/src/main/java/jodd/util/Wildcard.java;null
/home/taes/taes/projects/jodd/revisions/rev_fa133ef_2e3ee1e/rev_fa133ef-2e3ee1e/jodd-core/src/main/java/jodd/util/Wildcard.java;null
/home/taes/taes/projects/jodd/revisions/rev_fa133ef_2e3ee1e/rev_fa133ef-2e3ee1e/jodd-core/src/main/java/jodd/util/Wildcard.java;null
/home/taes/taes/projects/jodd/revisions/rev_fa133ef_2e3ee1e/rev_fa133ef-2e3ee1e/jodd-core/src/main/java/jodd/util/Wildcard.java;null
/home/taes/taes/projects/jodd/revisions/rev_fa133ef_2e3ee1e/rev_fa133ef-2e3ee1e/jodd-core/src/main/java/jodd/util/Wildcard.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/java/com/twitter/elephantbird/examples/ProtobufMRExample.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoAddressBookProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoAddressBookProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoAddressBookProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoAddressBookProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoPersonProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoPersonProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoPersonProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/output/LzoPersonProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoPersonProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoPersonProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoPersonProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoAddressBookProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoAddressBookProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoAddressBookProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoPersonProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoPersonProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoPersonProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoAddressBookProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoAddressBookProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/mapreduce/input/LzoAddressBookProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/examples/src/gen-java/com/twitter/elephantbird/examples/proto/Examples.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/util/Protobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/util/Protobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/util/Protobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/util/Protobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufBlockOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufBlockRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufWritable.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufConverter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufConverter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufConverter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/mapreduce/io/ProtobufBlockReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufBlockOutputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufBlockOutputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufBlockInputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufBlockInputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufBlockInputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufB64LineOutputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufB64LineOutputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufB64LineInputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufB64LineInputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufB64LineInputFormatGenerator.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/pig/piggybank/ProtobufBytesToTuple.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/pig/piggybank/ProtobufBytesToTuple.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/pig/piggybank/ProtobufBytesToTuple.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/java/com/twitter/elephantbird/pig/piggybank/ProtobufBytesToTuple.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/test/com/twitter/elephantbird/util/TestProtobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/test/com/twitter/elephantbird/util/TestProtobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/test/com/twitter/elephantbird/util/TestProtobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/test/com/twitter/elephantbird/util/TestProtobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/test/com/twitter/elephantbird/util/TestProtobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_1416fa5_839a574/rev_1416fa5-839a574/src/test/com/twitter/elephantbird/util/TestProtobufs.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/TestUtils.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/ValidationConstraintContextTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/VtorExceptionTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/VtorMatchProfilesWithResetedProfilesTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/VtorTestSupport.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/VtorMatchProfilesTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/ViolationTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/ManualTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/MinLengthConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/AssertValidConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/NotNullConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/RangeConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/WildcardMatchConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/AssertTrueConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/EqualToDeclaredFieldConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/TimeBeforeConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/AssertFalseConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/LengthConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/ConstraintTestBase.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/WildcardPathMatchConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/HasSubstringConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/TimeAfterConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/NotBlankConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/MaxConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/MinConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/EqualToFieldConstraintTest.java;null
/home/taes/taes/projects/jodd/revisions/rev_7afb21f_a3241cf/rev_7afb21f-a3241cf/jodd-vtor/src/test/java/jodd/vtor/constraint/MaxLengthConstraintTest.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8d0b4a1_811f95b/rev_8d0b4a1-811f95b/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8d0b4a1_811f95b/rev_8d0b4a1-811f95b/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8d0b4a1_811f95b/rev_8d0b4a1-811f95b/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_8d0b4a1_811f95b/rev_8d0b4a1-811f95b/src/java/com/twitter/elephantbird/mapreduce/input/LzoProtobufB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_6c5529a_64bb16e/rev_6c5529a-64bb16e/src/java/com/twitter/elephantbird/pig/piggybank/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_6c5529a_64bb16e/rev_6c5529a-64bb16e/src/java/com/twitter/elephantbird/pig/piggybank/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_6c5529a_64bb16e/rev_6c5529a-64bb16e/src/java/com/twitter/elephantbird/pig/piggybank/ThriftToPig.java;<<<<<<< MINE

  private class TProtoForStruct extends ThriftProtocol {
    // essentially a hack to get to STRUCT_DESC in a Thrift class
    TStruct structDesc;
    @Override
    public void writeStructBegin(TStruct struct) throws TException {
      structDesc = struct;
      throw new TException("expected");
    }
  }

||||||| BASE

  private class TProtoForStruct extends ThriftProtocol {
    // essentially a hack to get to STRUCT_DESC in a Thrift class
    TStruct structDesc;
    public void writeStructBegin(TStruct struct) throws TException {
      structDesc = struct;
      throw new TException("expected");
    }
  }

=======
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;<<<<<<< MINE
public class LzoThriftB64LineOutputFormat<M extends TBase<?>>
    extends LzoOutputFormat<M, ThriftWritable<M>> {

||||||| BASE
public class LzoThriftB64LineOutputFormat<M extends TBase<?>>
    extends LzoOutputFormat<M, ThriftWritable<M>> {  
  
=======
public class LzoThriftB64LineOutputFormat<M extends TBase<?, ?>>
    extends LzoOutputFormat<M, ThriftWritable<M>> {

>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;<<<<<<< MINE

  @Override
||||||| BASE
  
=======

>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/mapreduce/output/LzoThriftB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;<<<<<<< MINE
public class LzoThriftB64LineInputFormat<M extends TBase<?>>
||||||| BASE
public class LzoThriftB64LineInputFormat<M extends TBase<?>>  
=======
public class LzoThriftB64LineInputFormat<M extends TBase<?, ?>>
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;<<<<<<< MINE

  TypeRef<M> typeRef_ = null;

||||||| BASE
  
=======

>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;<<<<<<< MINE

  public LzoThriftB64LineInputFormat(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
  }

||||||| BASE
  
=======

>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;<<<<<<< MINE

  public static<M extends TBase<?>> LzoThriftB64LineInputFormat<M> newInstance(TypeRef<M> typeRef) {
    return new LzoThriftB64LineInputFormat<M>(typeRef);
  }

||||||| BASE
  
=======

>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/mapreduce/input/LzoThriftB64LineInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/pig/load/LzoThriftB64LinePigLoader.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.nio.charset.Charset;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.apache.thrift.TBase;
import org.apache.thrift.TException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.twitter.elephantbird.mapreduce.io.ThriftConverter;
import com.twitter.elephantbird.pig.piggybank.ThriftToPig;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;

public class LzoThriftB64LinePigLoader<M extends TBase<?>> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoThriftB64LinePigLoader.class);

  private final TypeRef<M> typeRef_;
  private final ThriftConverter<M> converter_;
  private final Base64 base64_ = new Base64();
  private final ThriftToPig<M> thriftToPig_;

  private static final Charset UTF8 = Charset.forName("UTF-8");
  private static final byte RECORD_DELIMITER = (byte)'\n';

  private Pair<String, String> linesRead;
  private Pair<String, String> thriftStructsRead;
  private Pair<String, String> thriftErrors;

  public LzoThriftB64LinePigLoader(String thriftClassName) {
    typeRef_ = ThriftUtils.getTypeRef(thriftClassName);
    converter_ = ThriftConverter.newInstance(typeRef_);
    thriftToPig_ =  ThriftToPig.newInstance(typeRef_);

    String group = "LzoB64Lines of " + typeRef_.getRawClass().getName();
    linesRead = new Pair<String, String>(group, "Lines Read");
    thriftStructsRead = new Pair<String, String>(group, "Thrift Structs");
    thriftErrors = new Pair<String, String>(group, "Errors");

    setLoaderSpec(getClass(), new String[]{thriftClassName});
  }

  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // Since we are not block aligned we throw away the first record of each split and count on a different
    // instance to read it.  The only split this doesn't work for is the first.
    if (!atFirstRecord) {
      getNext();
    }
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    String line;
    Tuple t = null;
    while ((line = is_.readLine(UTF8, RECORD_DELIMITER)) != null) {
      incrCounter(linesRead, 1L);
      M value = converter_.fromBytes(base64_.decode(line.getBytes("UTF-8")));
      if (value != null) {
        try {
          t = thriftToPig_.getPigTuple(value);
          incrCounter(thriftStructsRead, 1L);
          break;
        } catch (TException e) {
          incrCounter(thriftErrors, 1L);
          LOG.warn("ThriftToTuple error :", e); // may be struct mismatch
        }
      }
    }

    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return ThriftToPig.toSchema(typeRef_.getRawClass());
  }
}=======
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.nio.charset.Charset;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.apache.thrift.TBase;
import org.apache.thrift.TException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.twitter.elephantbird.mapreduce.io.ThriftConverter;
import com.twitter.elephantbird.pig.piggybank.ThriftToPig;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;

public class LzoThriftB64LinePigLoader<M extends TBase<?, ?>> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoThriftB64LinePigLoader.class);

  private final TypeRef<M> typeRef_;
  private final ThriftConverter<M> converter_;
  private final Base64 base64_ = new Base64();
  private final ThriftToPig<M> thriftToPig_;

  private static final Charset UTF8 = Charset.forName("UTF-8");
  private static final byte RECORD_DELIMITER = (byte)'\n';

  private Pair<String, String> linesRead;
  private Pair<String, String> thriftStructsRead;
  private Pair<String, String> thriftErrors;

  public LzoThriftB64LinePigLoader(String thriftClassName) {
    typeRef_ = ThriftUtils.getTypeRef(thriftClassName);
    converter_ = ThriftConverter.newInstance(typeRef_);
    thriftToPig_ =  ThriftToPig.newInstance(typeRef_);

    String group = "LzoB64Lines of " + typeRef_.getRawClass().getName();
    linesRead = new Pair<String, String>(group, "Lines Read");
    thriftStructsRead = new Pair<String, String>(group, "Thrift Structs");
    thriftErrors = new Pair<String, String>(group, "Errors");

    setLoaderSpec(getClass(), new String[]{thriftClassName});
  }

  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // Since we are not block aligned we throw away the first record of each split and count on a different
    // instance to read it.  The only split this doesn't work for is the first.
    if (!atFirstRecord) {
      getNext();
    }
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    String line;
    Tuple t = null;
    while ((line = is_.readLine(UTF8, RECORD_DELIMITER)) != null) {
      incrCounter(linesRead, 1L);
      M value = converter_.fromBytes(base64_.decode(line.getBytes("UTF-8")));
      if (value != null) {
        try {
          t = thriftToPig_.getPigTuple(value);
          incrCounter(thriftStructsRead, 1L);
          break;
        } catch (TException e) {
          incrCounter(thriftErrors, 1L);
          LOG.warn("ThriftToTuple error :", e); // may be struct mismatch
        }
      }
    }

    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return ThriftToPig.toSchema(typeRef_.getRawClass());
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/pig/load/LzoThriftBlockPigLoader.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.load;

import java.io.IOException;

import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.apache.thrift.TBase;
import org.apache.thrift.TException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.twitter.elephantbird.mapreduce.io.ThriftBlockReader;
import com.twitter.elephantbird.pig.piggybank.ThriftToPig;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;


public class LzoThriftBlockPigLoader<M extends TBase<?>> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoThriftBlockPigLoader.class);

  private final TypeRef<M> typeRef_;
  private final ThriftToPig<M> thriftToPig_;
  private ThriftBlockReader<M> reader_;

  private Pair<String, String> thriftStructsRead;
  private Pair<String, String> thriftErrors;

  public LzoThriftBlockPigLoader(String thriftClassName) {
    typeRef_ = ThriftUtils.getTypeRef(thriftClassName);
    thriftToPig_ =  ThriftToPig.newInstance(typeRef_);

    String group = "LzoBlocks of " + typeRef_.getRawClass().getName();
    thriftStructsRead = new Pair<String, String>(group, "Thrift Structs Read");
    thriftErrors = new Pair<String, String>(group, "Errors");

    setLoaderSpec(getClass(), new String[]{thriftClassName});
  }

  @Override
  public void postBind() throws IOException {
    reader_ = new ThriftBlockReader<M>(is_, typeRef_);
  }

  @Override
  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // We want to explicitly not do any special syncing here, because the reader_
    // handles this automatically.
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    // If we are past the end of the file split, tell the reader not to read any more new blocks.
    // Then continue reading until the last of the reader's already-parsed values are used up.
    // The next split will start at the next sync point and no records will be missed.
    if (is_.getPosition() > end_) {
      reader_.markNoMoreNewBlocks();
    }

    M value;
    while ((value = reader_.readNext()) != null) {
      try {
        Tuple t = thriftToPig_.getPigTuple(value);
        incrCounter(thriftStructsRead, 1L);
        return t;
      } catch (TException e) {
        incrCounter(thriftErrors, 1L);
        LOG.warn("ThriftToTuple error :", e); // may be corrupt data.
        // try next
      }
    }
    return null;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return ThriftToPig.toSchema(typeRef_.getRawClass());
  }
}=======
package com.twitter.elephantbird.pig.load;

import java.io.IOException;

import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.apache.thrift.TBase;
import org.apache.thrift.TException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.twitter.elephantbird.mapreduce.io.ThriftBlockReader;
import com.twitter.elephantbird.pig.piggybank.ThriftToPig;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;


public class LzoThriftBlockPigLoader<M extends TBase<?, ?>> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoThriftBlockPigLoader.class);

  private final TypeRef<M> typeRef_;
  private final ThriftToPig<M> thriftToPig_;
  private ThriftBlockReader<M> reader_;

  private Pair<String, String> thriftStructsRead;
  private Pair<String, String> thriftErrors;

  public LzoThriftBlockPigLoader(String thriftClassName) {
    typeRef_ = ThriftUtils.getTypeRef(thriftClassName);
    thriftToPig_ =  ThriftToPig.newInstance(typeRef_);

    String group = "LzoBlocks of " + typeRef_.getRawClass().getName();
    thriftStructsRead = new Pair<String, String>(group, "Thrift Structs Read");
    thriftErrors = new Pair<String, String>(group, "Errors");

    setLoaderSpec(getClass(), new String[]{thriftClassName});
  }

  @Override
  public void postBind() throws IOException {
    reader_ = new ThriftBlockReader<M>(is_, typeRef_);
  }

  @Override
  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // We want to explicitly not do any special syncing here, because the reader_
    // handles this automatically.
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    // If we are past the end of the file split, tell the reader not to read any more new blocks.
    // Then continue reading until the last of the reader's already-parsed values are used up.
    // The next split will start at the next sync point and no records will be missed.
    if (is_.getPosition() > end_) {
      reader_.markNoMoreNewBlocks();
    }

    M value;
    while ((value = reader_.readNext()) != null) {
      try {
        Tuple t = thriftToPig_.getPigTuple(value);
        incrCounter(thriftStructsRead, 1L);
        return t;
      } catch (TException e) {
        incrCounter(thriftErrors, 1L);
        LOG.warn("ThriftToTuple error :", e); // may be corrupt data.
        // try next
      }
    }
    return null;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return ThriftToPig.toSchema(typeRef_.getRawClass());
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/pig/load/HBaseSlice.java;<<<<<<< MINE
||||||| BASE
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with this
 * work for additional information regarding copyright ownership. The ASF
 * licenses this file to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * 
 * http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.math.BigDecimal;
import java.math.BigInteger;
import java.util.ArrayList;
import java.util.Map;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.UnknownScannerException;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.filter.BinaryComparator;
import org.apache.hadoop.hbase.filter.CompareFilter;
import org.apache.hadoop.hbase.filter.FilterList;
import org.apache.hadoop.hbase.filter.RowFilter;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.util.StringUtils;
import org.apache.pig.Slice;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;

import com.google.common.collect.Maps;
import com.twitter.elephantbird.pig.util.PigCounterHelper;

/**
 * HBase Slice to load a portion of range of a table. The key range will be
 * [start, end) Modeled from org.apache.hadoop.hbase.mapred.TableSplit.
 */
public class HBaseSlice implements Slice {

  /** A Generated Serial Version UID **/
  private static final long serialVersionUID = 9035916017187148965L;
  private static final Log LOG = LogFactory.getLog(HBaseSlice.class);
  private transient PigCounterHelper counterHelper_;

  // assigned during construction
  /** Table Name **/
  private final byte[] tableName_;
  /** Table Start Row **/
  private final byte[] startRow_;
  /** Table End Row **/
  private final byte[] endRow_;
  /** Table Region Location **/
  private final String regionLocation_;
  /** Input Columns **/
  private final byte[][] inputColumns_;
  /** Whether the row should be loaded **/
  private final boolean loadRowKey_;

  /** BigInteger representations of row range */
  private final BigInteger bigStart_;
  private final BigInteger bigEnd_;
  private final BigDecimal bigRange_;


  private Map<CompareFilter.CompareOp, String> innerFilters_ = Maps.newHashMap();
  private long limit_ = -1;


  // created as part of init
  /** The connection to the table in Hbase **/
  private transient HTable m_table;
  /** The scanner over the table **/
  private transient ResultScanner m_scanner;
  private transient long seenRows_ = 0;

  private transient ArrayList<Object> mProtoTuple;

  /**
   * Record the last processed row, so that we can restart the scanner when an
   * exception happened during scanning a table
   */
  private transient byte[] m_lastRow_;

  /**
   * Constructor
   * 
   * @param tableName
   *            table name
   * @param startRow
   *            start now, inclusive
   * @param endRow
   *            end row, exclusive
   * @param inputColumns
   *            input columns
   * @param location
   *            region location
   */
  public HBaseSlice(byte[] tableName, byte[] startRow, byte[] endRow,
      byte[][] inputColumns, boolean loadRowKey, final String location) {
    tableName_ = tableName;
    startRow_ = startRow;
    endRow_ = endRow;
    inputColumns_ = inputColumns;
    regionLocation_ = location;
    loadRowKey_ = loadRowKey;

    // We have to deal with different byte lengths of keys producing very different
    // BigIntegers (bigendianness is great this way). The code is mostly cribbed
    // from HBase's Bytes class.
    byte [] startPadded;
    byte [] endPadded;
    if (startRow.length < endRow.length) {
      startPadded = Bytes.padTail(startRow, endRow.length - startRow.length);
      endPadded = endRow;
    } else if (endRow.length < startRow.length) {
      startPadded = startRow;
      endPadded = Bytes.padTail(endRow, startRow.length - endRow.length);
    } else {
      startPadded = startRow;
      endPadded = endRow;
    }
    byte [] prependHeader = {1, 0};
    bigStart_ = new BigInteger(Bytes.add(prependHeader, startPadded));
    bigEnd_ = new BigInteger(Bytes.add(prependHeader, endPadded));
    bigRange_ = new BigDecimal(bigEnd_.subtract(bigStart_));
  }

  public void addFilter(CompareFilter.CompareOp compareOp, String filterValue) {
    innerFilters_.put(compareOp, filterValue);
  }

  /** @return table name */
  public byte[] getTableName() {
    return this.tableName_;
  }

  /** @return starting row key */
  public byte[] getStartRow() {
    return this.startRow_;
  }

  /** @return end row key */
  public byte[] getEndRow() {
    return this.endRow_;
  }

  /** @return input columns */
  public byte[][] getInputColumns() {
    return this.inputColumns_;
  }

  /** @return the region's hostname */
  public String getRegionLocation() {
    return this.regionLocation_;
  }

  @Override
  public long getStart() {
    // Not clear how to obtain this in a table...
    return 0;
  }

  @Override
  public long getLength() {
    // Not clear how to obtain this in a table...
    // it seems to be used only for sorting splits
    return 0;
  }

  @Override
  public String[] getLocations() {
    return new String[] { regionLocation_ };
  }

  @Override
  public long getPos() throws IOException {
    // This should be the ordinal tuple in the range;
    // not clear how to calculate...
    return 0;
  }

  @Override
  public float getProgress() throws IOException {

    // No way to know max.. just return 0. Sorry, reporting on the last slice is janky.
    // So is reporting on the first slice, by the way -- it will start out too high, possibly at 100%.
    if (endRow_.length==0) return 0;
    byte[] lastPadded = m_lastRow_;
    if (m_lastRow_.length < endRow_.length) {
      lastPadded = Bytes.padTail(m_lastRow_, endRow_.length - m_lastRow_.length);
    }
    if (m_lastRow_.length < startRow_.length) {
      lastPadded = Bytes.padTail(m_lastRow_, startRow_.length - m_lastRow_.length);
    }
    byte [] prependHeader = {1, 0};
    BigInteger bigLastRow = new BigInteger(Bytes.add(prependHeader, lastPadded));
    BigDecimal processed = new BigDecimal(bigLastRow.subtract(bigStart_));
    try {
      BigDecimal progress = processed.setScale(3).divide(bigRange_, BigDecimal.ROUND_HALF_DOWN);
      return progress.floatValue();
    } catch (java.lang.ArithmeticException e) {
      return 0;
    }
  }

  @Override
  public void init(DataStorage store) throws IOException {
    HBaseConfiguration conf = new HBaseConfiguration();
    // connect to the given table
    m_table = new HTable(conf, tableName_);
    // init the scanner
    initScanner();
  }

  /**
   * Init the table scanner
   * 
   * @throws IOException
   */
  private void initScanner() throws IOException {
    restart(startRow_);
    m_lastRow_ = startRow_;
  }

  /**
   * Restart scanning from survivable exceptions by creating a new scanner.
   * 
   * @param startRow
   *            the start row
   * @throws IOException
   */
  private void restart(byte[] startRow) throws IOException {
    Scan scan;
    if ((endRow_ != null) && (endRow_.length > 0)) {
      scan = new Scan(startRow, endRow_);
    } else {
      scan = new Scan(startRow);
    }

    // Set filters, if any.
    FilterList scanFilter = null;
    if (!innerFilters_.isEmpty()) {
      scanFilter = new FilterList();
      for (Map.Entry<CompareFilter.CompareOp, String>entry  : innerFilters_.entrySet()) {
        scanFilter.addFilter(new RowFilter(entry.getKey(), new BinaryComparator(Bytes.toBytesBinary(entry.getValue()) )));
      }
      scan.setFilter(scanFilter);
    }

    scan.addColumns(inputColumns_);
    this.m_scanner = this.m_table.getScanner(scan);
  }

  @Override
  public boolean next(Tuple value) throws IOException {
    Result result;
    try {
      result = m_scanner.next();
    } catch (UnknownScannerException e) {
      LOG.info("recovered from " + StringUtils.stringifyException(e));
      restart(m_lastRow_);
      if (m_lastRow_ != startRow_) {
        m_scanner.next(); // skip presumed already mapped row
      }
      result = this.m_scanner.next();
    }
    boolean hasMore = result != null && result.size() > 0 && (limit_ < 0 || limit_ > seenRows_);
    if (hasMore) {
      if (counterHelper_ == null) counterHelper_ = new PigCounterHelper();
      counterHelper_.incrCounter(HBaseSlice.class.getName(), Bytes.toString(tableName_) + " rows read", 1);
      m_lastRow_ = result.getRow();
      convertResultToTuple(result, value);
      seenRows_ += 1;
    }
    return hasMore;
  }

  /**
   * Convert a row result to a tuple
   * 
   * @param result
   *            row result
   * @param tuple
   *            tuple
   */
  private void convertResultToTuple(Result result, Tuple tuple) {
    if (mProtoTuple == null)
      mProtoTuple = new ArrayList<Object>(inputColumns_.length + (loadRowKey_ ? 1 : 0));

    if (loadRowKey_) {
      mProtoTuple.add(new DataByteArray(result.getRow()));
    }

    for (byte[] column : inputColumns_) {
      byte[] value = result.getValue(column);
      if (value == null) {
        mProtoTuple.add(null);
      } else {
        mProtoTuple.add(new DataByteArray(value));
      }
    }

    Tuple newT = TupleFactory.getInstance().newTuple(mProtoTuple);
    mProtoTuple.clear();
    tuple.reference(newT);
  }

  @Override
  public void close() throws IOException {
    if (m_scanner != null) {
      m_scanner.close();
      m_scanner = null;
    }
  }

  @Override
  public String toString() {
    return regionLocation_ + ":" + Bytes.toString(startRow_) + ","
    + Bytes.toString(endRow_);
  }

  public void setLimit(String limit) {
    LOG.info("Setting Slice limit to "+Long.valueOf(limit));
    limit_ = Long.valueOf(limit);
  }

}=======
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with this
 * work for additional information regarding copyright ownership. The ASF
 * licenses this file to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.math.BigDecimal;
import java.math.BigInteger;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.UnknownScannerException;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.filter.BinaryComparator;
import org.apache.hadoop.hbase.filter.CompareFilter;
import org.apache.hadoop.hbase.filter.FilterList;
import org.apache.hadoop.hbase.filter.RowFilter;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.util.StringUtils;
import org.apache.pig.Slice;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;

import com.google.common.collect.Maps;
import com.twitter.elephantbird.pig.util.PigCounterHelper;

/**
 * HBase Slice to load a portion of range of a table. The key range will be
 * [start, end) Modeled from org.apache.hadoop.hbase.mapred.TableSplit.
 */
public class HBaseSlice implements Slice {

  /** A Generated Serial Version UID **/
  private static final long serialVersionUID = 9035916017187148965L;
  private static final Log LOG = LogFactory.getLog(HBaseSlice.class);
  private transient PigCounterHelper counterHelper_;

  // assigned during construction
  /** Table Name **/
  private final byte[] tableName_;
  /** Table Start Row **/
  private final byte[] startRow_;
  /** Table End Row **/
  private final byte[] endRow_;
  /** Table Region Location **/
  private final String regionLocation_;
  /** Input Columns **/
  private final List<byte[][]> inputColumns_;
  /** Whether the row should be loaded **/
  private final boolean loadRowKey_;

  /** BigInteger representations of row range */
  private final BigInteger bigStart_;
  private final BigInteger bigEnd_;
  private final BigDecimal bigRange_;


  private Map<CompareFilter.CompareOp, String> innerFilters_ = Maps.newHashMap();
  private long limit_ = -1;


  // created as part of init
  /** The connection to the table in Hbase **/
  private transient HTable m_table;
  /** The scanner over the table **/
  private transient ResultScanner m_scanner;
  private transient long seenRows_ = 0;

  private transient ArrayList<Object> mProtoTuple;

  /**
   * Record the last processed row, so that we can restart the scanner when an
   * exception happened during scanning a table
   */
  private transient byte[] m_lastRow_;

  /**
   * Constructor
   *
   * @param tableName
   *            table name
   * @param startRow
   *            start now, inclusive
   * @param endRow
   *            end row, exclusive
   * @param inputColumns
   *            input columns
   * @param location
   *            region location
   */
  public HBaseSlice(byte[] tableName, byte[] startRow, byte[] endRow,
      List<byte[][]> inputColumns, boolean loadRowKey, final String location) {
    tableName_ = tableName;
    startRow_ = startRow;
    endRow_ = endRow;
    inputColumns_ = inputColumns;
    regionLocation_ = location;
    loadRowKey_ = loadRowKey;

    // We have to deal with different byte lengths of keys producing very different
    // BigIntegers (bigendianness is great this way). The code is mostly cribbed
    // from HBase's Bytes class.
    byte [] startPadded;
    byte [] endPadded;
    if (startRow.length < endRow.length) {
      startPadded = Bytes.padTail(startRow, endRow.length - startRow.length);
      endPadded = endRow;
    } else if (endRow.length < startRow.length) {
      startPadded = startRow;
      endPadded = Bytes.padTail(endRow, startRow.length - endRow.length);
    } else {
      startPadded = startRow;
      endPadded = endRow;
    }
    byte [] prependHeader = {1, 0};
    bigStart_ = new BigInteger(Bytes.add(prependHeader, startPadded));
    bigEnd_ = new BigInteger(Bytes.add(prependHeader, endPadded));
    bigRange_ = new BigDecimal(bigEnd_.subtract(bigStart_));
  }

  public void addFilter(CompareFilter.CompareOp compareOp, String filterValue) {
    innerFilters_.put(compareOp, filterValue);
  }

  /** @return table name */
  public byte[] getTableName() {
    return this.tableName_;
  }

  /** @return starting row key */
  public byte[] getStartRow() {
    return this.startRow_;
  }

  /** @return end row key */
  public byte[] getEndRow() {
    return this.endRow_;
  }

  /** @return input columns */
  public List<byte[][]> getInputColumns() {
    return this.inputColumns_;
  }

  /** @return the region's hostname */
  public String getRegionLocation() {
    return this.regionLocation_;
  }

  @Override
  public long getStart() {
    // Not clear how to obtain this in a table...
    return 0;
  }

  @Override
  public long getLength() {
    // Not clear how to obtain this in a table...
    // it seems to be used only for sorting splits
    return 0;
  }

  @Override
  public String[] getLocations() {
    return new String[] { regionLocation_ };
  }

  @Override
  public long getPos() throws IOException {
    // This should be the ordinal tuple in the range;
    // not clear how to calculate...
    return 0;
  }

  @Override
  public float getProgress() throws IOException {

    // No way to know max.. just return 0. Sorry, reporting on the last slice is janky.
    // So is reporting on the first slice, by the way -- it will start out too high, possibly at 100%.
    if (endRow_.length==0) return 0;
    byte[] lastPadded = m_lastRow_;
    if (m_lastRow_.length < endRow_.length) {
      lastPadded = Bytes.padTail(m_lastRow_, endRow_.length - m_lastRow_.length);
    }
    if (m_lastRow_.length < startRow_.length) {
      lastPadded = Bytes.padTail(m_lastRow_, startRow_.length - m_lastRow_.length);
    }
    byte [] prependHeader = {1, 0};
    BigInteger bigLastRow = new BigInteger(Bytes.add(prependHeader, lastPadded));
    BigDecimal processed = new BigDecimal(bigLastRow.subtract(bigStart_));
    try {
      BigDecimal progress = processed.setScale(3).divide(bigRange_, BigDecimal.ROUND_HALF_DOWN);
      return progress.floatValue();
    } catch (java.lang.ArithmeticException e) {
      return 0;
    }
  }

  @Override
  public void init(DataStorage store) throws IOException {
    Configuration conf = HBaseConfiguration.create();
    // connect to the given table
    m_table = new HTable(conf, tableName_);
    // init the scanner
    initScanner();
  }

  /**
   * Init the table scanner
   *
   * @throws IOException
   */
  private void initScanner() throws IOException {
    restart(startRow_);
    m_lastRow_ = startRow_;
  }

  /**
   * Restart scanning from survivable exceptions by creating a new scanner.
   *
   * @param startRow
   *            the start row
   * @throws IOException
   */
  private void restart(byte[] startRow) throws IOException {
    Scan scan;
    if ((endRow_ != null) && (endRow_.length > 0)) {
      scan = new Scan(startRow, endRow_);
    } else {
      scan = new Scan(startRow);
    }

    // Set filters, if any.
    FilterList scanFilter = null;
    if (!innerFilters_.isEmpty()) {
      scanFilter = new FilterList();
      for (Map.Entry<CompareFilter.CompareOp, String>entry  : innerFilters_.entrySet()) {
        scanFilter.addFilter(new RowFilter(entry.getKey(), new BinaryComparator(Bytes.toBytesBinary(entry.getValue()) )));
      }
      scan.setFilter(scanFilter);
    }

    for (byte[][] col : inputColumns_) {
      scan.addColumn(col[0], col[1]);
    }
    this.m_scanner = this.m_table.getScanner(scan);
  }

  @Override
  public boolean next(Tuple value) throws IOException {
    Result result;
    try {
      result = m_scanner.next();
    } catch (UnknownScannerException e) {
      LOG.info("recovered from " + StringUtils.stringifyException(e));
      restart(m_lastRow_);
      if (m_lastRow_ != startRow_) {
        m_scanner.next(); // skip presumed already mapped row
      }
      result = this.m_scanner.next();
    }
    boolean hasMore = result != null && result.size() > 0 && (limit_ < 0 || limit_ > seenRows_);
    if (hasMore) {
      if (counterHelper_ == null) counterHelper_ = new PigCounterHelper();
      counterHelper_.incrCounter(HBaseSlice.class.getName(), Bytes.toString(tableName_) + " rows read", 1);
      m_lastRow_ = result.getRow();
      convertResultToTuple(result, value);
      seenRows_ += 1;
    }
    return hasMore;
  }

  /**
   * Convert a row result to a tuple
   *
   * @param result
   *            row result
   * @param tuple
   *            tuple
   */
  private void convertResultToTuple(Result result, Tuple tuple) {
    if (mProtoTuple == null)
      mProtoTuple = new ArrayList<Object>(inputColumns_.size() + (loadRowKey_ ? 1 : 0));

    if (loadRowKey_) {
      mProtoTuple.add(new DataByteArray(result.getRow()));
    }

    for (byte[][] column : inputColumns_) {
      byte[] value = result.getValue(column[0], column[1]);
      if (value == null) {
        mProtoTuple.add(null);
      } else {
        mProtoTuple.add(new DataByteArray(value));
      }
    }

    Tuple newT = TupleFactory.getInstance().newTuple(mProtoTuple);
    mProtoTuple.clear();
    tuple.reference(newT);
  }

  @Override
  public void close() throws IOException {
    if (m_scanner != null) {
      m_scanner.close();
      m_scanner = null;
    }
  }

  @Override
  public String toString() {
    return regionLocation_ + ":" + Bytes.toString(startRow_) + ","
    + Bytes.toString(endRow_);
  }

  public void setLimit(String limit) {
    LOG.info("Setting Slice limit to "+Long.valueOf(limit));
    limit_ = Long.valueOf(limit);
  }

}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/pig/load/HBaseLoader.java;<<<<<<< MINE
||||||| BASE
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with this
 * work for additional information regarding copyright ownership. The ASF
 * licenses this file to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * 
 * http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.util.List;
import java.util.Map;

import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.CommandLineParser;
import org.apache.commons.cli.GnuParser;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HConstants;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.filter.BinaryComparator;
import org.apache.hadoop.hbase.filter.RowFilter;
import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.pig.ExecType;
import org.apache.pig.LoadFunc;
import org.apache.pig.Slice;
import org.apache.pig.Slicer;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.io.BufferedPositionedInputStream;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;

import com.google.common.collect.Lists;

/**
 * A <code>Slicer</code> that splits the hbase table into {@link HBaseSlice}s.
 * Actual loading is done in {@link HBaseSlice}. Derived from the HbaseStorage implementation
 * in 0.6 Piggybank.
 * <br>
 * TODO: row version controls<br>
 */
public class HBaseLoader implements Slicer,
LoadFunc {

  private static final Log LOG = LogFactory.getLog(HBaseLoader.class);


  private byte[][] cols_;
  private HTable table_;
  private final HBaseConfiguration conf_;
  private final boolean loadRowKey_;
  private final CommandLine configuredOptions_;
  private final static Options validOptions_ = new Options();
  private final static CommandLineParser parser_ = new GnuParser();

  private static void populateValidOptions() { 
    validOptions_.addOption("loadKey", false, "Load Key");
    validOptions_.addOption("gt", true, "Records must be greater than this value (binary, double-slash-escaped)");
    validOptions_.addOption("lt", true, "Records must be less than this value (binary, double-slash-escaped)");   
    validOptions_.addOption("gte", true, "Records must be greater than or equal to this value");
    validOptions_.addOption("lte", true, "Records must be less than or equal to this value");
    validOptions_.addOption("caching", true, "Number of rows scanners should cache");
    validOptions_.addOption("limit", true, "Per-region limit");
  }

  /**
   * Constructor. Construct a HBase Table loader to load the cells of the
   * provided columns.
   * 
   * @param columnList
   *            columnlist that is a presented string delimited by space.
   * @throws ParseException 
   */
  public HBaseLoader(String columnList) throws ParseException {
    this(columnList, "");
    LOG.info("no-arg constructor");
  }

  /**
   * 
   * @param columnList
   * @param optString Loader options. Known options:<ul>
   * <li>-loadKey=(true|false)  Load the row key as the first column
   * <li>-gt=minKeyVal
   * <li>-lt=maxKeyVal 
   * <li>-gte=minKeyVal
   * <li>-lte=maxKeyVal
   * <li>-caching=numRows  number of rows to cache (faster scans, more memory).
   * </ul>
   * @throws ParseException 
   */
  public HBaseLoader(String columnList, String optString) throws ParseException {
    populateValidOptions();
    String[] colNames = columnList.split(" ");
    String[] optsArr = optString.split(" ");
    try {
      configuredOptions_ = parser_.parse(validOptions_, optsArr);
    } catch (ParseException e) {
      HelpFormatter formatter = new HelpFormatter();
      formatter.printHelp( "", validOptions_ );
      throw e;
    }
    loadRowKey_ = configuredOptions_.hasOption("loadKey");  
    cols_ = new byte[colNames.length][];
    for (int i = 0; i < cols_.length; i++) {
      cols_[i] = Bytes.toBytes(colNames[i]);
    }

    conf_ = new HBaseConfiguration();
  }

  @Override
  public Slice[] slice(DataStorage store, String tablename)
  throws IOException {
    validate(store, tablename);
    if (configuredOptions_.hasOption("caching")) {
      table_.setScannerCaching(Integer.valueOf(configuredOptions_.getOptionValue("caching")));
    }
    
    byte[][] startKeys = table_.getStartKeys();
    if (startKeys == null || startKeys.length == 0) {
      throw new IOException("Expecting at least one region");
    }
    if (cols_ == null || cols_.length == 0) {
      throw new IOException("Expecting at least one column");
    }

    // one region one slice
    List<HBaseSlice> slices = Lists.newArrayList();
    for (int i = 0; i < startKeys.length; i++) {
      
      byte[] endKey = ((i + 1) < startKeys.length) ? startKeys[i + 1] : HConstants.LAST_ROW;
      
      // skip if the region doesn't satisfy configured options
      if ((skipRegion(CompareOp.LESS, startKeys[i], configuredOptions_.getOptionValue("lt"))) ||
          (skipRegion(CompareOp.GREATER, endKey, configuredOptions_.getOptionValue("gt"))) ||
          (skipRegion(CompareOp.GREATER, endKey, configuredOptions_.getOptionValue("gte"))) ||
          (skipRegion(CompareOp.LESS_OR_EQUAL, startKeys[i], configuredOptions_.getOptionValue("lte")))) {
        continue;
      }
      String regionLocation = table_.getRegionLocation(startKeys[i]).getServerAddress().getHostname();
      HBaseSlice slice = new HBaseSlice(table_.getTableName(), startKeys[i],
          endKey, cols_, loadRowKey_, regionLocation);

      if (configuredOptions_.hasOption("limit")) slice.setLimit(configuredOptions_.getOptionValue("limit"));
      if (configuredOptions_.hasOption("gt")) slice.addFilter(CompareOp.GREATER, slashisize(configuredOptions_.getOptionValue("gt")));
      if (configuredOptions_.hasOption("lt")) slice.addFilter(CompareOp.LESS, slashisize(configuredOptions_.getOptionValue("lt")));
      if (configuredOptions_.hasOption("gte")) slice.addFilter(CompareOp.GREATER_OR_EQUAL, slashisize(configuredOptions_.getOptionValue("gte")));
      if (configuredOptions_.hasOption("lte")) slice.addFilter(CompareOp.LESS_OR_EQUAL, slashisize(configuredOptions_.getOptionValue("lte")));
      slices.add(slice);
    }

    return slices.toArray(new HBaseSlice[] {});
  }

  private boolean skipRegion(CompareOp op, byte[] key, String option ) {
    if (option == null) return false;
    BinaryComparator comp = new BinaryComparator(Bytes.toBytesBinary(slashisize(option)));
    RowFilter rowFilter = new RowFilter(op, comp);
    return rowFilter.filterRowKey(key, 0, key.length);
  }

  /**
   * replace sequences of two slashes ("\\") with one slash ("\")
   * (not escaping a slash in grunt is disallowed, but a double slash doesn't get converted
   * into a regular slash, so we have to do it instead)
   * @param str
   * @return
   */
  private String slashisize(String str) {
    return str.replace("\\\\", "\\");
  }

  @Override
  public void validate(DataStorage store, String tablename)
  throws IOException {
    ensureTable(tablename);
  }

  private void ensureTable(String tablename) throws IOException {
    LOG.info("tablename: " + tablename);

    // We're looking for the right scheme here (actually, we don't
    // care what the scheme is as long as it is one and it's
    // different from hdfs and file. If the user specified to use
    // the multiquery feature and did not specify a scheme we will
    // have transformed it to an absolute path. In that case we'll
    // take the last component and guess that's what was
    // meant. We'll print a warning in that case.
    int index;
    if(-1 != (index = tablename.indexOf("://"))) {
      if (tablename.startsWith("hdfs:") 
          || tablename.startsWith("file:")) {
        index = tablename.lastIndexOf("/");
        if (-1 == index) {
          index = tablename.lastIndexOf("\\");
        }

        if (-1 == index) {
          throw new IOException("Got tablename: "+tablename
              +". Either turn off multiquery (-no_multiquery)"
              +" or specify load path as \"hbase://<tablename>\".");
        } else {
          String in = tablename;
          tablename = tablename.substring(index+1);
          LOG.warn("Got tablename: "+in+" Assuming you meant table: "
              +tablename+". Either turn off multiquery (-no_multiquery) "
              +"or specify load path as \"hbase://<tablename>\" "
              +"to avoid this warning.");
        }
      } else {
        tablename = tablename.substring(index+3);
      }
    }

    if (table_ == null) {
      table_ = new HTable(conf_, tablename);
    }
  }

  // HBase LoadFunc
  // Most of the action happens in the Slice class.

  @Override
  public void bindTo(String fileName, BufferedPositionedInputStream is,
      long offset, long end) throws IOException {
    // do nothing
  }

  @Override
  public Schema determineSchema(String fileName, ExecType execType,
      DataStorage storage) throws IOException {
    // do nothing
    return null;
  }

  @Override
  public LoadFunc.RequiredFieldResponse fieldsToRead(LoadFunc.RequiredFieldList requiredFieldList) throws FrontendException {
      return new LoadFunc.RequiredFieldResponse(false);
  }

  @Override
  public Tuple getNext() throws IOException {
    // do nothing
    return null;
  }

  @Override
  public String bytesToCharArray(byte[] b) throws IOException {
    return Bytes.toString(b);    
  }

  @Override
  public Double bytesToDouble(byte[] b) throws IOException {
    if (Bytes.SIZEOF_DOUBLE > b.length){ 
      return Bytes.toDouble(Bytes.padHead(b, Bytes.SIZEOF_DOUBLE - b.length));
    } else {
      return Bytes.toDouble(Bytes.head(b, Bytes.SIZEOF_DOUBLE));
    }
  }

  @Override
  public Float bytesToFloat(byte[] b) throws IOException {
    if (Bytes.SIZEOF_FLOAT > b.length){ 
      return Bytes.toFloat(Bytes.padHead(b, Bytes.SIZEOF_FLOAT - b.length));
    } else {
      return Bytes.toFloat(Bytes.head(b, Bytes.SIZEOF_FLOAT));
    }
  }

  @Override
  public Integer bytesToInteger(byte[] b) throws IOException {
    if (Bytes.SIZEOF_INT > b.length){ 
      return Bytes.toInt(Bytes.padHead(b, Bytes.SIZEOF_INT - b.length));
    } else {
      return Bytes.toInt(Bytes.head(b, Bytes.SIZEOF_INT));
    }
  }

  @Override
  public Long bytesToLong(byte[] b) throws IOException {
    if (Bytes.SIZEOF_LONG > b.length){ 
      return Bytes.toLong(Bytes.padHead(b, Bytes.SIZEOF_LONG - b.length));
    } else {
      return Bytes.toLong(Bytes.head(b, Bytes.SIZEOF_LONG));
    }
  }

  /**
   * NOT IMPLEMENTED
   */
   @Override
   public Map<String, Object> bytesToMap(byte[] b) throws IOException {
     throw new ExecException("can't generate a Map from byte[]");
   }

   /**
    * NOT IMPLEMENTED
    */
   @Override
   public Tuple bytesToTuple(byte[] b) throws IOException {
     throw new ExecException("can't generate a Tuple from byte[]");
   }

   /**
    * NOT IMPLEMENTED
    */
   @Override
   public DataBag bytesToBag(byte[] b) throws IOException {
     throw new ExecException("can't generate DataBags from byte[]");
   }
}=======
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with this
 * work for additional information regarding copyright ownership. The ASF
 * licenses this file to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.util.List;
import java.util.Map;

import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.CommandLineParser;
import org.apache.commons.cli.GnuParser;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HConstants;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.filter.BinaryComparator;
import org.apache.hadoop.hbase.filter.RowFilter;
import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.pig.ExecType;
import org.apache.pig.LoadFunc;
import org.apache.pig.Slice;
import org.apache.pig.Slicer;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.io.BufferedPositionedInputStream;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;

import com.google.common.collect.Lists;

/**
 * A <code>Slicer</code> that splits the hbase table into {@link HBaseSlice}s.
 * Actual loading is done in {@link HBaseSlice}. Derived from the HbaseStorage implementation
 * in 0.6 Piggybank.
 * <br>
 * TODO: row version controls<br>
 */
public class HBaseLoader implements Slicer,
LoadFunc {

  private static final Log LOG = LogFactory.getLog(HBaseLoader.class);

  private List<byte[][]> cols_;
  private HTable table_;
  private final Configuration conf_;
  private final boolean loadRowKey_;
  private final CommandLine configuredOptions_;
  private final static Options validOptions_ = new Options();
  private final static CommandLineParser parser_ = new GnuParser();

  private static void populateValidOptions() {
    validOptions_.addOption("loadKey", false, "Load Key");
    validOptions_.addOption("gt", true, "Records must be greater than this value (binary, double-slash-escaped)");
    validOptions_.addOption("lt", true, "Records must be less than this value (binary, double-slash-escaped)");
    validOptions_.addOption("gte", true, "Records must be greater than or equal to this value");
    validOptions_.addOption("lte", true, "Records must be less than or equal to this value");
    validOptions_.addOption("caching", true, "Number of rows scanners should cache");
    validOptions_.addOption("limit", true, "Per-region limit");
  }

  /**
   * Constructor. Construct a HBase Table loader to load the cells of the
   * provided columns.
   *
   * @param columnList
   *            columnlist that is a presented string delimited by space.
   * @throws ParseException
   */
  public HBaseLoader(String columnList) throws ParseException {
    this(columnList, "");
    LOG.info("no-arg constructor");
  }

  /**
   *
   * @param columnList
   * @param optString Loader options. Known options:<ul>
   * <li>-loadKey=(true|false)  Load the row key as the first column
   * <li>-gt=minKeyVal
   * <li>-lt=maxKeyVal
   * <li>-gte=minKeyVal
   * <li>-lte=maxKeyVal
   * <li>-caching=numRows  number of rows to cache (faster scans, more memory).
   * </ul>
   * @throws ParseException
   */
  public HBaseLoader(String columnList, String optString) throws ParseException {
    populateValidOptions();
    String[] colNames = columnList.split(" ");
    String[] optsArr = optString.split(" ");
    try {
      configuredOptions_ = parser_.parse(validOptions_, optsArr);
    } catch (ParseException e) {
      HelpFormatter formatter = new HelpFormatter();
      formatter.printHelp( "", validOptions_ );
      throw e;
    }
    loadRowKey_ = configuredOptions_.hasOption("loadKey");
    cols_ = Lists.newArrayListWithExpectedSize(colNames.length);
    for (int i = 0; i < colNames.length; i++) {
      cols_.add(Bytes.toByteArrays(colNames[i].split(":")));
    }

    conf_ = HBaseConfiguration.create();
  }

  @Override
  public Slice[] slice(DataStorage store, String tablename)
  throws IOException {
    validate(store, tablename);
    if (configuredOptions_.hasOption("caching")) {
      table_.setScannerCaching(Integer.valueOf(configuredOptions_.getOptionValue("caching")));
    }

    byte[][] startKeys = table_.getStartKeys();
    if (startKeys == null || startKeys.length == 0) {
      throw new IOException("Expecting at least one region");
    }
    if (cols_ == null || cols_.size() == 0) {
      throw new IOException("Expecting at least one column");
    }

    // one region one slice
    List<HBaseSlice> slices = Lists.newArrayList();
    for (int i = 0; i < startKeys.length; i++) {

      byte[] endKey = ((i + 1) < startKeys.length) ? startKeys[i + 1] : HConstants.LAST_ROW;

      // skip if the region doesn't satisfy configured options
      if ((skipRegion(CompareOp.LESS, startKeys[i], configuredOptions_.getOptionValue("lt"))) ||
          (skipRegion(CompareOp.GREATER, endKey, configuredOptions_.getOptionValue("gt"))) ||
          (skipRegion(CompareOp.GREATER, endKey, configuredOptions_.getOptionValue("gte"))) ||
          (skipRegion(CompareOp.LESS_OR_EQUAL, startKeys[i], configuredOptions_.getOptionValue("lte")))) {
        continue;
      }
      String regionLocation = table_.getRegionLocation(startKeys[i]).getServerAddress().getHostname();
      HBaseSlice slice = new HBaseSlice(table_.getTableName(), startKeys[i],
          endKey, cols_, loadRowKey_, regionLocation);

      if (configuredOptions_.hasOption("limit")) slice.setLimit(configuredOptions_.getOptionValue("limit"));
      if (configuredOptions_.hasOption("gt")) slice.addFilter(CompareOp.GREATER, slashisize(configuredOptions_.getOptionValue("gt")));
      if (configuredOptions_.hasOption("lt")) slice.addFilter(CompareOp.LESS, slashisize(configuredOptions_.getOptionValue("lt")));
      if (configuredOptions_.hasOption("gte")) slice.addFilter(CompareOp.GREATER_OR_EQUAL, slashisize(configuredOptions_.getOptionValue("gte")));
      if (configuredOptions_.hasOption("lte")) slice.addFilter(CompareOp.LESS_OR_EQUAL, slashisize(configuredOptions_.getOptionValue("lte")));
      slices.add(slice);
    }

    return slices.toArray(new HBaseSlice[] {});
  }

  private boolean skipRegion(CompareOp op, byte[] key, String option ) {
    if (option == null) return false;
    BinaryComparator comp = new BinaryComparator(Bytes.toBytesBinary(slashisize(option)));
    RowFilter rowFilter = new RowFilter(op, comp);
    return rowFilter.filterRowKey(key, 0, key.length);
  }

  /**
   * replace sequences of two slashes ("\\") with one slash ("\")
   * (not escaping a slash in grunt is disallowed, but a double slash doesn't get converted
   * into a regular slash, so we have to do it instead)
   * @param str
   * @return
   */
  private String slashisize(String str) {
    return str.replace("\\\\", "\\");
  }

  @Override
  public void validate(DataStorage store, String tablename)
  throws IOException {
    ensureTable(tablename);
  }

  private void ensureTable(String tablename) throws IOException {
    LOG.info("tablename: " + tablename);

    // We're looking for the right scheme here (actually, we don't
    // care what the scheme is as long as it is one and it's
    // different from hdfs and file. If the user specified to use
    // the multiquery feature and did not specify a scheme we will
    // have transformed it to an absolute path. In that case we'll
    // take the last component and guess that's what was
    // meant. We'll print a warning in that case.
    int index;
    if(-1 != (index = tablename.indexOf("://"))) {
      if (tablename.startsWith("hdfs:")
          || tablename.startsWith("file:")) {
        index = tablename.lastIndexOf("/");
        if (-1 == index) {
          index = tablename.lastIndexOf("\\");
        }

        if (-1 == index) {
          throw new IOException("Got tablename: "+tablename
              +". Either turn off multiquery (-no_multiquery)"
              +" or specify load path as \"hbase://<tablename>\".");
        } else {
          String in = tablename;
          tablename = tablename.substring(index+1);
          LOG.warn("Got tablename: "+in+" Assuming you meant table: "
              +tablename+". Either turn off multiquery (-no_multiquery) "
              +"or specify load path as \"hbase://<tablename>\" "
              +"to avoid this warning.");
        }
      } else {
        tablename = tablename.substring(index+3);
      }
    }

    if (table_ == null) {
      table_ = new HTable(conf_, tablename);
    }
  }

  // HBase LoadFunc
  // Most of the action happens in the Slice class.

  @Override
  public void bindTo(String fileName, BufferedPositionedInputStream is,
      long offset, long end) throws IOException {
    // do nothing
  }

  @Override
  public Schema determineSchema(String fileName, ExecType execType,
      DataStorage storage) throws IOException {
    // do nothing
    return null;
  }

  @Override
  public LoadFunc.RequiredFieldResponse fieldsToRead(LoadFunc.RequiredFieldList requiredFieldList) throws FrontendException {
      return new LoadFunc.RequiredFieldResponse(false);
  }

  @Override
  public Tuple getNext() throws IOException {
    // do nothing
    return null;
  }

  @Override
  public String bytesToCharArray(byte[] b) throws IOException {
    return Bytes.toString(b);
  }

  @Override
  public Double bytesToDouble(byte[] b) throws IOException {
    if (Bytes.SIZEOF_DOUBLE > b.length){
      return Bytes.toDouble(Bytes.padHead(b, Bytes.SIZEOF_DOUBLE - b.length));
    } else {
      return Bytes.toDouble(Bytes.head(b, Bytes.SIZEOF_DOUBLE));
    }
  }

  @Override
  public Float bytesToFloat(byte[] b) throws IOException {
    if (Bytes.SIZEOF_FLOAT > b.length){
      return Bytes.toFloat(Bytes.padHead(b, Bytes.SIZEOF_FLOAT - b.length));
    } else {
      return Bytes.toFloat(Bytes.head(b, Bytes.SIZEOF_FLOAT));
    }
  }

  @Override
  public Integer bytesToInteger(byte[] b) throws IOException {
    if (Bytes.SIZEOF_INT > b.length){
      return Bytes.toInt(Bytes.padHead(b, Bytes.SIZEOF_INT - b.length));
    } else {
      return Bytes.toInt(Bytes.head(b, Bytes.SIZEOF_INT));
    }
  }

  @Override
  public Long bytesToLong(byte[] b) throws IOException {
    if (Bytes.SIZEOF_LONG > b.length){
      return Bytes.toLong(Bytes.padHead(b, Bytes.SIZEOF_LONG - b.length));
    } else {
      return Bytes.toLong(Bytes.head(b, Bytes.SIZEOF_LONG));
    }
  }

  /**
   * NOT IMPLEMENTED
   */
   @Override
   public Map<String, Object> bytesToMap(byte[] b) throws IOException {
     throw new ExecException("can't generate a Map from byte[]");
   }

   /**
    * NOT IMPLEMENTED
    */
   @Override
   public Tuple bytesToTuple(byte[] b) throws IOException {
     throw new ExecException("can't generate a Tuple from byte[]");
   }

   /**
    * NOT IMPLEMENTED
    */
   @Override
   public DataBag bytesToBag(byte[] b) throws IOException {
     throw new ExecException("can't generate DataBags from byte[]");
   }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/pig/piggybank/ThriftToPig.java;<<<<<<< MINE
  private final Class<? extends TBase<?>> tClass_;
  private final ThriftProtocol tProtocol_ = new ThriftProtocol();
  private final Deque<PigContainer> containerStack_ = new ArrayDeque<PigContainer>();
||||||| BASE
  private Class<? extends TBase<?>> tClass_;
  private ThriftProtocol tProtocol_ = new ThriftProtocol();
  private Deque<PigContainer> containerStack_ = new ArrayDeque<PigContainer>();
=======
  private Class<? extends TBase<?, ?>> tClass_;
  private ThriftProtocol tProtocol_ = new ThriftProtocol();
  private Deque<PigContainer> containerStack_ = new ArrayDeque<PigContainer>();
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b87762a_3125301/rev_b87762a-3125301/src/java/com/twitter/elephantbird/pig/piggybank/BytesToThriftTuple.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.piggybank;

import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.thrift.TBase;
import org.apache.thrift.TDeserializer;
import org.apache.thrift.TException;
import org.apache.thrift.protocol.TBinaryProtocol;
import org.apache.thrift.transport.TMemoryBuffer;

import com.twitter.elephantbird.util.TypeRef;

/**
 * This is an abstract UDF for converting serialized Thrift objects into Pig tuples.
 * To create a converter for your Thrift class <code>MyThriftClass</code>, you simply need to extend
 * <code>BytesToThriftTuple</code> with something like this:
 *<pre>
 * {@code
 * public class BytesToSimpleLocation extends BytesToThriftTuple<MyThriftClass> {
 *
 *   public BytesToSimpleLocation() {
 *     setTypeRef(new TypeRef<MyThriftClass>() {});
 *   }
 * }}
 *</pre>
 */
public abstract class BytesToThriftTuple<T extends TBase<?>> extends EvalFunc<Tuple> {

  private final TDeserializer deserializer_ = new TDeserializer(new TBinaryProtocol.Factory());
  private ThriftToPig<T> thriftToTuple_;
  private TypeRef<T> typeRef_;

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called by the constructor!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<T> typeRef) {
    typeRef_ = typeRef;
    thriftToTuple_ = ThriftToPig.newInstance(typeRef);
  }


  @Override
  public Tuple exec(org.apache.pig.data.Tuple input) throws IOException {
    if (input == null || input.size() < 1) return null;
    try {
      T tObj = typeRef_.safeNewInstance();
      DataByteArray dbarr = (DataByteArray) input.get(0);
      deserializer_.deserialize(tObj, dbarr.get());
      return thriftToTuple_.getPigTuple(tObj);
    } catch (IOException e) {
      log.warn("Caught exception "+e.getMessage());
      return null;
    } catch (TException e) {
      log.warn("Unable to deserialize Thrift object: "+e);
      return null;
    }
  }
}=======
package com.twitter.elephantbird.pig.piggybank;

import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.thrift.TBase;
import org.apache.thrift.TDeserializer;
import org.apache.thrift.TException;
import org.apache.thrift.protocol.TBinaryProtocol;
import org.apache.thrift.transport.TMemoryBuffer;

import com.twitter.elephantbird.util.TypeRef;

/**
 * This is an abstract UDF for converting serialized Thrift objects into Pig tuples.
 * To create a converter for your Thrift class <code>MyThriftClass</code>, you simply need to extend
 * <code>BytesToThriftTuple</code> with something like this:
 *<pre>
 * {@code
 * public class BytesToSimpleLocation extends BytesToThriftTuple<MyThriftClass> {
 *
 *   public BytesToSimpleLocation() {
 *     setTypeRef(new TypeRef<MyThriftClass>() {});
 *   }
 * }}
 *</pre>
 */
public abstract class BytesToThriftTuple<T extends TBase<?, ?>> extends EvalFunc<Tuple> {

  private final TDeserializer deserializer_ = new TDeserializer(new TBinaryProtocol.Factory());
  private ThriftToPig<T> thriftToTuple_;
  private TypeRef<T> typeRef_;

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called by the constructor!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<T> typeRef) {
    typeRef_ = typeRef;
    thriftToTuple_ = ThriftToPig.newInstance(typeRef);
  }


  @Override
  public Tuple exec(org.apache.pig.data.Tuple input) throws IOException {
    if (input == null || input.size() < 1) return null;
    try {
      T tObj = typeRef_.safeNewInstance();
      DataByteArray dbarr = (DataByteArray) input.get(0);
      deserializer_.deserialize(tObj, dbarr.get());
      return thriftToTuple_.getPigTuple(tObj);
    } catch (IOException e) {
      log.warn("Caught exception "+e.getMessage());
      return null;
    } catch (TException e) {
      log.warn("Unable to deserialize Thrift object: "+e);
      return null;
    }
  }
}>>>>>>> YOURS
/home/taes/taes/projects/curator/revisions/rev_b5a2a9f_0ca5859/rev_b5a2a9f-0ca5859/curator-framework/src/main/java/com/netflix/curator/framework/imps/CuratorTransactionImpl.java;<<<<<<< MINE
||||||| BASE
/*
 *
 *  Copyright 2011 Netflix, Inc.
 *
 *     Licensed under the Apache License, Version 2.0 (the "License");
 *     you may not use this file except in compliance with the License.
 *     You may obtain a copy of the License at
 *
 *         http://www.apache.org/licenses/LICENSE-2.0
 *
 *     Unless required by applicable law or agreed to in writing, software
 *     distributed under the License is distributed on an "AS IS" BASIS,
 *     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *     See the License for the specific language governing permissions and
 *     limitations under the License.
 *
 */

package com.netflix.curator.framework.imps;

import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.netflix.curator.RetryLoop;
import com.netflix.curator.framework.api.Pathable;
import com.netflix.curator.framework.api.transaction.CuratorTransaction;
import com.netflix.curator.framework.api.transaction.CuratorTransactionBridge;
import com.netflix.curator.framework.api.transaction.CuratorTransactionFinal;
import com.netflix.curator.framework.api.transaction.CuratorTransactionResult;
import com.netflix.curator.framework.api.transaction.OperationType;
import com.netflix.curator.framework.api.transaction.TransactionCheckBuilder;
import com.netflix.curator.framework.api.transaction.TransactionCreateBuilder;
import com.netflix.curator.framework.api.transaction.TransactionDeleteBuilder;
import com.netflix.curator.framework.api.transaction.TransactionSetDataBuilder;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.Op;
import org.apache.zookeeper.OpResult;
import org.apache.zookeeper.ZooDefs;
import org.apache.zookeeper.data.Stat;
import java.util.Collection;
import java.util.List;
import java.util.concurrent.Callable;
import java.util.concurrent.atomic.AtomicBoolean;

class CuratorTransactionImpl implements CuratorTransaction, CuratorTransactionBridge, CuratorTransactionFinal
{
    private final CuratorFrameworkImpl              client;
    private final CuratorMultiTransactionRecord     transaction;

    private boolean         isCommitted = false;

    CuratorTransactionImpl(CuratorFrameworkImpl client)
    {
        this.client = client;
        transaction = new CuratorMultiTransactionRecord();
    }

    @Override
    public CuratorTransactionFinal and()
    {
        return this;
    }

    @Override
    public TransactionCreateBuilder create()
    {
        Preconditions.checkArgument(!isCommitted);

        return new CreateBuilderImpl(client).asTransactionCreateBuilder(this, transaction);
    }

    @Override
    public TransactionDeleteBuilder delete()
    {
        Preconditions.checkArgument(!isCommitted);

        return new DeleteBuilderImpl(client).asTransactionDeleteBuilder(this, transaction);
    }

    @Override
    public TransactionSetDataBuilder setData()
    {
        Preconditions.checkArgument(!isCommitted);

        return new SetDataBuilderImpl(client).asTransactionSetDataBuilder(this, transaction);
    }

    @Override
    public TransactionCheckBuilder check()
    {
        Preconditions.checkArgument(!isCommitted);

        return new TransactionCheckBuilder()
        {
            private int         version = -1;

            @Override
            public CuratorTransactionBridge forPath(String path) throws Exception
            {
                String      fixedPath = client.fixForNamespace(path);
                transaction.add(Op.check(fixedPath, version), OperationType.CHECK, path);

                return CuratorTransactionImpl.this;
            }

            @Override
            public Pathable<CuratorTransactionBridge> withVersion(int version)
            {
                this.version = version;
                return this;
            }
        };
    }

    @Override
    public Collection<CuratorTransactionResult> commit() throws Exception
    {
        Preconditions.checkArgument(!isCommitted);
        isCommitted = true;

        final AtomicBoolean firstTime = new AtomicBoolean(true);
        List<OpResult>      resultList = RetryLoop.callWithRetry
        (
            client.getZookeeperClient(),
            new Callable<List<OpResult>>()
            {
                @Override
                public List<OpResult> call() throws Exception
                {
                    return doOperation(firstTime);
                }
            }
        );
        
        if ( resultList.size() != transaction.metadataSize() )
        {
            throw new IllegalStateException(String.format("Result size (%d) doesn't match input size (%d)", resultList.size(), transaction.metadataSize()));
        }

        ImmutableList.Builder<CuratorTransactionResult>     builder = ImmutableList.builder();
        for ( int i = 0; i < resultList.size(); ++i )
        {
            OpResult                                    opResult = resultList.get(i);
            CuratorMultiTransactionRecord.TypeAndPath   metadata = transaction.getMetadata(i);
            CuratorTransactionResult                    curatorResult = makeCuratorResult(opResult, metadata);
            builder.add(curatorResult);
        }

        return builder.build();
    }

    private List<OpResult> doOperation(AtomicBoolean firstTime) throws Exception
    {
        boolean         localFirstTime = firstTime.getAndSet(false);
        if ( !localFirstTime )
        {

        }

        List<OpResult>  opResults = client.getZooKeeper().multi(transaction);
        if ( opResults.size() > 0 )
        {
            OpResult        firstResult = opResults.get(0);
            if ( firstResult.getType() == ZooDefs.OpCode.error )
            {
                OpResult.ErrorResult        error = (OpResult.ErrorResult)firstResult;
                KeeperException.Code        code = KeeperException.Code.get(error.getErr());
                if ( code == null )
                {
                    code = KeeperException.Code.UNIMPLEMENTED;
                }
                throw KeeperException.create(code);
            }
        }
        return opResults;
    }

    private CuratorTransactionResult makeCuratorResult(OpResult opResult, CuratorMultiTransactionRecord.TypeAndPath metadata)
    {
        String                                      resultPath = null;
        Stat resultStat = null;
        switch ( opResult.getType() )
        {
            default:
            {
                // NOP
                break;
            }

            case ZooDefs.OpCode.create:
            {
                OpResult.CreateResult       createResult = (OpResult.CreateResult)opResult;
                resultPath = client.unfixForNamespace(createResult.getPath());
                break;
            }

            case ZooDefs.OpCode.setData:
            {
                OpResult.SetDataResult      setDataResult = (OpResult.SetDataResult)opResult;
                resultStat = setDataResult.getStat();
                break;
            }
        }

        return new CuratorTransactionResult(metadata.type, metadata.forPath, resultPath, resultStat);
    }
}=======
/*
 *
 *  Copyright 2011 Netflix, Inc.
 *
 *     Licensed under the Apache License, Version 2.0 (the "License");
 *     you may not use this file except in compliance with the License.
 *     You may obtain a copy of the License at
 *
 *         http://www.apache.org/licenses/LICENSE-2.0
 *
 *     Unless required by applicable law or agreed to in writing, software
 *     distributed under the License is distributed on an "AS IS" BASIS,
 *     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *     See the License for the specific language governing permissions and
 *     limitations under the License.
 *
 */

package com.netflix.curator.framework.imps;

import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.netflix.curator.RetryLoop;
import com.netflix.curator.framework.api.Pathable;
import com.netflix.curator.framework.api.transaction.CuratorTransaction;
import com.netflix.curator.framework.api.transaction.CuratorTransactionBridge;
import com.netflix.curator.framework.api.transaction.CuratorTransactionFinal;
import com.netflix.curator.framework.api.transaction.CuratorTransactionResult;
import com.netflix.curator.framework.api.transaction.OperationType;
import com.netflix.curator.framework.api.transaction.TransactionCheckBuilder;
import com.netflix.curator.framework.api.transaction.TransactionCreateBuilder;
import com.netflix.curator.framework.api.transaction.TransactionDeleteBuilder;
import com.netflix.curator.framework.api.transaction.TransactionSetDataBuilder;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.Op;
import org.apache.zookeeper.OpResult;
import org.apache.zookeeper.ZooDefs;
import org.apache.zookeeper.data.Stat;
import java.util.Collection;
import java.util.List;
import java.util.concurrent.Callable;
import java.util.concurrent.atomic.AtomicBoolean;

class CuratorTransactionImpl implements CuratorTransaction, CuratorTransactionBridge, CuratorTransactionFinal
{
    private final CuratorFrameworkImpl              client;
    private final CuratorMultiTransactionRecord     transaction;

    private boolean         isCommitted = false;

    CuratorTransactionImpl(CuratorFrameworkImpl client)
    {
        this.client = client;
        transaction = new CuratorMultiTransactionRecord();
    }

    @Override
    public CuratorTransactionFinal and()
    {
        return this;
    }

    @Override
    public TransactionCreateBuilder create()
    {
        Preconditions.checkState(!isCommitted, "transaction already committed");

        return new CreateBuilderImpl(client).asTransactionCreateBuilder(this, transaction);
    }

    @Override
    public TransactionDeleteBuilder delete()
    {
        Preconditions.checkState(!isCommitted, "transaction already committed");

        return new DeleteBuilderImpl(client).asTransactionDeleteBuilder(this, transaction);
    }

    @Override
    public TransactionSetDataBuilder setData()
    {
        Preconditions.checkState(!isCommitted, "transaction already committed");

        return new SetDataBuilderImpl(client).asTransactionSetDataBuilder(this, transaction);
    }

    @Override
    public TransactionCheckBuilder check()
    {
        Preconditions.checkState(!isCommitted, "transaction already committed");

        return new TransactionCheckBuilder()
        {
            private int         version = -1;

            @Override
            public CuratorTransactionBridge forPath(String path) throws Exception
            {
                String      fixedPath = client.fixForNamespace(path);
                transaction.add(Op.check(fixedPath, version), OperationType.CHECK, path);

                return CuratorTransactionImpl.this;
            }

            @Override
            public Pathable<CuratorTransactionBridge> withVersion(int version)
            {
                this.version = version;
                return this;
            }
        };
    }

    @Override
    public Collection<CuratorTransactionResult> commit() throws Exception
    {
        Preconditions.checkState(!isCommitted, "transaction already committed");
        isCommitted = true;

        final AtomicBoolean firstTime = new AtomicBoolean(true);
        List<OpResult>      resultList = RetryLoop.callWithRetry
        (
            client.getZookeeperClient(),
            new Callable<List<OpResult>>()
            {
                @Override
                public List<OpResult> call() throws Exception
                {
                    return doOperation(firstTime);
                }
            }
        );
        
        if ( resultList.size() != transaction.metadataSize() )
        {
            throw new IllegalStateException(String.format("Result size (%d) doesn't match input size (%d)", resultList.size(), transaction.metadataSize()));
        }

        ImmutableList.Builder<CuratorTransactionResult>     builder = ImmutableList.builder();
        for ( int i = 0; i < resultList.size(); ++i )
        {
            OpResult                                    opResult = resultList.get(i);
            CuratorMultiTransactionRecord.TypeAndPath   metadata = transaction.getMetadata(i);
            CuratorTransactionResult                    curatorResult = makeCuratorResult(opResult, metadata);
            builder.add(curatorResult);
        }

        return builder.build();
    }

    private List<OpResult> doOperation(AtomicBoolean firstTime) throws Exception
    {
        boolean         localFirstTime = firstTime.getAndSet(false);
        if ( !localFirstTime )
        {

        }

        List<OpResult>  opResults = client.getZooKeeper().multi(transaction);
        if ( opResults.size() > 0 )
        {
            OpResult        firstResult = opResults.get(0);
            if ( firstResult.getType() == ZooDefs.OpCode.error )
            {
                OpResult.ErrorResult        error = (OpResult.ErrorResult)firstResult;
                KeeperException.Code        code = KeeperException.Code.get(error.getErr());
                if ( code == null )
                {
                    code = KeeperException.Code.UNIMPLEMENTED;
                }
                throw KeeperException.create(code);
            }
        }
        return opResults;
    }

    private CuratorTransactionResult makeCuratorResult(OpResult opResult, CuratorMultiTransactionRecord.TypeAndPath metadata)
    {
        String                                      resultPath = null;
        Stat resultStat = null;
        switch ( opResult.getType() )
        {
            default:
            {
                // NOP
                break;
            }

            case ZooDefs.OpCode.create:
            {
                OpResult.CreateResult       createResult = (OpResult.CreateResult)opResult;
                resultPath = client.unfixForNamespace(createResult.getPath());
                break;
            }

            case ZooDefs.OpCode.setData:
            {
                OpResult.SetDataResult      setDataResult = (OpResult.SetDataResult)opResult;
                resultStat = setDataResult.getStat();
                break;
            }
        }

        return new CuratorTransactionResult(metadata.type, metadata.forPath, resultPath, resultStat);
    }
}>>>>>>> YOURS
/home/taes/taes/projects/curator/revisions/rev_b5a2a9f_0ca5859/rev_b5a2a9f-0ca5859/curator-framework/src/main/java/com/netflix/curator/framework/imps/CuratorFrameworkImpl.java;<<<<<<< MINE
||||||| BASE
    public CuratorTransaction inTransaction()
    {
        Preconditions.checkState(state.get() == State.STARTED);

        return new CuratorTransactionImpl(this);
    }

    @Override
=======
    public CuratorTransaction inTransaction()
    {
        Preconditions.checkState(state.get() == State.STARTED, "instance must be started before calling this method");

        return new CuratorTransactionImpl(this);
    }

    @Override
>>>>>>> YOURS
/home/taes/taes/projects/curator/revisions/rev_1ee1673_859a623/rev_1ee1673-859a623/curator-recipes/src/main/java/com/netflix/curator/framework/recipes/queue/DistributedQueue.java;null
/home/taes/taes/projects/curator/revisions/rev_1ee1673_859a623/rev_1ee1673-859a623/curator-recipes/src/main/java/com/netflix/curator/framework/recipes/queue/DistributedQueue.java;null
/home/taes/taes/projects/curator/revisions/rev_1ee1673_859a623/rev_1ee1673-859a623/curator-recipes/src/test/java/com/netflix/curator/framework/recipes/queue/TestQueueSharder.java;null
/home/taes/taes/projects/curator/revisions/rev_1ee1673_859a623/rev_1ee1673-859a623/curator-recipes/src/test/java/com/netflix/curator/framework/recipes/queue/TestQueueSharder.java;null
/home/taes/taes/projects/curator/revisions/rev_1ee1673_859a623/rev_1ee1673-859a623/curator-recipes/src/test/java/com/netflix/curator/framework/recipes/queue/TestQueueSharder.java;null
/home/taes/taes/projects/curator/revisions/rev_1ee1673_859a623/rev_1ee1673-859a623/curator-recipes/src/test/java/com/netflix/curator/framework/recipes/queue/TestQueueSharder.java;null
/home/taes/taes/projects/curator/revisions/rev_1ee1673_859a623/rev_1ee1673-859a623/curator-recipes/src/test/java/com/netflix/curator/framework/recipes/queue/TestQueueSharder.java;null
/home/taes/taes/projects/curator/revisions/rev_1ee1673_859a623/rev_1ee1673-859a623/curator-recipes/src/test/java/com/netflix/curator/framework/recipes/queue/TestQueueSharder.java;null
/home/taes/taes/projects/jodd/revisions/rev_b29bb89_8ab7c2d/rev_b29bb89-8ab7c2d/jodd-core/src/main/java/jodd/util/BCrypt.java;null
/home/taes/taes/projects/jodd/revisions/rev_b29bb89_8ab7c2d/rev_b29bb89-8ab7c2d/jodd-core/src/main/java/jodd/util/BCrypt.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_110e154_738e6ce/rev_110e154-738e6ce/src/java/com/twitter/elephantbird/pig/store/LzoProtobufB64LinePigStorage.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.store;

import java.io.IOException;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.data.Tuple;

import com.google.protobuf.Message;
import com.google.protobuf.Message.Builder;
import com.twitter.elephantbird.pig.util.PigToProtobuf;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * Serializes Pig Tuples into Base-64 encoded, line-delimited protocol buffers.
 * The fields in the pig tuple must correspond exactly to the fields in the protobuf, as
 * no name-matching is performed (names of the tuple fields are not currently accessible to
 * a StoreFunc. It will be in 0.7, so something more flexible will be possible)
 *
 * @param <M> Protocol Buffer Message class being serialized
 */
public abstract class LzoProtobufB64LinePigStorage<M extends Message> extends LzoBaseStoreFunc {

  private TypeRef<M> typeRef_;
  private Base64 base64_ = new Base64();
  private final PigToProtobuf pigToProto_ = new PigToProtobuf();

  protected void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
  }

  public void putNext(Tuple f) throws IOException {
    if (f == null) return;
	Builder builder = Protobufs.getMessageBuilder(typeRef_.getRawClass());
    os_.write(base64_.encode(pigToProto_.tupleToMessage(builder, f).toByteArray()));
    os_.write("\n".getBytes("UTF-8"));
  }

}=======
package com.twitter.elephantbird.pig.store;

import java.io.IOException;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.data.Tuple;

import com.google.protobuf.Message;
import com.google.protobuf.Message.Builder;
import com.twitter.elephantbird.pig.util.PigToProtobuf;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * Serializes Pig Tuples into Base-64 encoded, line-delimited protocol buffers.
 * The fields in the pig tuple must correspond exactly to the fields in the protobuf, as
 * no name-matching is performed (names of the tuple fields are not currently accessible to
 * a StoreFunc. It will be in 0.7, so something more flexible will be possible)
 *
 * @param <M> Protocol Buffer Message class being serialized
 */
public class LzoProtobufB64LinePigStorage<M extends Message> extends LzoBaseStoreFunc {

  private TypeRef<M> typeRef_;
  private Base64 base64_ = new Base64();
  Builder builder_;

  protected LzoProtobufB64LinePigStorage(){}

  public LzoProtobufB64LinePigStorage(String protoClassName) {
    TypeRef<M> typeRef = Protobufs.getTypeRef(protoClassName);
    setTypeRef(typeRef);
  }

  protected void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    builder_ =  Protobufs.getMessageBuilder(typeRef_.getRawClass());
  }

  public void putNext(Tuple f) throws IOException {
    if (f == null) return;
    os_.write(base64_.encode(PigToProtobuf.tupleToMessage(builder_, f).toByteArray()));
    os_.write("\n".getBytes("UTF-8"));
  }

}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_110e154_738e6ce/rev_110e154-738e6ce/src/java/com/twitter/elephantbird/pig/store/LzoProtobufBlockPigStorage.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.store;

import java.io.IOException;
import java.util.List;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.data.Tuple;

import com.google.protobuf.Message;
import com.google.protobuf.Descriptors.FieldDescriptor;
import com.google.protobuf.Message.Builder;
import com.twitter.elephantbird.pig.util.PigToProtobuf;
import com.twitter.elephantbird.mapreduce.io.ProtobufBlockWriter;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;
import java.io.OutputStream;
import org.apache.hadoop.io.compress.CompressionOutputStream;


/**
 * Serializes Pig Tuples into Block encodedprotocol buffers.
 * The fields in the pig tuple must correspond exactly to the fields in the protobuf, as
 * no name-matching is performed (names of the tuple fields are not currently accessible to
 * a StoreFunc. It will be in 0.7, so something more flexible will be possible)
 *
 * @param <M> Protocol Buffer Message class being serialized
 */
public abstract class LzoProtobufBlockPigStorage<M extends Message> extends LzoBaseStoreFunc {

  private TypeRef<M> typeRef_;
  private final PigToProtobuf pigToProto_ = new PigToProtobuf();
  protected ProtobufBlockWriter writer_ = null;
	private int numRecordsPerBlock_ = 10000;
	
  @Override
  public void bindTo(OutputStream os) throws IOException {
		super.bindTo(os);
		writer_ = new ProtobufBlockWriter(os_, typeRef_.getRawClass(), numRecordsPerBlock_); 
  }

  protected void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
  }

  public void putNext(Tuple f) throws IOException {
    if (f == null) return;
	  Builder builder = Protobufs.getMessageBuilder(typeRef_.getRawClass());
	  writer_.write(pigToProto_.tupleToMessage(builder, f));
  }

	@Override
	public void finish() throws IOException {
    if (writer_ != null) {
      writer_.close();
    }
  }
}=======
package com.twitter.elephantbird.pig.store;

import java.io.IOException;

import org.apache.pig.data.Tuple;

import com.google.protobuf.Message;
import com.google.protobuf.Message.Builder;
import com.twitter.elephantbird.pig.util.PigToProtobuf;
import com.twitter.elephantbird.mapreduce.io.ProtobufBlockWriter;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;
import java.io.OutputStream;


/**
 * Serializes Pig Tuples into Block encodedprotocol buffers.
 * The fields in the pig tuple must correspond exactly to the fields in the protobuf, as
 * no name-matching is performed (names of the tuple fields are not currently accessible to
 * a StoreFunc. It will be in 0.7, so something more flexible will be possible)
 *
 * @param <M> Protocol Buffer Message class being serialized
 */
public class LzoProtobufBlockPigStorage<M extends Message> extends LzoBaseStoreFunc {

  private TypeRef<M> typeRef_;
  Builder builder_;
  protected ProtobufBlockWriter<M> writer_ = null;
  private int numRecordsPerBlock_ = 10000;

  protected LzoProtobufBlockPigStorage() {
  }

  public LzoProtobufBlockPigStorage(String protoClassName) {
    TypeRef<M> typeRef = Protobufs.getTypeRef(protoClassName);
    setTypeRef(typeRef);
  }

  @Override
  public void bindTo(OutputStream os) throws IOException {
		super.bindTo(os);
		writer_ = new ProtobufBlockWriter<M>(os_, typeRef_.getRawClass(), numRecordsPerBlock_);
  }

  protected void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    builder_ = Protobufs.getMessageBuilder(typeRef_.getRawClass());
  }

  @SuppressWarnings("unchecked")
  public void putNext(Tuple f) throws IOException {
    if (f == null) return;
	  writer_.write((M)PigToProtobuf.tupleToMessage(builder_, f));
  }

	@Override
	public void finish() throws IOException {
    if (writer_ != null) {
      writer_.close();
    }
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_110e154_738e6ce/rev_110e154-738e6ce/src/java/com/twitter/elephantbird/pig/util/PigToProtobuf.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.util;

import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;

import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.Tuple;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.Message;
import com.google.protobuf.Descriptors.FieldDescriptor;
import com.google.protobuf.Message.Builder;

/**
 * A class for turning Pig Tuples into codegen'd protos for custom Pig StoreFuncs.
 * @author Vikram Oberoi
 */
public class PigToProtobuf {
  private static final Logger LOG = LoggerFactory.getLogger(PigToProtobuf.class);

  public PigToProtobuf() {}

  /**
   * Turn a Tuple into a Message with the given type.
   * @param builder a builder for the Message type the tuple will be converted to
   * @param tuple the tuple
   * @return a message representing the given tuple
   */
  public Message tupleToMessage(Builder builder, Tuple tuple) {
    List<FieldDescriptor> fieldDescriptors = builder.getDescriptorForType().getFields();

    if (tuple == null) {
      return builder.build();
    }

    for (int i = 0; i < fieldDescriptors.size() && i < tuple.size(); i++) {
      Object tupleField = null;
      FieldDescriptor fieldDescriptor = fieldDescriptors.get(i);

      try {
        tupleField = tuple.get(i);
      } catch (ExecException e) {
        LOG.warn("Could not convert tuple field " + tupleField + " to field with descriptor " + fieldDescriptor);
        continue;
      }

      if (tupleField != null) {
        if (fieldDescriptor.isRepeated()) { 
          // Repeated fields are set with Lists containing objects of the fields' Java type.
          builder.setField(fieldDescriptor, dataBagToRepeatedField(builder, fieldDescriptor, (DataBag)tupleField));
        } else {
          if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
            Builder nestedMessageBuilder = builder.newBuilderForField(fieldDescriptor);
            builder.setField(fieldDescriptor, tupleToMessage((Builder)nestedMessageBuilder, (Tuple)tupleField));
          } else {
            builder.setField(fieldDescriptor, tupleFieldToSingleField(fieldDescriptor, tupleField));
          }
        }
      }
    }

    return builder.build();
  }

  /**
   * Converts a DataBag into a List of objects with the type in the given FieldDescriptor. DataBags
   * don't map cleanly to repeated protobuf types, so each Tuple has to be unwrapped (by taking the
   * first element if the type is primitive or by converting the Tuple to a Message if the type is
   * MESSAGE), and the contents have to be appended to a List.
   * @param containingMessageBuilder a Message builder for the Message that contains this repeated field
   * @param fieldDescriptor a FieldDescriptor for this repeated field
   * @param bag the DataBag being serialized
   * @return a protobuf-friendly List of fieldDescriptor-type objects
   */
  public List<Object> dataBagToRepeatedField(Builder containingMessageBuilder, FieldDescriptor fieldDescriptor, DataBag bag) {
    ArrayList<Object> bagContents = new ArrayList<Object>((int)bag.size());
    Iterator<Tuple> bagIter = bag.iterator();

    while (bagIter.hasNext()) {
      Tuple tuple = bagIter.next();
      if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
        Builder nestedMessageBuilder = containingMessageBuilder.newBuilderForField(fieldDescriptor);
        bagContents.add(tupleToMessage((Builder)nestedMessageBuilder, tuple));
      } else {
        try {
          bagContents.add(tupleFieldToSingleField(fieldDescriptor, tuple.get(0)));
        } catch (ExecException e) {
          LOG.warn("Could not add a value for repeated field with descriptor " + fieldDescriptor);
        }		
      }
    }

    return bagContents;
  }

  /**
   * Converts a tupleField string to its corresponding protobuf enum type if necessary, otherwise
   * returns the tupleField as is.
   * @param fieldDescriptor the FieldDescriptor for the given tuple field
   * @param tupleField the tupleField being converted to a protobuf field
   * @return the protobuf type for the given tupleField. This will be the tupleField itself unless it's an enum, in which case this will return the enum type for the field.
   */
  public Object tupleFieldToSingleField(FieldDescriptor fieldDescriptor, Object tupleField) {
    if (fieldDescriptor.getType() == FieldDescriptor.Type.ENUM) {
      // Convert tupleField to the enum value.
      return fieldDescriptor.getEnumType().findValueByName((String)tupleField);
    } else {
      return tupleField;
    }
  }
}=======
package com.twitter.elephantbird.pig.util;

import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;

import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;
import com.google.protobuf.Message;
import com.google.protobuf.Descriptors.FieldDescriptor;
import com.google.protobuf.Message.Builder;
import com.twitter.elephantbird.util.Protobufs;

/**
 * A class for turning Pig Tuples into codegen'd protos for custom Pig StoreFuncs.
 * @author Vikram Oberoi
 */
public class PigToProtobuf {
  private static final Logger LOG = LoggerFactory.getLogger(PigToProtobuf.class);

  public PigToProtobuf() {}

  @SuppressWarnings("unchecked")
  public static <M extends Message> M tupleToMessage(Class<M> protoClass, Tuple tuple) {
    Builder builder = Protobufs.getMessageBuilder(protoClass);
    return (M) tupleToMessage(builder, tuple);
  }

  /**
   * Turn a Tuple into a Message with the given type.
   * @param builder a builder for the Message type the tuple will be converted to
   * @param tuple the tuple
   * @return a message representing the given tuple
   */

  public static Message tupleToMessage(Builder builder, Tuple tuple) {
    List<FieldDescriptor> fieldDescriptors = builder.getDescriptorForType().getFields();

    if (tuple == null) {
      return  builder.build();
    }

    for (int i = 0; i < fieldDescriptors.size() && i < tuple.size(); i++) {
      Object tupleField = null;
      FieldDescriptor fieldDescriptor = fieldDescriptors.get(i);

      try {
        tupleField = tuple.get(i);
      } catch (ExecException e) {
        LOG.warn("Could not convert tuple field " + tupleField + " to field with descriptor " + fieldDescriptor);
        continue;
      }

      if (tupleField != null) {
        if (fieldDescriptor.isRepeated()) {
          // Repeated fields are set with Lists containing objects of the fields' Java type.
          builder.setField(fieldDescriptor, dataBagToRepeatedField(builder, fieldDescriptor, (DataBag)tupleField));
        } else {
          if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
            Builder nestedMessageBuilder = builder.newBuilderForField(fieldDescriptor);
            builder.setField(fieldDescriptor, tupleToMessage((Builder)nestedMessageBuilder, (Tuple)tupleField));
          } else {
            builder.setField(fieldDescriptor, tupleFieldToSingleField(fieldDescriptor, tupleField));
          }
        }
      }
    }

    return builder.build();
  }

  /**
   * Converts a DataBag into a List of objects with the type in the given FieldDescriptor. DataBags
   * don't map cleanly to repeated protobuf types, so each Tuple has to be unwrapped (by taking the
   * first element if the type is primitive or by converting the Tuple to a Message if the type is
   * MESSAGE), and the contents have to be appended to a List.
   * @param containingMessageBuilder a Message builder for the Message that contains this repeated field
   * @param fieldDescriptor a FieldDescriptor for this repeated field
   * @param bag the DataBag being serialized
   * @return a protobuf-friendly List of fieldDescriptor-type objects
   */
  private static List<Object> dataBagToRepeatedField(Builder containingMessageBuilder, FieldDescriptor fieldDescriptor, DataBag bag) {
    ArrayList<Object> bagContents = new ArrayList<Object>((int)bag.size());
    Iterator<Tuple> bagIter = bag.iterator();

    while (bagIter.hasNext()) {
      Tuple tuple = bagIter.next();
      if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
        Builder nestedMessageBuilder = containingMessageBuilder.newBuilderForField(fieldDescriptor);
        bagContents.add(tupleToMessage((Builder)nestedMessageBuilder, tuple));
      } else {
        try {
          bagContents.add(tupleFieldToSingleField(fieldDescriptor, tuple.get(0)));
        } catch (ExecException e) {
          LOG.warn("Could not add a value for repeated field with descriptor " + fieldDescriptor);
        }
      }
    }

    return bagContents;
  }

  /**
   * Converts a tupleField string to its corresponding protobuf enum type if necessary, otherwise
   * returns the tupleField as is.
   * @param fieldDescriptor the FieldDescriptor for the given tuple field
   * @param tupleField the tupleField being converted to a protobuf field
   * @return the protobuf type for the given tupleField. This will be the tupleField itself unless it's an enum, in which case this will return the enum type for the field.
   */
  private static Object tupleFieldToSingleField(FieldDescriptor fieldDescriptor, Object tupleField) {
    // type convertion should match with ProtobufToPig.getPigScriptDataType
    switch (fieldDescriptor.getType()) {
    case ENUM:
      // Convert tupleField to the enum value.
      return fieldDescriptor.getEnumType().findValueByName((String)tupleField);
    case BOOL:
      return Boolean.valueOf((Integer)tupleField != 0);
    case BYTES:
      return ByteString.copyFrom(((DataByteArray)tupleField).get());
    default:
      return tupleField;
    }
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_110e154_738e6ce/rev_110e154-738e6ce/src/java/com/twitter/elephantbird/pig/load/LzoThriftB64LinePigLoader.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.nio.charset.Charset;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.apache.thrift.TBase;
import org.apache.thrift.TException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.twitter.elephantbird.mapreduce.io.ThriftConverter;
import com.twitter.elephantbird.pig.piggybank.ThriftToPig;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;

public class LzoThriftB64LinePigLoader<M extends TBase<?, ?>> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoThriftB64LinePigLoader.class);

  private final TypeRef<M> typeRef_;
  private final ThriftConverter<M> converter_;
  private final Base64 base64_ = new Base64();
  private final ThriftToPig<M> thriftToPig_;

  private static final Charset UTF8 = Charset.forName("UTF-8");
  private static final byte RECORD_DELIMITER = (byte)'\n';

  private Pair<String, String> linesRead;
  private Pair<String, String> thriftStructsRead;
  private Pair<String, String> thriftErrors;

  public LzoThriftB64LinePigLoader(String thriftClassName) {
    typeRef_ = ThriftUtils.getTypeRef(thriftClassName);
    converter_ = ThriftConverter.newInstance(typeRef_);
    thriftToPig_ =  ThriftToPig.newInstance(typeRef_);

    String group = "LzoB64Lines of " + typeRef_.getRawClass().getName();
    linesRead = new Pair<String, String>(group, "Lines Read");
    thriftStructsRead = new Pair<String, String>(group, "Thrift Structs");
    thriftErrors = new Pair<String, String>(group, "Errors");

    setLoaderSpec(getClass(), new String[]{thriftClassName});
  }

  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // Since we are not block aligned we throw away the first record of each split and count on a different
    // instance to read it.  The only split this doesn't work for is the first.
    if (!atFirstRecord) {
      getNext();
    }
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    String line;
    Tuple t = null;
    while ((line = is_.readLine(UTF8, RECORD_DELIMITER)) != null) {
      incrCounter(linesRead, 1L);
      M value = converter_.fromBytes(base64_.decode(line.getBytes("UTF-8")));
      if (value != null) {
        try {
          t = thriftToPig_.getPigTuple(value);
          incrCounter(thriftStructsRead, 1L);
          break;
        } catch (TException e) {
          incrCounter(thriftErrors, 1L);
          LOG.warn("ThriftToTuple error :", e); // may be struct mismatch
        }
      }
    }

    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return ThriftToPig.toSchema(typeRef_.getRawClass());
  }
}=======
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.nio.charset.Charset;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.apache.thrift.TBase;
import org.apache.thrift.TException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.twitter.elephantbird.mapreduce.io.ThriftConverter;
import com.twitter.elephantbird.pig.util.ThriftToPig;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;

public class LzoThriftB64LinePigLoader<M extends TBase<?, ?>> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoThriftB64LinePigLoader.class);

  private final TypeRef<M> typeRef_;
  private final ThriftConverter<M> converter_;
  private final Base64 base64_ = new Base64();
  private final ThriftToPig<M> thriftToPig_;

  private static final Charset UTF8 = Charset.forName("UTF-8");
  private static final byte RECORD_DELIMITER = (byte)'\n';

  private Pair<String, String> linesRead;
  private Pair<String, String> thriftStructsRead;
  private Pair<String, String> thriftErrors;

  public LzoThriftB64LinePigLoader(String thriftClassName) {
    typeRef_ = ThriftUtils.getTypeRef(thriftClassName);
    converter_ = ThriftConverter.newInstance(typeRef_);
    thriftToPig_ =  ThriftToPig.newInstance(typeRef_);

    String group = "LzoB64Lines of " + typeRef_.getRawClass().getName();
    linesRead = new Pair<String, String>(group, "Lines Read");
    thriftStructsRead = new Pair<String, String>(group, "Thrift Structs");
    thriftErrors = new Pair<String, String>(group, "Errors");

    setLoaderSpec(getClass(), new String[]{thriftClassName});
  }

  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // Since we are not block aligned we throw away the first record of each split and count on a different
    // instance to read it.  The only split this doesn't work for is the first.
    if (!atFirstRecord) {
      getNext();
    }
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    String line;
    Tuple t = null;
    while ((line = is_.readLine(UTF8, RECORD_DELIMITER)) != null) {
      incrCounter(linesRead, 1L);
      M value = converter_.fromBytes(base64_.decode(line.getBytes("UTF-8")));
      if (value != null) {
        try {
          t = thriftToPig_.getPigTuple(value);
          incrCounter(thriftStructsRead, 1L);
          break;
        } catch (TException e) {
          incrCounter(thriftErrors, 1L);
          LOG.warn("ThriftToTuple error :", e); // may be struct mismatch
        }
      }
    }

    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return ThriftToPig.toSchema(typeRef_.getRawClass());
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_110e154_738e6ce/rev_110e154-738e6ce/src/java/com/twitter/elephantbird/pig/load/LzoThriftBlockPigLoader.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.load;

import java.io.IOException;

import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.apache.thrift.TBase;
import org.apache.thrift.TException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.twitter.elephantbird.mapreduce.io.ThriftBlockReader;
import com.twitter.elephantbird.pig.piggybank.ThriftToPig;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;


public class LzoThriftBlockPigLoader<M extends TBase<?, ?>> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoThriftBlockPigLoader.class);

  private final TypeRef<M> typeRef_;
  private final ThriftToPig<M> thriftToPig_;
  private ThriftBlockReader<M> reader_;

  private Pair<String, String> thriftStructsRead;
  private Pair<String, String> thriftErrors;

  public LzoThriftBlockPigLoader(String thriftClassName) {
    typeRef_ = ThriftUtils.getTypeRef(thriftClassName);
    thriftToPig_ =  ThriftToPig.newInstance(typeRef_);

    String group = "LzoBlocks of " + typeRef_.getRawClass().getName();
    thriftStructsRead = new Pair<String, String>(group, "Thrift Structs Read");
    thriftErrors = new Pair<String, String>(group, "Errors");

    setLoaderSpec(getClass(), new String[]{thriftClassName});
  }

  @Override
  public void postBind() throws IOException {
    reader_ = new ThriftBlockReader<M>(is_, typeRef_);
  }

  @Override
  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // We want to explicitly not do any special syncing here, because the reader_
    // handles this automatically.
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    // If we are past the end of the file split, tell the reader not to read any more new blocks.
    // Then continue reading until the last of the reader's already-parsed values are used up.
    // The next split will start at the next sync point and no records will be missed.
    if (is_.getPosition() > end_) {
      reader_.markNoMoreNewBlocks();
    }

    M value;
    while ((value = reader_.readNext()) != null) {
      try {
        Tuple t = thriftToPig_.getPigTuple(value);
        incrCounter(thriftStructsRead, 1L);
        return t;
      } catch (TException e) {
        incrCounter(thriftErrors, 1L);
        LOG.warn("ThriftToTuple error :", e); // may be corrupt data.
        // try next
      }
    }
    return null;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return ThriftToPig.toSchema(typeRef_.getRawClass());
  }
}=======
package com.twitter.elephantbird.pig.load;

import java.io.IOException;

import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.apache.thrift.TBase;
import org.apache.thrift.TException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.twitter.elephantbird.mapreduce.io.ThriftBlockReader;
import com.twitter.elephantbird.pig.util.ThriftToPig;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;


public class LzoThriftBlockPigLoader<M extends TBase<?, ?>> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoThriftBlockPigLoader.class);

  private final TypeRef<M> typeRef_;
  private final ThriftToPig<M> thriftToPig_;
  private ThriftBlockReader<M> reader_;

  private Pair<String, String> thriftStructsRead;
  private Pair<String, String> thriftErrors;

  public LzoThriftBlockPigLoader(String thriftClassName) {
    typeRef_ = ThriftUtils.getTypeRef(thriftClassName);
    thriftToPig_ =  ThriftToPig.newInstance(typeRef_);

    String group = "LzoBlocks of " + typeRef_.getRawClass().getName();
    thriftStructsRead = new Pair<String, String>(group, "Thrift Structs Read");
    thriftErrors = new Pair<String, String>(group, "Errors");

    setLoaderSpec(getClass(), new String[]{thriftClassName});
  }

  @Override
  public void postBind() throws IOException {
    reader_ = new ThriftBlockReader<M>(is_, typeRef_);
  }

  @Override
  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // We want to explicitly not do any special syncing here, because the reader_
    // handles this automatically.
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    // If we are past the end of the file split, tell the reader not to read any more new blocks.
    // Then continue reading until the last of the reader's already-parsed values are used up.
    // The next split will start at the next sync point and no records will be missed.
    if (is_.getPosition() > end_) {
      reader_.markNoMoreNewBlocks();
    }

    M value;
    while ((value = reader_.readNext()) != null) {
      try {
        Tuple t = thriftToPig_.getPigTuple(value);
        incrCounter(thriftStructsRead, 1L);
        return t;
      } catch (TException e) {
        incrCounter(thriftErrors, 1L);
        LOG.warn("ThriftToTuple error :", e); // may be corrupt data.
        // try next
      }
    }
    return null;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return ThriftToPig.toSchema(typeRef_.getRawClass());
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_110e154_738e6ce/rev_110e154-738e6ce/src/java/com/twitter/elephantbird/pig/piggybank/ThriftToPig.java;<<<<<<< MINE
package com.twitter.elephantbird.pig.piggybank;

import java.lang.reflect.Field;
import java.nio.ByteBuffer;
import java.util.ArrayDeque;
import java.util.Deque;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;

import org.apache.pig.LoadFunc;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.apache.thrift.TBase;
import org.apache.thrift.TEnum;
import org.apache.thrift.TException;
import org.apache.thrift.TFieldIdEnum;
import org.apache.thrift.meta_data.EnumMetaData;
import org.apache.thrift.meta_data.FieldMetaData;
import org.apache.thrift.meta_data.FieldValueMetaData;
import org.apache.thrift.meta_data.ListMetaData;
import org.apache.thrift.meta_data.MapMetaData;
import org.apache.thrift.meta_data.SetMetaData;
import org.apache.thrift.meta_data.StructMetaData;
import org.apache.thrift.protocol.TField;
import org.apache.thrift.protocol.TList;
import org.apache.thrift.protocol.TMap;
import org.apache.thrift.protocol.TMessage;
import org.apache.thrift.protocol.TProtocol;
import org.apache.thrift.protocol.TSet;
import org.apache.thrift.protocol.TStruct;
import org.apache.thrift.protocol.TType;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Lists;
import com.google.common.collect.Maps;
import com.twitter.elephantbird.pig8.load.LzoThriftB64LinePigLoader;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;

/**
 * <li> converts a Thrift struct to a Pig tuple
 * <li> utilities to provide schema for Pig loaders and Pig scripts
 */
public class ThriftToPig<M extends TBase<?, ?>> {
  private static final Logger LOG = LoggerFactory.getLogger(ThriftToPig.class);

  /* TODO :
   * 1. Add lazy deserialization like ProtobufTuple does. Not sure if it can be done
   *    efficiently for Thrift.
   * 2. Converting Enum to names (strings) is supported only in the common
   *    case where Enum is part of a struct. Enum used directly in containers
   *    (e.g. list<SomeEnum>) are still integers. The issue is that Thrift
   *    does not explicitly tell that it is writing an Enum. We need to
   *    deduce that from the context. In the case of Structs, we already
   *    maintain this contexts.
   *
   *    In order to support enums-to-strings correctly we need to maintain more
   *    state and we should always know exact context/recursion of Thrift
   *    struct's write() method.
   *
   *    This is certainly do-able. Once we keep track of serialization
   *    so closely, we not far from implementing our own generic write() method.
   *    implementing generic write method will let us deserialize thrift buffer
   *    directly to a Pig Tuple and there is no need to use a Thrift object
   *    as intermediate step. This will also let us support
   *    lazy-deserialization and projections efficiently since we direclty
   *    access the thrift buffer.
   */
  private static BagFactory bagFactory_ = BagFactory.getInstance();
  private static TupleFactory tupleFactory_  = TupleFactory.getInstance();

  private Class<? extends TBase<?, ?>> tClass_;
  private ThriftProtocol tProtocol_ = new ThriftProtocol();
  private Deque<PigContainer> containerStack_ = new ArrayDeque<PigContainer>();
  private PigContainer curContainer_;
  private Tuple curTuple_;

  // We want something that provides a generic interface for populating
  // Pig Tuples, Bags, and Maps. This does the trick.

  private abstract class PigContainer {
    StructDescriptor structDesc; // The current thrift struct being written
    FieldDescriptor curFieldDesc;
    public abstract Object getContents();
    public abstract void add(Object o) throws TException;

    /** set curFieldDesc if the container is is Thrift Struct. */
    public void setCurField(TField tField) throws TException {
      if (structDesc != null) {
        curFieldDesc = structDesc.fieldMap.get(tField.id);
        if (curFieldDesc == null) {
          throw new TException("Unexpected TField " + tField + " for " + tClass_.getName());
        }
      }
    }
  }

  private class TupleWrap extends PigContainer {

    private final Tuple t;

    public TupleWrap(int size) {
      t = tupleFactory_.newTuple(size);
    }

    @Override
    public Object getContents() { return t; }

    @Override
    public void add(Object o) throws TException {
      if (curFieldDesc == null) {
        throw new TException("Internal Error. curFieldDesc is not set");
      }
      if (curFieldDesc.enumMap != null && // map enum to string
          (o = curFieldDesc.enumMap.get(o)) == null) {
        throw new TException("cound not find Enum string");
      }
      try {
        t.set(curFieldDesc.tupleIdx, o);
       } catch (ExecException e) {
          throw new TException(e);
       }
    }
  }

  private class BagWrap extends PigContainer {
    List<Tuple> tuples;

    public BagWrap(int size) {
      tuples =  Lists.newArrayListWithCapacity(size);
    }

    @Override
    public void add(Object o) throws TException {
      // Pig bags contain tuples of objects, so we must wrap a tuple around
      // everything we get.
      if (o instanceof Tuple) {
        tuples.add((Tuple) o);
      } else {
        tuples.add(tupleFactory_.newTuple(o));
      }
    }

    @Override
    public Object getContents() {
      return bagFactory_.newDefaultBag(tuples);
    }
  }

  private class MapWrap extends PigContainer {
    private final Map<String, Object> map;
    String currKey = null;

    public MapWrap(int size) {
      map = new HashMap<String, Object>(size);
    }

    @Override
    public void add(Object o) throws TException {
      //we alternate between String keys and (converted) DataByteArray values.
      if (currKey == null) {
        try {
          currKey = (String) o;
        } catch (ClassCastException e) {
          throw new TException("Only String keys are allowed in maps.");
        }
      } else {
        map.put(currKey, o);
        currKey = null;
      }
    }

    @Override
    public Object getContents() {
      return map;
    }
  }


  private void pushContainer(PigContainer c) {
    containerStack_.addLast(c);
    curContainer_ = c;
  }

  private PigContainer popContainer() throws TException {
    PigContainer c = containerStack_.removeLast();
    curContainer_ = containerStack_.peekLast();
    if (curContainer_ == null) { // All done!
      curTuple_ = (Tuple) c.getContents();
    } else {
      curContainer_.add(c.getContents());
    }
    return c;
  }

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(Class<M> tClass) {
    return new ThriftToPig<M>(tClass);
  }

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(TypeRef<M> typeRef) {
    return new ThriftToPig<M>(typeRef.getRawClass());
  }

  public ThriftToPig(Class<M> tClass) {
    this.tClass_ = tClass;
    structMap = Maps.newHashMap();
    updateStructMap(tClass_);
    structMap = ImmutableMap.copyOf(structMap);
    reset();
  }

  /**
   * The protocol should be reset before each object that is serialized.
   * This is important since, the protocol itself can not reliably
   * realize if it at the beginning of a new object. It can not always
   * rely on the last object being correct written because of
   * any exceptions while processing previous object.
   */
  public void reset() {
    containerStack_.clear();
    curContainer_ = null;
    curTuple_ = null;
  }

  /**
   * Converts a thrift object to Pig tuple.
   * Throws TException in case of any errors.
   */
  public Tuple getPigTuple(M thriftObj) throws TException {
    reset();
    thriftObj.write(tProtocol_);
    if (curTuple_ != null) {
      return curTuple_;
    }
    // unexpected
    throw new TException("Internal error. tuple is not set");
  }

  /**
   * returns 'enum int -> enum name' mapping
   */
  static private Map<Integer, String> extractEnumMap(FieldValueMetaData field) {
    MetaData f = new MetaData(field);
    if (!f.isEnum()) {
      return null;
    }
    Map<Integer, String> map = Maps.newHashMap();
    for(TEnum e : f.getEnumClass().getEnumConstants()) {
      map.put(e.getValue(), e.toString());
    }
    return ImmutableMap.copyOf(map);
  }

  /**
   * holds relevant info for a field in a Thrift Struct including
   * index into tuple array.
   */
  private static class FieldDescriptor {
    TFieldIdEnum fieldEnum;
    int tupleIdx;
    Map<Integer, String> enumMap = null; // set for enums
  }

  /**
   * describes a Thrift struct. Contains following info :
   * <li> Thrift field descriptor map
   * <li> ...
   */
  private static class StructDescriptor {
    Map<Short, FieldDescriptor> fieldMap;

    public StructDescriptor(Class<? extends TBase<?, ?>> tClass) {
      fieldMap = Maps.newHashMap();
      int idx = 0;
      for (Entry<? extends TFieldIdEnum, FieldMetaData> e : FieldMetaData.getStructMetaDataMap(tClass).entrySet()) {
        FieldDescriptor desc = new FieldDescriptor();
        desc.fieldEnum = e.getKey();
        desc.tupleIdx = idx++;
        if (e.getValue().valueMetaData.type == TType.ENUM) {
          desc.enumMap = extractEnumMap(e.getValue().valueMetaData);
        }
        fieldMap.put(desc.fieldEnum.getThriftFieldId(), desc);
      }
      fieldMap = ImmutableMap.copyOf(fieldMap);
    }
  }

  private Map<TStruct, StructDescriptor> structMap;

  private void updateStructMap(Class<? extends TBase<?, ?>> tClass) {
    final TStruct tStruct = getStructDesc(tClass);

    if (structMap.get(tStruct) != null) {
      return;
    }

    StructDescriptor desc = new StructDescriptor(tClass);
    LOG.debug("adding struct descriptor for " + tClass.getName()
        + " with " + desc.fieldMap.size() + " fields");
    structMap.put(tStruct, desc);
    // recursively add any referenced classes.
    for (FieldMetaData field : FieldMetaData.getStructMetaDataMap(tClass).values()) {
      updateStructMap(field.valueMetaData);
    }
  }

  /**
   * Look for any class embedded in the in the container or struct fields
   * and update the struct map with them.
   */
  private void updateStructMap(FieldValueMetaData field) {
    MetaData f = new MetaData(field);

    if (f.isStruct()) {
      updateStructMap(f.getStructClass());
    }

    if (f.isList()) {
      updateStructMap(f.getListElem());
    }

    if (f.isMap()) {
      if (f.getMapKey().type != TType.STRING) {
        throw new IllegalArgumentException("Pig does not support maps with non-string keys "
            + "while initializing ThriftToPig for " + tClass_.getName());
      }
      updateStructMap(f.getMapKey());
      updateStructMap(f.getMapValue());
    }

    if (f.isSet()) {
      updateStructMap(f.getSetElem());
    }
  }

  private class ThriftProtocol extends TProtocol {

    ThriftProtocol() {
      super(null); // this protocol is not used for transport.
    }

    @Override
    public void writeBinary(ByteBuffer bin) throws TException {
      byte[] buf = new byte[bin.remaining()];
      bin.mark();
      bin.get(buf);
      bin.reset();
      curContainer_.add(new DataByteArray(buf));
      /* We could use DataByteArray(byte[], start, end) and avoid a
       * copy here.  But the constructor will make a (quite inefficient) copy.
       */
    }

    @Override
    public void writeBool(boolean b) throws TException {
      curContainer_.add(Integer.valueOf(b ? 1 : 0));
    }

    @Override
    public void writeByte(byte b) throws TException {
      curContainer_.add(Integer.valueOf(b));
    }

    @Override
    public void writeDouble(double dub) throws TException {
      curContainer_.add(Double.valueOf(dub));
    }

    @Override
    public void writeFieldBegin(TField field) throws TException {
      curContainer_.setCurField(field);
    }

    @Override
    public void writeFieldEnd() throws TException {
    }

    @Override
    public void writeFieldStop() throws TException {
    }

    @Override
    public void writeI16(short i16) throws TException {
      curContainer_.add(Integer.valueOf(i16));
    }

    @Override
    public void writeI32(int i32) throws TException {
      curContainer_.add(i32);
    }

    @Override
    public void writeI64(long i64) throws TException {
      curContainer_.add(i64);
    }

    @Override
    public void writeListBegin(TList list) throws TException {
      pushContainer(new BagWrap(list.size));
    }

    @Override
    public void writeListEnd() throws TException {
      popContainer();
    }

    @Override
    public void writeMapBegin(TMap map) throws TException {
      pushContainer(new MapWrap(map.size));
    }

    @Override
    public void writeMapEnd() throws TException {
      popContainer();
    }

    @Override
    public void writeSetBegin(TSet set) throws TException {
      pushContainer(new BagWrap(set.size));
    }

    @Override
    public void writeSetEnd() throws TException {
      popContainer();
    }

    @Override
    public void writeString(String str) throws TException {
      curContainer_.add(str);
    }

    @Override
    public void writeStructBegin(TStruct struct) throws TException {
      StructDescriptor desc = structMap.get(struct);
      if (desc == null) {
        throw new TException("Unexpected TStruct " + struct.name + " for " + tClass_.getName());
      }
      PigContainer c = new TupleWrap(desc.fieldMap.size());
      c.structDesc = desc;
      pushContainer(c);
    }

    @Override
    public void writeStructEnd() throws TException {
      popContainer();
    }

    @Override
    public void writeMessageBegin(TMessage message) throws TException {
      throw new TException("method not implemented.");
    }
    @Override
    public void writeMessageEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public ByteBuffer readBinary() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public boolean readBool() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public byte readByte() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public double readDouble() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TField readFieldBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readFieldEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public short readI16() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public int readI32() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public long readI64() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TList readListBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readListEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TMap readMapBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readMapEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TMessage readMessageBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readMessageEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TSet readSetBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readSetEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public String readString() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TStruct readStructBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readStructEnd() throws TException {
      throw new TException("method not implemented.");
    }
  }

  /**
   * A utility class to help with type checking of a ThriftField.
   * Avoids checking type and not-so-readable casting in many places.
   */
  static class MetaData {
    final FieldValueMetaData field;

    MetaData(FieldValueMetaData field) {
      this.field = field;
    }

    FieldValueMetaData getField() {
      return field;
    }

    // List
    boolean isList() {
      return field instanceof ListMetaData;
    }

    FieldValueMetaData getListElem() {
      return ((ListMetaData)field).elemMetaData;
    }

    // Enum
    boolean isEnum() {
      return field instanceof EnumMetaData;
    }

    Class<? extends TEnum> getEnumClass() {
      return ((EnumMetaData)field).enumClass;
    }

    // Map
    boolean isMap() {
      return field instanceof MapMetaData;
    }

    FieldValueMetaData getMapKey() {
      return ((MapMetaData)field).keyMetaData;
    }

    FieldValueMetaData getMapValue() {
      return ((MapMetaData)field).valueMetaData;
    }

    // Set
    boolean isSet() {
      return field instanceof SetMetaData;
    }

    FieldValueMetaData getSetElem() {
      return ((SetMetaData)field).elemMetaData;
    }

    // Struct
    boolean isStruct() {
      return field instanceof StructMetaData;
    }

    @SuppressWarnings("unchecked")
    Class<? extends TBase<?, ?>> getStructClass() {
      return (Class <? extends TBase<?, ?>>)((StructMetaData)field).structClass;
    }
  }

  /**
   * Returns Pig schema for the Thrift struct.
   */
  public static Schema toSchema(Class<? extends TBase<?, ?>> tClass) {
    Schema schema = new Schema();

    try {
      for (Entry<? extends TFieldIdEnum, FieldMetaData> e : FieldMetaData.getStructMetaDataMap(tClass).entrySet()) {
        FieldMetaData meta = e.getValue();
        FieldValueMetaData field = e.getValue().valueMetaData;
        MetaData fm = new MetaData(field);
        if (fm.isStruct()) {
          schema.add(new FieldSchema(meta.fieldName, toSchema(fm.getStructClass()), DataType.TUPLE));
        } else if (fm.isEnum()) { // enums in Structs are strings (enums in containers are not, yet)
          schema.add(new FieldSchema(meta.fieldName, null, DataType.CHARARRAY));
        } else {
          schema.add(singleFieldToFieldSchema(meta.fieldName, field));
        }
      }
    } catch (FrontendException t) {
      throw new RuntimeException(t);
    }

    return schema;
  }

  private static FieldSchema singleFieldToFieldSchema(String fieldName, FieldValueMetaData field) throws FrontendException {

    MetaData fm = new MetaData(field);

    switch (field.type) {
      case TType.LIST:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", fm.getListElem()), DataType.BAG);
      case TType.SET:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", fm.getSetElem()), DataType.BAG);
      case TType.MAP:
        // can not specify types for maps in Pig.
        return new FieldSchema(fieldName, null, DataType.MAP);
      default:
        return new FieldSchema(fieldName, null, getPigDataType(field));
    }
  }

  /**
   * Returns a schema with single tuple (for Pig bags).
   */
  private static Schema singleFieldToTupleSchema(String fieldName, FieldValueMetaData field) throws FrontendException {
    MetaData fm = new MetaData(field);
    FieldSchema fieldSchema = null;

    switch (field.type) {
      case TType.STRUCT:
        fieldSchema = new FieldSchema(fieldName, toSchema(fm.getStructClass()), DataType.TUPLE);
        break;
      case TType.LIST:
        fieldSchema = singleFieldToFieldSchema(fieldName, fm.getListElem());
        break;
      case TType.SET:
        fieldSchema = singleFieldToFieldSchema(fieldName, fm.getSetElem());
        break;
      default:
        fieldSchema = new FieldSchema(fieldName, null, getPigDataType(fm.getField()));
    }

    Schema schema = new Schema();
    schema.add(fieldSchema);
    return schema;
  }

  private static byte getPigDataType(FieldValueMetaData field) {
    switch (field.type) {
      case TType.BOOL:
      case TType.BYTE:
      case TType.I16:
      case TType.I32:
      case TType.ENUM: // will revisit this once Enums in containers are also strings.
        return DataType.INTEGER;
      case TType.I64:
        return DataType.LONG;
      case TType.STRING:
        return DataType.CHARARRAY;
      default:
        throw new IllegalArgumentException("Unexpected type where a simple type is expected : " + field.type);
    }
  }

  /**
   * Turn a Thrift Struct into a loading schema for a pig script.
   */
  public static String toPigScript(Class<? extends TBase<?, ?>> thriftClass,
                                   Class<? extends LoadFunc> pigLoader) {
    StringBuilder sb = new StringBuilder();
    /* we are commenting out explicit schema specification. The schema is
     * included mainly to help the readers of the pig script. Pig learns the
     * schema directly from the loader.
     * If explicit schema is not commented, we might have surprising results
     * when a Thrift class (possibly in control of another team) changes,
     * but the Pig script is not updated. Commenting it out work around this.
     */
    StringBuilder prefix = new StringBuilder("       --  ");
    sb.append("raw_data = load '$INPUT_FILES' using ")
      .append(pigLoader.getName())
      .append("('")
      .append(thriftClass.getName())
      .append("');\n")
      .append(prefix)
      .append("as ");
    prefix.append("   ");

    try {
      stringifySchema(sb, toSchema(thriftClass), DataType.TUPLE, prefix);
    } catch (FrontendException e) {
      throw new RuntimeException(e);
    }

    sb.append("\n");
    return sb.toString();
  }

  /**
   * Print formatted schema. This is a modified version of
   * {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
   * with support for (indented) pretty printing.
   */
  // This is used for building up output string
  // type can only be BAG or TUPLE
  public static void stringifySchema(StringBuilder sb,
                                     Schema schema,
                                     byte type,
                                     StringBuilder prefix)
                                          throws FrontendException{
      // this is a modified version of {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
      if (type == DataType.TUPLE) {
          sb.append("(") ;
      }
      else if (type == DataType.BAG) {
          sb.append("{") ;
      }

      prefix.append("  ");
      sb.append("\n").append(prefix);

      if (schema == null) {
          sb.append("null") ;
      }
      else {
          boolean isFirst = true ;
          for (int i=0; i< schema.size() ;i++) {

              if (!isFirst) {
                  sb.append(",\n").append(prefix);
              }
              else {
                  isFirst = false ;
              }

              FieldSchema fs = schema.getField(i) ;

              if(fs == null) {
                  sb.append("null");
                  continue;
              }

              if (fs.alias != null) {
                  sb.append(fs.alias);
                  sb.append(": ");
              }

              if (DataType.isAtomic(fs.type)) {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
              else if ( (fs.type == DataType.TUPLE) ||
                        (fs.type == DataType.BAG) ) {
                  // safety net
                  if (schema != fs.schema) {
                      stringifySchema(sb, fs.schema, fs.type, prefix) ;
                  }
                  else {
                      throw new AssertionError("Schema refers to itself "
                                               + "as inner schema") ;
                  }
              } else if (fs.type == DataType.MAP) {
                  sb.append(DataType.findTypeName(fs.type) + "[ ]") ;
              } else {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
          }
      }

      prefix.setLength(prefix.length()-2);
      sb.append("\n").append(prefix);

      if (type == DataType.TUPLE) {
          sb.append(")") ;
      }
      else if (type == DataType.BAG) {
          sb.append("}") ;
      }
  }

  private TStruct getStructDesc(Class<? extends TBase<?, ?>> tClass) {
    // hack to get hold of STRUCT_DESC of a thrift class:
    // Access 'private static final' field STRUCT_DESC using reflection.
    // Bad practice, but not sure if there is a better way.
    try {
      Field f = tClass.getDeclaredField("STRUCT_DESC");
      f.setAccessible(true);
      return (TStruct) f.get(null);
    } catch (Throwable t) {
      throw new RuntimeException(t);
    }
  }

  public static void main(String[] args) throws Exception {
    if (args.length > 0) {
      Class<? extends TBase<?, ?>> tClass = ThriftUtils.getTypeRef(args[0]).getRawClass();
      System.out.println(args[0] + " : " + toSchema(tClass).toString());
      System.out.println(toPigScript(tClass, LzoThriftB64LinePigLoader.class));
    }
  }
}||||||| BASE
package com.twitter.elephantbird.pig.piggybank;

import java.lang.reflect.Field;
import java.nio.ByteBuffer;
import java.util.ArrayDeque;
import java.util.Deque;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;

import org.apache.pig.LoadFunc;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.apache.thrift.TBase;
import org.apache.thrift.TEnum;
import org.apache.thrift.TException;
import org.apache.thrift.TFieldIdEnum;
import org.apache.thrift.meta_data.EnumMetaData;
import org.apache.thrift.meta_data.FieldMetaData;
import org.apache.thrift.meta_data.FieldValueMetaData;
import org.apache.thrift.meta_data.ListMetaData;
import org.apache.thrift.meta_data.MapMetaData;
import org.apache.thrift.meta_data.SetMetaData;
import org.apache.thrift.meta_data.StructMetaData;
import org.apache.thrift.protocol.TField;
import org.apache.thrift.protocol.TList;
import org.apache.thrift.protocol.TMap;
import org.apache.thrift.protocol.TMessage;
import org.apache.thrift.protocol.TProtocol;
import org.apache.thrift.protocol.TSet;
import org.apache.thrift.protocol.TStruct;
import org.apache.thrift.protocol.TType;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Lists;
import com.google.common.collect.Maps;
import com.twitter.elephantbird.pig.load.LzoThriftB64LinePigLoader;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;

/**
 * <li> converts a Thrift struct to a Pig tuple
 * <li> utilities to provide schema for Pig loaders and Pig scripts
 */
public class ThriftToPig<M extends TBase<?, ?>> {
  private static final Logger LOG = LoggerFactory.getLogger(ThriftToPig.class);

  /* TODO :
   * 1. Add lazy deserialization like ProtobufTuple does. Not sure if it can be done
   *    efficiently for Thrift.
   * 2. Converting Enum to names (strings) is supported only in the common
   *    case where Enum is part of a struct. Enum used directly in containers
   *    (e.g. list<SomeEnum>) are still integers. The issue is that Thrift
   *    does not explicitly tell that it is writing an Enum. We need to
   *    deduce that from the context. In the case of Structs, we already
   *    maintain this contexts.
   *
   *    In order to support enums-to-strings correctly we need to maintain more
   *    state and we should always know exact context/recursion of Thrift
   *    struct's write() method.
   *
   *    This is certainly do-able. Once we keep track of serialization
   *    so closely, we not far from implementing our own generic write() method.
   *    implementing generic write method will let us deserialize thrift buffer
   *    directly to a Pig Tuple and there is no need to use a Thrift object
   *    as intermediate step. This will also let us support
   *    lazy-deserialization and projections efficiently since we direclty
   *    access the thrift buffer.
   */
  private static BagFactory bagFactory_ = BagFactory.getInstance();
  private static TupleFactory tupleFactory_  = TupleFactory.getInstance();

  private Class<? extends TBase<?, ?>> tClass_;
  private ThriftProtocol tProtocol_ = new ThriftProtocol();
  private Deque<PigContainer> containerStack_ = new ArrayDeque<PigContainer>();
  private PigContainer curContainer_;
  private Tuple curTuple_;

  // We want something that provides a generic interface for populating
  // Pig Tuples, Bags, and Maps. This does the trick.

  private abstract class PigContainer {
    StructDescriptor structDesc; // The current thrift struct being written
    FieldDescriptor curFieldDesc;
    public abstract Object getContents();
    public abstract void add(Object o) throws TException;

    /** set curFieldDesc if the container is is Thrift Struct. */
    public void setCurField(TField tField) throws TException {
      if (structDesc != null) {
        curFieldDesc = structDesc.fieldMap.get(tField.id);
        if (curFieldDesc == null) {
          throw new TException("Unexpected TField " + tField + " for " + tClass_.getName());
        }
      }
    }
  }

  private class TupleWrap extends PigContainer {

    private final Tuple t;

    public TupleWrap(int size) {
      t = tupleFactory_.newTuple(size);
    }

    public Object getContents() { return t; }

    public void add(Object o) throws TException {
      if (curFieldDesc == null) {
        throw new TException("Internal Error. curFieldDesc is not set");
      }
      if (curFieldDesc.enumMap != null && // map enum to string
          (o = curFieldDesc.enumMap.get(o)) == null) {
        throw new TException("cound not find Enum string");
      }
      try {
        t.set(curFieldDesc.tupleIdx, o);
       } catch (ExecException e) {
          throw new TException(e);
       }
    }
  }

  private class BagWrap extends PigContainer {
    List<Tuple> tuples;

    public BagWrap(int size) {
      tuples =  Lists.newArrayListWithCapacity(size);
    }

    @Override
    public void add(Object o) throws TException {
      // Pig bags contain tuples of objects, so we must wrap a tuple around
      // everything we get.
      if (o instanceof Tuple) {
        tuples.add((Tuple) o);
      } else {
        tuples.add(tupleFactory_.newTuple(o));
      }
    }

    @Override
    public Object getContents() {
      return bagFactory_.newDefaultBag(tuples);
    }
  }

  private class MapWrap extends PigContainer {
    private final Map<String, Object> map;
    String currKey = null;

    public MapWrap(int size) {
      map = new HashMap<String, Object>(size);
    }

    @Override
    public void add(Object o) throws TException {
      //we alternate between String keys and (converted) DataByteArray values.
      if (currKey == null) {
        try {
          currKey = (String) o;
        } catch (ClassCastException e) {
          throw new TException("Only String keys are allowed in maps.");
        }
      } else {
        map.put(currKey, o);
        currKey = null;
      }
    }

    @Override
    public Object getContents() {
      return map;
    }
  }


  private void pushContainer(PigContainer c) {
    containerStack_.addLast(c);
    curContainer_ = c;
  }

  private PigContainer popContainer() throws TException {
    PigContainer c = containerStack_.removeLast();
    curContainer_ = containerStack_.peekLast();
    if (curContainer_ == null) { // All done!
      curTuple_ = (Tuple) c.getContents();
    } else {
      curContainer_.add(c.getContents());
    }
    return c;
  }

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(Class<M> tClass) {
    return new ThriftToPig<M>(tClass);
  }

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(TypeRef<M> typeRef) {
    return new ThriftToPig<M>(typeRef.getRawClass());
  }

  public ThriftToPig(Class<M> tClass) {
    this.tClass_ = tClass;
    structMap = Maps.newHashMap();
    updateStructMap(tClass_);
    structMap = ImmutableMap.copyOf(structMap);
    reset();
  }

  /**
   * The protocol should be reset before each object that is serialized.
   * This is important since, the protocol itself can not reliably
   * realize if it at the beginning of a new object. It can not always
   * rely on the last object being correct written because of
   * any exceptions while processing previous object.
   */
  public void reset() {
    containerStack_.clear();
    curContainer_ = null;
    curTuple_ = null;
  }

  /**
   * Converts a thrift object to Pig tuple.
   * Throws TException in case of any errors.
   */
  public Tuple getPigTuple(M thriftObj) throws TException {
    reset();
    thriftObj.write(tProtocol_);
    if (curTuple_ != null) {
      return curTuple_;
    }
    // unexpected
    throw new TException("Internal error. tuple is not set");
  }

  /**
   * returns 'enum int -> enum name' mapping
   */
  static private Map<Integer, String> extractEnumMap(FieldValueMetaData field) {
    MetaData f = new MetaData(field);
    if (!f.isEnum()) {
      return null;
    }
    Map<Integer, String> map = Maps.newHashMap();
    for(TEnum e : f.getEnumClass().getEnumConstants()) {
      map.put(e.getValue(), e.toString());
    }
    return ImmutableMap.copyOf(map);
  }

  /**
   * holds relevant info for a field in a Thrift Struct including
   * index into tuple array.
   */
  private static class FieldDescriptor {
    TFieldIdEnum fieldEnum;
    int tupleIdx;
    Map<Integer, String> enumMap = null; // set for enums
  }

  /**
   * describes a Thrift struct. Contains following info :
   * <li> Thrift field descriptor map
   * <li> ...
   */
  private static class StructDescriptor {
    Map<Short, FieldDescriptor> fieldMap;

    public StructDescriptor(Class<? extends TBase<?, ?>> tClass) {
      fieldMap = Maps.newHashMap();
      int idx = 0;
      for (Entry<? extends TFieldIdEnum, FieldMetaData> e : FieldMetaData.getStructMetaDataMap(tClass).entrySet()) {
        FieldDescriptor desc = new FieldDescriptor();
        desc.fieldEnum = e.getKey();
        desc.tupleIdx = idx++;
        if (e.getValue().valueMetaData.type == TType.ENUM) {
          desc.enumMap = extractEnumMap(e.getValue().valueMetaData);
        }
        fieldMap.put(desc.fieldEnum.getThriftFieldId(), desc);
      }
      fieldMap = ImmutableMap.copyOf(fieldMap);
    }
  }

  private Map<TStruct, StructDescriptor> structMap;

  private void updateStructMap(Class<? extends TBase<?, ?>> tClass) {
    final TStruct tStruct = getStructDesc(tClass);

    if (structMap.get(tStruct) != null) {
      return;
    }

    StructDescriptor desc = new StructDescriptor(tClass);
    LOG.debug("adding struct descriptor for " + tClass.getName()
        + " with " + desc.fieldMap.size() + " fields");
    structMap.put(tStruct, desc);
    // recursively add any referenced classes.
    for (FieldMetaData field : FieldMetaData.getStructMetaDataMap(tClass).values()) {
      updateStructMap(field.valueMetaData);
    }
  }

  /**
   * Look for any class embedded in the in the container or struct fields
   * and update the struct map with them.
   */
  private void updateStructMap(FieldValueMetaData field) {
    MetaData f = new MetaData(field);

    if (f.isStruct()) {
      updateStructMap(f.getStructClass());
    }

    if (f.isList()) {
      updateStructMap(f.getListElem());
    }

    if (f.isMap()) {
      if (f.getMapKey().type != TType.STRING) {
        throw new IllegalArgumentException("Pig does not support maps with non-string keys "
            + "while initializing ThriftToPig for " + tClass_.getName());
      }
      updateStructMap(f.getMapKey());
      updateStructMap(f.getMapValue());
    }

    if (f.isSet()) {
      updateStructMap(f.getSetElem());
    }
  }

  private class ThriftProtocol extends TProtocol {

    ThriftProtocol() {
      super(null); // this protocol is not used for transport.
    }

    @Override
    public void writeBinary(ByteBuffer bin) throws TException {
      byte[] buf = new byte[bin.remaining()];
      bin.mark();
      bin.get(buf);
      bin.reset();
      curContainer_.add(new DataByteArray(buf));
      /* We could use DataByteArray(byte[], start, end) and avoid a
       * copy here.  But the constructor will make a (quite inefficient) copy.
       */
    }

    @Override
    public void writeBool(boolean b) throws TException {
      curContainer_.add(Integer.valueOf(b ? 1 : 0));
    }

    @Override
    public void writeByte(byte b) throws TException {
      curContainer_.add(Integer.valueOf(b));
    }

    @Override
    public void writeDouble(double dub) throws TException {
      curContainer_.add(Double.valueOf(dub));
    }

    @Override
    public void writeFieldBegin(TField field) throws TException {
      curContainer_.setCurField(field);
    }

    @Override
    public void writeFieldEnd() throws TException {
    }

    @Override
    public void writeFieldStop() throws TException {
    }

    @Override
    public void writeI16(short i16) throws TException {
      curContainer_.add(Integer.valueOf(i16));
    }

    @Override
    public void writeI32(int i32) throws TException {
      curContainer_.add(i32);
    }

    @Override
    public void writeI64(long i64) throws TException {
      curContainer_.add(i64);
    }

    @Override
    public void writeListBegin(TList list) throws TException {
      pushContainer(new BagWrap(list.size));
    }

    @Override
    public void writeListEnd() throws TException {
      popContainer();
    }

    @Override
    public void writeMapBegin(TMap map) throws TException {
      pushContainer(new MapWrap(map.size));
    }

    @Override
    public void writeMapEnd() throws TException {
      popContainer();
    }

    @Override
    public void writeSetBegin(TSet set) throws TException {
      pushContainer(new BagWrap(set.size));
    }

    @Override
    public void writeSetEnd() throws TException {
      popContainer();
    }

    @Override
    public void writeString(String str) throws TException {
      curContainer_.add(str);
    }

    @Override
    public void writeStructBegin(TStruct struct) throws TException {
      StructDescriptor desc = structMap.get(struct);
      if (desc == null) {
        throw new TException("Unexpected TStruct " + struct.name + " for " + tClass_.getName());
      }
      PigContainer c = new TupleWrap(desc.fieldMap.size());
      c.structDesc = desc;
      pushContainer(c);
    }

    @Override
    public void writeStructEnd() throws TException {
      popContainer();
    }

    @Override
    public void writeMessageBegin(TMessage message) throws TException {
      throw new TException("method not implemented.");
    }
    @Override
    public void writeMessageEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public ByteBuffer readBinary() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public boolean readBool() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public byte readByte() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public double readDouble() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TField readFieldBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readFieldEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public short readI16() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public int readI32() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public long readI64() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TList readListBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readListEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TMap readMapBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readMapEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TMessage readMessageBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readMessageEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TSet readSetBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readSetEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public String readString() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TStruct readStructBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readStructEnd() throws TException {
      throw new TException("method not implemented.");
    }
  }

  /**
   * A utility class to help with type checking of a ThriftField.
   * Avoids checking type and not-so-readable casting in many places.
   */
  static class MetaData {
    final FieldValueMetaData field;

    MetaData(FieldValueMetaData field) {
      this.field = field;
    }

    FieldValueMetaData getField() {
      return field;
    }

    // List
    boolean isList() {
      return field instanceof ListMetaData;
    }

    FieldValueMetaData getListElem() {
      return ((ListMetaData)field).elemMetaData;
    }

    // Enum
    boolean isEnum() {
      return field instanceof EnumMetaData;
    }

    Class<? extends TEnum> getEnumClass() {
      return ((EnumMetaData)field).enumClass;
    }

    // Map
    boolean isMap() {
      return field instanceof MapMetaData;
    }

    FieldValueMetaData getMapKey() {
      return ((MapMetaData)field).keyMetaData;
    }

    FieldValueMetaData getMapValue() {
      return ((MapMetaData)field).valueMetaData;
    }

    // Set
    boolean isSet() {
      return field instanceof SetMetaData;
    }

    FieldValueMetaData getSetElem() {
      return ((SetMetaData)field).elemMetaData;
    }

    // Struct
    boolean isStruct() {
      return field instanceof StructMetaData;
    }

    @SuppressWarnings("unchecked")
    Class<? extends TBase<?, ?>> getStructClass() {
      return (Class <? extends TBase<?, ?>>)((StructMetaData)field).structClass;
    }
  }

  /**
   * Returns Pig schema for the Thrift struct.
   */
  public static Schema toSchema(Class<? extends TBase<?, ?>> tClass) {
    Schema schema = new Schema();

    try {
      for (Entry<? extends TFieldIdEnum, FieldMetaData> e : FieldMetaData.getStructMetaDataMap(tClass).entrySet()) {
        FieldMetaData meta = e.getValue();
        FieldValueMetaData field = e.getValue().valueMetaData;
        MetaData fm = new MetaData(field);
        if (fm.isStruct()) {
          schema.add(new FieldSchema(meta.fieldName, toSchema(fm.getStructClass()), DataType.TUPLE));
        } else if (fm.isEnum()) { // enums in Structs are strings (enums in containers are not, yet)
          schema.add(new FieldSchema(meta.fieldName, null, DataType.CHARARRAY));
        } else {
          schema.add(singleFieldToFieldSchema(meta.fieldName, field));
        }
      }
    } catch (FrontendException t) {
      throw new RuntimeException(t);
    }

    return schema;
  }

  private static FieldSchema singleFieldToFieldSchema(String fieldName, FieldValueMetaData field) throws FrontendException {

    MetaData fm = new MetaData(field);

    switch (field.type) {
      case TType.LIST:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", fm.getListElem()), DataType.BAG);
      case TType.SET:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", fm.getSetElem()), DataType.BAG);
      case TType.MAP:
        // can not specify types for maps in Pig.
        return new FieldSchema(fieldName, null, DataType.MAP);
      default:
        return new FieldSchema(fieldName, null, getPigDataType(field));
    }
  }

  /**
   * Returns a schema with single tuple (for Pig bags).
   */
  private static Schema singleFieldToTupleSchema(String fieldName, FieldValueMetaData field) throws FrontendException {
    MetaData fm = new MetaData(field);
    FieldSchema fieldSchema = null;

    switch (field.type) {
      case TType.STRUCT:
        fieldSchema = new FieldSchema(fieldName, toSchema(fm.getStructClass()), DataType.TUPLE);
        break;
      case TType.LIST:
        fieldSchema = singleFieldToFieldSchema(fieldName, fm.getListElem());
        break;
      case TType.SET:
        fieldSchema = singleFieldToFieldSchema(fieldName, fm.getSetElem());
        break;
      default:
        fieldSchema = new FieldSchema(fieldName, null, getPigDataType(fm.getField()));
    }

    Schema schema = new Schema();
    schema.add(fieldSchema);
    return schema;
  }

  private static byte getPigDataType(FieldValueMetaData field) {
    switch (field.type) {
      case TType.BOOL:
      case TType.BYTE:
      case TType.I16:
      case TType.I32:
      case TType.ENUM: // will revisit this once Enums in containers are also strings.
        return DataType.INTEGER;
      case TType.I64:
        return DataType.LONG;
      case TType.STRING:
        return DataType.CHARARRAY;
      default:
        throw new IllegalArgumentException("Unexpected type where a simple type is expected : " + field.type);
    }
  }

  /**
   * Turn a Thrift Struct into a loading schema for a pig script.
   */
  public static String toPigScript(Class<? extends TBase<?, ?>> thriftClass,
                                   Class<? extends LoadFunc> pigLoader) {
    StringBuilder sb = new StringBuilder();
    /* we are commenting out explicit schema specification. The schema is
     * included mainly to help the readers of the pig script. Pig learns the
     * schema directly from the loader.
     * If explicit schema is not commented, we might have surprising results
     * when a Thrift class (possibly in control of another team) changes,
     * but the Pig script is not updated. Commenting it out work around this.
     */
    StringBuilder prefix = new StringBuilder("       --  ");
    sb.append("raw_data = load '$INPUT_FILES' using ")
      .append(pigLoader.getName())
      .append("('")
      .append(thriftClass.getName())
      .append("');\n")
      .append(prefix)
      .append("as ");
    prefix.append("   ");

    try {
      stringifySchema(sb, toSchema(thriftClass), DataType.TUPLE, prefix);
    } catch (FrontendException e) {
      throw new RuntimeException(e);
    }

    sb.append("\n");
    return sb.toString();
  }

  /**
   * Print formatted schema. This is a modified version of
   * {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
   * with support for (indented) pretty printing.
   */
  // This is used for building up output string
  // type can only be BAG or TUPLE
  public static void stringifySchema(StringBuilder sb,
                                     Schema schema,
                                     byte type,
                                     StringBuilder prefix)
                                          throws FrontendException{
      // this is a modified version of {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
      if (type == DataType.TUPLE) {
          sb.append("(") ;
      }
      else if (type == DataType.BAG) {
          sb.append("{") ;
      }

      prefix.append("  ");
      sb.append("\n").append(prefix);

      if (schema == null) {
          sb.append("null") ;
      }
      else {
          boolean isFirst = true ;
          for (int i=0; i< schema.size() ;i++) {

              if (!isFirst) {
                  sb.append(",\n").append(prefix);
              }
              else {
                  isFirst = false ;
              }

              FieldSchema fs = schema.getField(i) ;

              if(fs == null) {
                  sb.append("null");
                  continue;
              }

              if (fs.alias != null) {
                  sb.append(fs.alias);
                  sb.append(": ");
              }

              if (DataType.isAtomic(fs.type)) {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
              else if ( (fs.type == DataType.TUPLE) ||
                        (fs.type == DataType.BAG) ) {
                  // safety net
                  if (schema != fs.schema) {
                      stringifySchema(sb, fs.schema, fs.type, prefix) ;
                  }
                  else {
                      throw new AssertionError("Schema refers to itself "
                                               + "as inner schema") ;
                  }
              } else if (fs.type == DataType.MAP) {
                  sb.append(DataType.findTypeName(fs.type) + "[ ]") ;
              } else {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
          }
      }

      prefix.setLength(prefix.length()-2);
      sb.append("\n").append(prefix);

      if (type == DataType.TUPLE) {
          sb.append(")") ;
      }
      else if (type == DataType.BAG) {
          sb.append("}") ;
      }
  }

  private TStruct getStructDesc(Class<? extends TBase<?, ?>> tClass) {
    // hack to get hold of STRUCT_DESC of a thrift class:
    // Access 'private static final' field STRUCT_DESC using reflection.
    // Bad practice, but not sure if there is a better way.
    try {
      Field f = tClass.getDeclaredField("STRUCT_DESC");
      f.setAccessible(true);
      return (TStruct) f.get(null);
    } catch (Throwable t) {
      throw new RuntimeException(t);
    }
  }

  public static void main(String[] args) throws Exception {
    if (args.length > 0) {
      Class<? extends TBase<?, ?>> tClass = ThriftUtils.getTypeRef(args[0]).getRawClass();
      System.out.println(args[0] + " : " + toSchema(tClass).toString());
      System.out.println(toPigScript(tClass, LzoThriftB64LinePigLoader.class));
    }
  }
}=======
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_110e154_738e6ce/rev_110e154-738e6ce/src/java/com/twitter/elephantbird/pig/piggybank/BytesToThriftTuple.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.piggybank;

import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.thrift.TBase;
import org.apache.thrift.TDeserializer;
import org.apache.thrift.TException;
import org.apache.thrift.protocol.TBinaryProtocol;
import org.apache.thrift.transport.TMemoryBuffer;

import com.twitter.elephantbird.util.TypeRef;

/**
 * This is an abstract UDF for converting serialized Thrift objects into Pig tuples.
 * To create a converter for your Thrift class <code>MyThriftClass</code>, you simply need to extend
 * <code>BytesToThriftTuple</code> with something like this:
 *<pre>
 * {@code
 * public class BytesToSimpleLocation extends BytesToThriftTuple<MyThriftClass> {
 *
 *   public BytesToSimpleLocation() {
 *     setTypeRef(new TypeRef<MyThriftClass>() {});
 *   }
 * }}
 *</pre>
 */
public abstract class BytesToThriftTuple<T extends TBase<?, ?>> extends EvalFunc<Tuple> {

  private final TDeserializer deserializer_ = new TDeserializer(new TBinaryProtocol.Factory());
  private ThriftToPig<T> thriftToTuple_;
  private TypeRef<T> typeRef_;

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called by the constructor!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<T> typeRef) {
    typeRef_ = typeRef;
    thriftToTuple_ = ThriftToPig.newInstance(typeRef);
  }


  @Override
  public Tuple exec(org.apache.pig.data.Tuple input) throws IOException {
    if (input == null || input.size() < 1) return null;
    try {
      T tObj = typeRef_.safeNewInstance();
      DataByteArray dbarr = (DataByteArray) input.get(0);
      deserializer_.deserialize(tObj, dbarr.get());
      return thriftToTuple_.getPigTuple(tObj);
    } catch (IOException e) {
      log.warn("Caught exception "+e.getMessage());
      return null;
    } catch (TException e) {
      log.warn("Unable to deserialize Thrift object: "+e);
      return null;
    }
  }
}=======
package com.twitter.elephantbird.pig.piggybank;

import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.thrift.TBase;
import org.apache.thrift.TDeserializer;
import org.apache.thrift.TException;
import org.apache.thrift.protocol.TBinaryProtocol;
import org.apache.thrift.transport.TMemoryBuffer;

import com.twitter.elephantbird.pig.util.ThriftToPig;
import com.twitter.elephantbird.util.TypeRef;

/**
 * This is an abstract UDF for converting serialized Thrift objects into Pig tuples.
 * To create a converter for your Thrift class <code>MyThriftClass</code>, you simply need to extend
 * <code>BytesToThriftTuple</code> with something like this:
 *<pre>
 * {@code
 * public class BytesToSimpleLocation extends BytesToThriftTuple<MyThriftClass> {
 *
 *   public BytesToSimpleLocation() {
 *     setTypeRef(new TypeRef<MyThriftClass>() {});
 *   }
 * }}
 *</pre>
 */
public abstract class BytesToThriftTuple<T extends TBase<?, ?>> extends EvalFunc<Tuple> {

  private final TDeserializer deserializer_ = new TDeserializer(new TBinaryProtocol.Factory());
  private ThriftToPig<T> thriftToTuple_;
  private TypeRef<T> typeRef_;

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called by the constructor!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<T> typeRef) {
    typeRef_ = typeRef;
    thriftToTuple_ = ThriftToPig.newInstance(typeRef);
  }


  @Override
  public Tuple exec(org.apache.pig.data.Tuple input) throws IOException {
    if (input == null || input.size() < 1) return null;
    try {
      T tObj = typeRef_.safeNewInstance();
      DataByteArray dbarr = (DataByteArray) input.get(0);
      deserializer_.deserialize(tObj, dbarr.get());
      return thriftToTuple_.getPigTuple(tObj);
    } catch (IOException e) {
      log.warn("Caught exception "+e.getMessage());
      return null;
    } catch (TException e) {
      log.warn("Unable to deserialize Thrift object: "+e);
      return null;
    }
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_110e154_738e6ce/rev_110e154-738e6ce/src/java/com/twitter/elephantbird/pig/piggybank/ProtobufBytesToTuple.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.piggybank;

import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;

import com.google.common.base.Function;
import com.google.protobuf.Message;
import com.twitter.elephantbird.mapreduce.io.ProtobufConverter;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * The base class for a Pig UDF that takes as input a tuple containing a single element, the
 * bytes of a serialized protocol buffer as a DataByteArray.  It outputs the protobuf in
 * expanded form.  The specific protocol buffer is a template parameter, generally specified by a
 * codegen'd derived class. See com.twitter.elephantbird.proto.HadoopProtoCodeGenerator.
 */
public abstract class ProtobufBytesToTuple<M extends Message> extends EvalFunc<Tuple> {
  private TypeRef<M> typeRef_ = null;
  private ProtobufConverter<M> protoConverter_ = null;
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  /**
   * Set the type parameter so it doesn't get erased by Java. Must be called during
   * initialization.
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    protoConverter_ = ProtobufConverter.newInstance(typeRef);
  }

  @Override
  public Tuple exec(Tuple input) throws IOException {
    if (input == null || input.size() < 1) return null;
    try {
      DataByteArray bytes = (DataByteArray) input.get(0);
      M value_ = protoConverter_.fromBytes(bytes.get());
      return new ProtobufTuple(value_);
    } catch (IOException e) {
      return null;
    }
  }

  @Override
  public Schema outputSchema(Schema input) {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}=======
package com.twitter.elephantbird.pig.piggybank;

import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;

import com.google.protobuf.Message;
import com.twitter.elephantbird.mapreduce.io.ProtobufConverter;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * The base class for a Pig UDF that takes as input a tuple containing a single element, the
 * bytes of a serialized protocol buffer as a DataByteArray.  It outputs the protobuf in
 * expanded form.  The specific protocol buffer is a template parameter, generally specified by a
 * codegen'd derived class. See com.twitter.elephantbird.proto.HadoopProtoCodeGenerator.
 * Alternatly, full class name could be passed to the constructor in Pig:
 * <pre>
 *   DEFINE PersonProtobufBytesToTuple com.twitter.elephantbird.pig.piggybank.ProtobufBytesToTuple('com.twitter.elephantbird.proto.Person');
 *   persons = FOREACH protobufs GENERATE PersonProtobufBytesToTuple($0);
 * </pre>
 */
public class ProtobufBytesToTuple<M extends Message> extends EvalFunc<Tuple> {
  private TypeRef<M> typeRef_ = null;
  private ProtobufConverter<M> protoConverter_ = null;
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  public ProtobufBytesToTuple() {}

  public ProtobufBytesToTuple(String protoClassName) {
    TypeRef<M> typeRef = Protobufs.getTypeRef(protoClassName);
    setTypeRef(typeRef);
  }

  /**
   * Set the type parameter so it doesn't get erased by Java. Must be called during
   * initialization.
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    protoConverter_ = ProtobufConverter.newInstance(typeRef);
  }

  @Override
  public Tuple exec(Tuple input) throws IOException {
    if (input == null || input.size() < 1) return null;
    try {
      DataByteArray bytes = (DataByteArray) input.get(0);
      M value_ = protoConverter_.fromBytes(bytes.get());
      return new ProtobufTuple(value_);
    } catch (IOException e) {
      return null;
    }
  }

  @Override
  public Schema outputSchema(Schema input) {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_110e154_738e6ce/rev_110e154-738e6ce/src/test/com/twitter/elephantbird/mapreduce/io/TestProtobufWritable.java;<<<<<<< MINE

import org.junit.Test;
||||||| BASE
import org.junit.Test;
=======
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_110e154_738e6ce/rev_110e154-738e6ce/src/test/com/twitter/elephantbird/pig/piggybank/TestPigToProto.java;<<<<<<< MINE
import com.twitter.elephantbird.pig8.util.PigToProtobuf;
||||||| BASE
import com.twitter.elephantbird.pig.util.PigToProtobuf;
=======
import com.twitter.elephantbird.examples.proto.ThriftFixtures.OneOfEach;
import com.twitter.elephantbird.pig.util.PigToProtobuf;
import com.twitter.elephantbird.pig.util.ThriftToPig;
import com.twitter.elephantbird.util.ThriftToProto;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_110e154_738e6ce/rev_110e154-738e6ce/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;<<<<<<< MINE
        "1-0-35-27000-16777216-6000000000-3.141592653589793-JSON THIS! \"-"+ooe.zomg_unicode+"-0-base64-{(1),(2),(3)}-{(1),(2),(3)}-{(1),(2),(3)}",
        toTuple(ooe).toDelimitedString("-"));
||||||| BASE
        "1-0-35-27000-16777216-6000000000-3.141592653589793-JSON THIS! \"-"+ooe.zomg_unicode+"-0-base64-{(1),(2),(3)}-{(1),(2),(3)}-{(1L),(2L),(3L)}",
        toTuple(ooe).toDelimitedString("-"));
=======
        "1-0-35-27000-16777216-6000000000-3.141592653589793-JSON THIS! \"-"+ooe.zomg_unicode+"-0-base64-{(1),(2),(3)}-{(1),(2),(3)}-{(1L),(2L),(3L)}",
        toTuple(type, ooe).toDelimitedString("-"));
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_110e154_738e6ce/rev_110e154-738e6ce/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;<<<<<<< MINE
    assertEquals("(31337,I am a bonk... xor!)-(1,0,35,27000,16777216,6000000000,3.141592653589793,JSON THIS! \","+n.my_ooe.zomg_unicode+",0,base64,{(1),(2),(3)},{(1),(2),(3)},{(1),(2),(3)})",
        toTuple(n).toDelimitedString("-"));
||||||| BASE
    assertEquals("(31337,I am a bonk... xor!)-(1,0,35,27000,16777216,6000000000L,3.141592653589793,JSON THIS! \","+n.my_ooe.zomg_unicode+",0,base64,{(1),(2),(3)},{(1),(2),(3)},{(1L),(2L),(3L)})",
        toTuple(n).toDelimitedString("-"));
=======
    assertEquals("(31337,I am a bonk... xor!)-(1,0,35,27000,16777216,6000000000L,3.141592653589793,JSON THIS! \","+n.my_ooe.zomg_unicode+",0,base64,{(1),(2),(3)},{(1),(2),(3)},{(1L),(2L),(3L)})",
        toTuple(type, n).toDelimitedString("-"));
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_110e154_738e6ce/rev_110e154-738e6ce/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;<<<<<<< MINE
    assertEquals("{(1,0,34,27000,16777216,6000000000,3.141592653589793,JSON THIS! \"," + ooe.zomg_unicode +
        ",0,base64,{(1),(2),(3)},{(1),(2),(3)},{(1),(2),(3)}),(1,0,35,27000,16777216,6000000000,3.141592653589793,JSON THIS! \"," +
        ooe.zomg_unicode + ",0,base64,{(1),(2),(3)},{(1),(2),(3)},{(1),(2),(3)})}-{({}),({(then a one, two),(three!),(FOUR!!)}),({(and a one),(and a two)})}-{zero={}, three={}, two={(1,Wait.),(2,What?)}}",
        (toTuple(hm).toDelimitedString("-")));
||||||| BASE
    assertEquals("{(1,0,34,27000,16777216,6000000000L,3.141592653589793,JSON THIS! \"," + ooe.zomg_unicode +
        ",0,base64,{(1),(2),(3)},{(1),(2),(3)},{(1L),(2L),(3L)}),(1,0,35,27000,16777216,6000000000L,3.141592653589793,JSON THIS! \"," +
        ooe.zomg_unicode + ",0,base64,{(1),(2),(3)},{(1),(2),(3)},{(1L),(2L),(3L)})}-{({}),({(then a one, two),(three!),(FOUR!!)}),({(and a one),(and a two)})}-{zero={}, three={}, two={(1,Wait.),(2,What?)}}",
        (toTuple(hm).toDelimitedString("-")));
=======
    assertEquals("{(1,0,34,27000,16777216,6000000000L,3.141592653589793,JSON THIS! \"," + ooe.zomg_unicode +
        ",0,base64,{(1),(2),(3)},{(1),(2),(3)},{(1L),(2L),(3L)}),(1,0,35,27000,16777216,6000000000L,3.141592653589793,JSON THIS! \"," +
        ooe.zomg_unicode + ",0,base64,{(1),(2),(3)},{(1),(2),(3)},{(1L),(2L),(3L)})}-{({}),({(and a one),(and a two)}),({(then a one, two),(three!),(FOUR!!)})}-{zero={}, three={}, two={(1,Wait.),(2,What?)}}",
        (toTuple(type, hm).toDelimitedString("-")));
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_110e154_738e6ce/rev_110e154-738e6ce/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;<<<<<<< MINE
        "1-0-35-27000-16777216-6000000000-3.141592653589793-JSON THIS! \"-"+ooe.zomg_unicode+"-0--{(1),(2),(3)}--{(1),(2),(3)}",
        toTuple(mostly_ooe).toDelimitedString("-"));
||||||| BASE
        "1-0-35-27000-16777216-6000000000-3.141592653589793-JSON THIS! \"-"+ooe.zomg_unicode+"-0--{(1),(2),(3)}--{(1L),(2L),(3L)}",
        toTuple(mostly_ooe).toDelimitedString("-"));
=======
        "1-0-35-27000-16777216-6000000000-3.141592653589793-JSON THIS! \"--0--{(1),(2),(3)}-{(1),(2),(3)}-{(1L),(2L),(3L)}",
        toTuple(type, mostly_ooe).toDelimitedString("-"));
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_110e154_738e6ce/rev_110e154-738e6ce/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;<<<<<<< MINE
    assertEquals("(31337,)-(1,0,35,27000,16777216,6000000000,3.141592653589793,JSON THIS! \","+n.my_ooe.zomg_unicode+",0,,{(1),(2),(3)},,{(1),(2),(3)})",
        toTuple(n2).toDelimitedString("-"));
||||||| BASE
    assertEquals("(31337,)-(1,0,35,27000,16777216,6000000000L,3.141592653589793,JSON THIS! \","+n.my_ooe.zomg_unicode+",0,,{(1),(2),(3)},,{(1L),(2L),(3L)})",
        toTuple(n2).toDelimitedString("-"));
=======
    assertEquals("(31337,)-(1,0,35,27000,16777216,6000000000L,3.141592653589793,JSON THIS! \",,0,,{(1),(2),(3)},{(1),(2),(3)},{(1L),(2L),(3L)})",
        toTuple(type, n2).toDelimitedString("-"));
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_4ece7ba_811018f/rev_4ece7ba-811018f/src/java/com/twitter/elephantbird/pig/store/LzoProtobufBlockPigStorage.java;<<<<<<< MINE
  private Message msgObj; // for newBuilder().
||||||| BASE
  Builder builder_;
=======
  private Builder builder_;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_4ece7ba_811018f/rev_4ece7ba-811018f/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_cc1e4c7_f3518a2/rev_cc1e4c7-f3518a2/src/java/com/twitter/elephantbird/pig/store/LzoProtobufB64LinePigStorage.java;<<<<<<< MINE
import com.twitter.elephantbird.pig.util.PigUtil;
||||||| BASE
=======
import com.twitter.elephantbird.util.Codecs;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/mapreduce/output/LzoProtobufB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/mapreduce/output/LzoBinaryB64LineRecordWriter.java;<<<<<<< MINE
||||||| BASE
import com.twitter.elephantbird.mapreduce.io.BinaryConverter;
import com.twitter.elephantbird.mapreduce.io.BinaryWritable;

=======
import com.twitter.elephantbird.mapreduce.io.BinaryConverter;
import com.twitter.elephantbird.mapreduce.io.BinaryWritable;
import com.twitter.elephantbird.util.Codecs;
import com.twitter.elephantbird.util.Protobufs;

>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/mapreduce/output/LzoBinaryB64LineRecordWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/mapreduce/io/BinaryBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/mapreduce/io/BinaryBlockWriter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/pig/store/LzoProtobufB64LinePigStorage.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.store;

import java.io.IOException;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.data.Tuple;

import com.google.protobuf.Message;
import com.google.protobuf.Message.Builder;
import com.twitter.elephantbird.pig.util.PigToProtobuf;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * Serializes Pig Tuples into Base-64 encoded, line-delimited protocol buffers.
 * The fields in the pig tuple must correspond exactly to the fields in the protobuf, as
 * no name-matching is performed (names of the tuple fields are not currently accessible to
 * a StoreFunc. It will be in 0.7, so something more flexible will be possible)
 *
 * @param <M> Protocol Buffer Message class being serialized
 */
public class LzoProtobufB64LinePigStorage<M extends Message> extends LzoBaseStoreFunc {

  private TypeRef<M> typeRef_;
  private Base64 base64_ = new Base64();
  Builder builder_;

  protected LzoProtobufB64LinePigStorage(){}

  public LzoProtobufB64LinePigStorage(String protoClassName) {
    TypeRef<M> typeRef = Protobufs.getTypeRef(protoClassName);
    setTypeRef(typeRef);
  }

  protected void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    builder_ =  Protobufs.getMessageBuilder(typeRef_.getRawClass());
  }

  public void putNext(Tuple f) throws IOException {
    if (f == null) return;
    os_.write(base64_.encode(PigToProtobuf.tupleToMessage(builder_, f).toByteArray()));
    os_.write("\n".getBytes("UTF-8"));
  }

}=======
package com.twitter.elephantbird.pig.store;

import java.io.IOException;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.data.Tuple;
import org.omg.IOP.Codec;

import com.google.protobuf.Message;
import com.twitter.elephantbird.pig.util.PigToProtobuf;
import com.twitter.elephantbird.pig.util.PigUtil;
import com.twitter.elephantbird.util.Codecs;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * Serializes Pig Tuples into Base-64 encoded, line-delimited protocol buffers.
 * The fields in the pig tuple must correspond exactly to the fields in the protobuf, as
 * no name-matching is performed (names of the tuple fields are not currently accessible to
 * a StoreFunc. It will be in 0.7, so something more flexible will be possible)
 *
 * @param <M> Protocol Buffer Message class being serialized
 */
public class LzoProtobufB64LinePigStorage<M extends Message> extends LzoBaseStoreFunc {

  private TypeRef<M> typeRef_;
  private Base64 base64_ = Codecs.createStandardBase64();
  private Message msgObj; // for newBuilder()

  protected LzoProtobufB64LinePigStorage(){}

  public LzoProtobufB64LinePigStorage(String protoClassName) {
    TypeRef<M> typeRef = PigUtil.getProtobufTypeRef(protoClassName);
    setTypeRef(typeRef);
  }

  protected void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    msgObj =  Protobufs.getMessageBuilder(typeRef_.getRawClass()).build();
  }

  public void putNext(Tuple f) throws IOException {
    if (f == null) return;
    Message message = PigToProtobuf.tupleToMessage(msgObj.newBuilderForType(), f);
    os_.write(base64_.encode(message.toByteArray()));
    os_.write(Protobufs.NEWLINE_UTF8_BYTE);
  }

}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/pig/store/LzoProtobufBlockPigStorage.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.store;

import java.io.IOException;

import org.apache.pig.data.Tuple;

import com.google.protobuf.Message;
import com.google.protobuf.Message.Builder;
import com.twitter.elephantbird.pig.util.PigToProtobuf;
import com.twitter.elephantbird.mapreduce.io.ProtobufBlockWriter;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;
import java.io.OutputStream;


/**
 * Serializes Pig Tuples into Block encodedprotocol buffers.
 * The fields in the pig tuple must correspond exactly to the fields in the protobuf, as
 * no name-matching is performed (names of the tuple fields are not currently accessible to
 * a StoreFunc. It will be in 0.7, so something more flexible will be possible)
 *
 * @param <M> Protocol Buffer Message class being serialized
 */
public class LzoProtobufBlockPigStorage<M extends Message> extends LzoBaseStoreFunc {

  private TypeRef<M> typeRef_;
  Builder builder_;
  protected ProtobufBlockWriter<M> writer_ = null;
  private int numRecordsPerBlock_ = 10000;

  protected LzoProtobufBlockPigStorage() {
  }

  public LzoProtobufBlockPigStorage(String protoClassName) {
    TypeRef<M> typeRef = Protobufs.getTypeRef(protoClassName);
    setTypeRef(typeRef);
  }

  @Override
  public void bindTo(OutputStream os) throws IOException {
		super.bindTo(os);
		writer_ = new ProtobufBlockWriter<M>(os_, typeRef_.getRawClass(), numRecordsPerBlock_);
  }

  protected void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    builder_ = Protobufs.getMessageBuilder(typeRef_.getRawClass());
  }

  @SuppressWarnings("unchecked")
  public void putNext(Tuple f) throws IOException {
    if (f == null) return;
	  writer_.write((M)PigToProtobuf.tupleToMessage(builder_, f));
  }

	@Override
	public void finish() throws IOException {
    if (writer_ != null) {
      writer_.close();
    }
  }
}=======
package com.twitter.elephantbird.pig.store;

import java.io.IOException;

import org.apache.pig.data.Tuple;

import com.google.protobuf.Message;
import com.twitter.elephantbird.pig.util.PigToProtobuf;
import com.twitter.elephantbird.pig.util.PigUtil;
import com.twitter.elephantbird.mapreduce.io.ProtobufBlockWriter;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;
import java.io.OutputStream;


/**
 * Serializes Pig Tuples into Block encodedprotocol buffers.
 * The fields in the pig tuple must correspond exactly to the fields in the protobuf, as
 * no name-matching is performed (names of the tuple fields are not currently accessible to
 * a StoreFunc. It will be in 0.7, so something more flexible will be possible)
 *
 * @param <M> Protocol Buffer Message class being serialized
 */
public class LzoProtobufBlockPigStorage<M extends Message> extends LzoBaseStoreFunc {

  private TypeRef<M> typeRef_;
  private Message msgObj; // for newBuilder().
  protected ProtobufBlockWriter<M> writer_ = null;
  private int numRecordsPerBlock_ = 10000;

  protected LzoProtobufBlockPigStorage() {
  }

  public LzoProtobufBlockPigStorage(String protoClassName) {
    TypeRef<M> typeRef = PigUtil.getProtobufTypeRef(protoClassName);
    setTypeRef(typeRef);
  }

  @Override
  public void bindTo(OutputStream os) throws IOException {
		super.bindTo(os);
		writer_ = new ProtobufBlockWriter<M>(os_, typeRef_.getRawClass(), numRecordsPerBlock_);
  }

  protected void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    msgObj = Protobufs.getMessageBuilder(typeRef_.getRawClass()).build();
  }

  @SuppressWarnings("unchecked")
  public void putNext(Tuple f) throws IOException {
    if (f == null) return;
	  writer_.write((M)PigToProtobuf.tupleToMessage(msgObj.newBuilderForType(), f));
  }

	@Override
	public void finish() throws IOException {
    if (writer_ != null) {
      writer_.close();
    }
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.util;

import java.lang.reflect.Field;
import java.nio.ByteBuffer;
import java.util.ArrayDeque;
import java.util.Deque;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;

import org.apache.pig.LoadFunc;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.apache.thrift.TBase;
import org.apache.thrift.TEnum;
import org.apache.thrift.TException;
import org.apache.thrift.TFieldIdEnum;
import org.apache.thrift.meta_data.EnumMetaData;
import org.apache.thrift.meta_data.FieldMetaData;
import org.apache.thrift.meta_data.FieldValueMetaData;
import org.apache.thrift.meta_data.ListMetaData;
import org.apache.thrift.meta_data.MapMetaData;
import org.apache.thrift.meta_data.SetMetaData;
import org.apache.thrift.meta_data.StructMetaData;
import org.apache.thrift.protocol.TField;
import org.apache.thrift.protocol.TList;
import org.apache.thrift.protocol.TMap;
import org.apache.thrift.protocol.TMessage;
import org.apache.thrift.protocol.TProtocol;
import org.apache.thrift.protocol.TSet;
import org.apache.thrift.protocol.TStruct;
import org.apache.thrift.protocol.TType;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Lists;
import com.google.common.collect.Maps;
import com.twitter.elephantbird.pig.load.LzoThriftB64LinePigLoader;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;

/**
 * <li> converts a Thrift struct to a Pig tuple
 * <li> utilities to provide schema for Pig loaders and Pig scripts
 */
public class ThriftToPig<M extends TBase<?, ?>> {
  private static final Logger LOG = LoggerFactory.getLogger(ThriftToPig.class);

  /* TODO :
   * 1. Add lazy deserialization like ProtobufTuple does. Not sure if it can be done
   *    efficiently for Thrift.
   * 2. Converting Enum to names (strings) is supported only in the common
   *    case where Enum is part of a struct. Enum used directly in containers
   *    (e.g. list<SomeEnum>) are still integers. The issue is that Thrift
   *    does not explicitly tell that it is writing an Enum. We need to
   *    deduce that from the context. In the case of Structs, we already
   *    maintain this contexts.
   *
   *    In order to support enums-to-strings correctly we need to maintain more
   *    state and we should always know exact context/recursion of Thrift
   *    struct's write() method.
   *
   *    This is certainly do-able. Once we keep track of serialization
   *    so closely, we not far from implementing our own generic write() method.
   *    implementing generic write method will let us deserialize thrift buffer
   *    directly to a Pig Tuple and there is no need to use a Thrift object
   *    as intermediate step. This will also let us support
   *    lazy-deserialization and projections efficiently since we direclty
   *    access the thrift buffer.
   */
  private static BagFactory bagFactory_ = BagFactory.getInstance();
  private static TupleFactory tupleFactory_  = TupleFactory.getInstance();

  /** for some reason there is no TType.BINARY. */
  private static final byte TTYPE_BINARY = 83;

  private Class<? extends TBase<?, ?>> tClass_;
  private ThriftProtocol tProtocol_ = new ThriftProtocol();
  private Deque<PigContainer> containerStack_ = new ArrayDeque<PigContainer>();
  private PigContainer curContainer_;
  private Tuple curTuple_;

  // We want something that provides a generic interface for populating
  // Pig Tuples, Bags, and Maps. This does the trick.

  private abstract class PigContainer {
    StructDescriptor structDesc; // The current thrift struct being written
    FieldDescriptor curFieldDesc;
    public abstract Object getContents();
    public abstract void add(Object o) throws TException;

    /** set curFieldDesc if the container is is Thrift Struct. */
    public void setCurField(TField tField) throws TException {
      if (structDesc != null) {
        curFieldDesc = structDesc.fieldMap.get(tField.id);
        if (curFieldDesc == null) {
          throw new TException("Unexpected TField " + tField + " for " + tClass_.getName());
        }
      }
    }
  }

  private class TupleWrap extends PigContainer {

    private final Tuple t;

    public TupleWrap(int size) {
      t = tupleFactory_.newTuple(size);
    }

    public Object getContents() { return t; }

    public void add(Object o) throws TException {
      if (curFieldDesc == null) {
        throw new TException("Internal Error. curFieldDesc is not set");
      }
      if (curFieldDesc.enumMap != null && // map enum to string
          (o = curFieldDesc.enumMap.get(o)) == null) {
        throw new TException("cound not find Enum string");
      }
      try {
        t.set(curFieldDesc.tupleIdx, o);
       } catch (ExecException e) {
          throw new TException(e);
       }
    }
  }

  private class BagWrap extends PigContainer {
    List<Tuple> tuples;

    public BagWrap(int size) {
      tuples =  Lists.newArrayListWithCapacity(size);
    }

    @Override
    public void add(Object o) throws TException {
      // Pig bags contain tuples of objects, so we must wrap a tuple around
      // everything we get.
      if (o instanceof Tuple) {
        tuples.add((Tuple) o);
      } else {
        tuples.add(tupleFactory_.newTuple(o));
      }
    }

    @Override
    public Object getContents() {
      return bagFactory_.newDefaultBag(tuples);
    }
  }

  private class MapWrap extends PigContainer {
    private final Map<String, Object> map;
    String currKey = null;

    public MapWrap(int size) {
      map = new HashMap<String, Object>(size);
    }

    @Override
    public void add(Object o) throws TException {
      //we alternate between String keys and (converted) DataByteArray values.
      if (currKey == null) {
        try {
          currKey = (String) o;
        } catch (ClassCastException e) {
          throw new TException("Only String keys are allowed in maps.");
        }
      } else {
        map.put(currKey, o);
        currKey = null;
      }
    }

    @Override
    public Object getContents() {
      return map;
    }
  }


  private void pushContainer(PigContainer c) {
    containerStack_.addLast(c);
    curContainer_ = c;
  }

  private PigContainer popContainer() throws TException {
    PigContainer c = containerStack_.removeLast();
    curContainer_ = containerStack_.peekLast();
    if (curContainer_ == null) { // All done!
      curTuple_ = (Tuple) c.getContents();
    } else {
      curContainer_.add(c.getContents());
    }
    return c;
  }

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(Class<M> tClass) {
    return new ThriftToPig<M>(tClass);
  }

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(TypeRef<M> typeRef) {
    return new ThriftToPig<M>(typeRef.getRawClass());
  }

  public ThriftToPig(Class<M> tClass) {
    this.tClass_ = tClass;
    structMap = Maps.newHashMap();
    updateStructMap(tClass_);
    structMap = ImmutableMap.copyOf(structMap);
    reset();
  }

  /**
   * The protocol should be reset before each object that is serialized.
   * This is important since, the protocol itself can not reliably
   * realize if it at the beginning of a new object. It can not always
   * rely on the last object being correct written because of
   * any exceptions while processing previous object.
   */
  public void reset() {
    containerStack_.clear();
    curContainer_ = null;
    curTuple_ = null;
  }

  /**
   * Converts a thrift object to Pig tuple.
   * Throws TException in case of any errors.
   */
  public Tuple getPigTuple(M thriftObj) throws TException {
    reset();
    thriftObj.write(tProtocol_);
    if (curTuple_ != null) {
      return curTuple_;
    }
    // unexpected
    throw new TException("Internal error. tuple is not set");
  }

  /**
   * returns 'enum int -> enum name' mapping
   */
  static private Map<Integer, String> extractEnumMap(FieldValueMetaData field) {
    MetaData f = new MetaData(field);
    if (!f.isEnum()) {
      return null;
    }
    Map<Integer, String> map = Maps.newHashMap();
    for(TEnum e : f.getEnumClass().getEnumConstants()) {
      map.put(e.getValue(), e.toString());
    }
    return ImmutableMap.copyOf(map);
  }

  /**
   * holds relevant info for a field in a Thrift Struct including
   * index into tuple array.
   */
  private static class FieldDescriptor {
    TFieldIdEnum fieldEnum;
    int tupleIdx;
    Map<Integer, String> enumMap = null; // set for enums
  }

  /**
   * describes a Thrift struct. Contains following info :
   * <li> Thrift field descriptor map
   * <li> ...
   */
  private static class StructDescriptor {
    Map<Short, FieldDescriptor> fieldMap;

    public StructDescriptor(Class<? extends TBase<?, ?>> tClass) {
      fieldMap = Maps.newHashMap();
      int idx = 0;
      for (Entry<? extends TFieldIdEnum, FieldMetaData> e : FieldMetaData.getStructMetaDataMap(tClass).entrySet()) {
        FieldDescriptor desc = new FieldDescriptor();
        desc.fieldEnum = e.getKey();
        desc.tupleIdx = idx++;
        if (e.getValue().valueMetaData.type == TType.ENUM) {
          desc.enumMap = extractEnumMap(e.getValue().valueMetaData);
        }
        fieldMap.put(desc.fieldEnum.getThriftFieldId(), desc);
      }
      fieldMap = ImmutableMap.copyOf(fieldMap);
    }
  }

  private Map<TStruct, StructDescriptor> structMap;

  private void updateStructMap(Class<? extends TBase<?, ?>> tClass) {
    final TStruct tStruct = getStructDesc(tClass);

    if (structMap.get(tStruct) != null) {
      return;
    }

    StructDescriptor desc = new StructDescriptor(tClass);
    LOG.debug("adding struct descriptor for " + tClass.getName()
        + " with " + desc.fieldMap.size() + " fields");
    structMap.put(tStruct, desc);
    // recursively add any referenced classes.
    for (FieldMetaData field : FieldMetaData.getStructMetaDataMap(tClass).values()) {
      updateStructMap(field.valueMetaData);
    }
  }

  /**
   * Look for any class embedded in the in the container or struct fields
   * and update the struct map with them.
   */
  private void updateStructMap(FieldValueMetaData field) {
    MetaData f = new MetaData(field);

    if (f.isStruct()) {
      updateStructMap(f.getStructClass());
    }

    if (f.isList()) {
      updateStructMap(f.getListElem());
    }

    if (f.isMap()) {
      if (f.getMapKey().type != TType.STRING) {
        throw new IllegalArgumentException("Pig does not support maps with non-string keys "
            + "while initializing ThriftToPig for " + tClass_.getName());
      }
      updateStructMap(f.getMapKey());
      updateStructMap(f.getMapValue());
    }

    if (f.isSet()) {
      updateStructMap(f.getSetElem());
    }
  }

  private class ThriftProtocol extends TProtocol {

    ThriftProtocol() {
      super(null); // this protocol is not used for transport.
    }

    @Override
    public void writeBinary(ByteBuffer bin) throws TException {
      byte[] buf = new byte[bin.remaining()];
      bin.mark();
      bin.get(buf);
      bin.reset();
      curContainer_.add(new DataByteArray(buf));
      /* We could use DataByteArray(byte[], start, end) and avoid a
       * copy here.  But the constructor will make a (quite inefficient) copy.
       */
    }

    @Override
    public void writeBool(boolean b) throws TException {
      curContainer_.add(Integer.valueOf(b ? 1 : 0));
    }

    @Override
    public void writeByte(byte b) throws TException {
      curContainer_.add(Integer.valueOf(b));
    }

    @Override
    public void writeDouble(double dub) throws TException {
      curContainer_.add(Double.valueOf(dub));
    }

    @Override
    public void writeFieldBegin(TField field) throws TException {
      curContainer_.setCurField(field);
    }

    @Override
    public void writeFieldEnd() throws TException {
    }

    @Override
    public void writeFieldStop() throws TException {
    }

    @Override
    public void writeI16(short i16) throws TException {
      curContainer_.add(Integer.valueOf(i16));
    }

    @Override
    public void writeI32(int i32) throws TException {
      curContainer_.add(i32);
    }

    @Override
    public void writeI64(long i64) throws TException {
      curContainer_.add(i64);
    }

    @Override
    public void writeListBegin(TList list) throws TException {
      pushContainer(new BagWrap(list.size));
    }

    @Override
    public void writeListEnd() throws TException {
      popContainer();
    }

    @Override
    public void writeMapBegin(TMap map) throws TException {
      pushContainer(new MapWrap(map.size));
    }

    @Override
    public void writeMapEnd() throws TException {
      popContainer();
    }

    @Override
    public void writeSetBegin(TSet set) throws TException {
      pushContainer(new BagWrap(set.size));
    }

    @Override
    public void writeSetEnd() throws TException {
      popContainer();
    }

    @Override
    public void writeString(String str) throws TException {
      curContainer_.add(str);
    }

    @Override
    public void writeStructBegin(TStruct struct) throws TException {
      StructDescriptor desc = structMap.get(struct);
      if (desc == null) {
        throw new TException("Unexpected TStruct " + struct.name + " for " + tClass_.getName());
      }
      PigContainer c = new TupleWrap(desc.fieldMap.size());
      c.structDesc = desc;
      pushContainer(c);
    }

    @Override
    public void writeStructEnd() throws TException {
      popContainer();
    }

    @Override
    public void writeMessageBegin(TMessage message) throws TException {
      throw new TException("method not implemented.");
    }
    @Override
    public void writeMessageEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public ByteBuffer readBinary() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public boolean readBool() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public byte readByte() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public double readDouble() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TField readFieldBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readFieldEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public short readI16() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public int readI32() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public long readI64() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TList readListBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readListEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TMap readMapBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readMapEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TMessage readMessageBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readMessageEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TSet readSetBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readSetEnd() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public String readString() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public TStruct readStructBegin() throws TException {
      throw new TException("method not implemented.");
    }

    @Override
    public void readStructEnd() throws TException {
      throw new TException("method not implemented.");
    }
  }

  /**
   * A utility class to help with type checking of a ThriftField.
   * Avoids checking type and not-so-readable casting in many places.
   */
  static class MetaData {
    final FieldValueMetaData field;

    MetaData(FieldValueMetaData field) {
      this.field = field;
    }

    FieldValueMetaData getField() {
      return field;
    }

    // List
    boolean isList() {
      return field instanceof ListMetaData;
    }

    FieldValueMetaData getListElem() {
      return ((ListMetaData)field).elemMetaData;
    }

    // Enum
    boolean isEnum() {
      return field instanceof EnumMetaData;
    }

    Class<? extends TEnum> getEnumClass() {
      return ((EnumMetaData)field).enumClass;
    }

    // Map
    boolean isMap() {
      return field instanceof MapMetaData;
    }

    FieldValueMetaData getMapKey() {
      return ((MapMetaData)field).keyMetaData;
    }

    FieldValueMetaData getMapValue() {
      return ((MapMetaData)field).valueMetaData;
    }

    // Set
    boolean isSet() {
      return field instanceof SetMetaData;
    }

    FieldValueMetaData getSetElem() {
      return ((SetMetaData)field).elemMetaData;
    }

    // Struct
    boolean isStruct() {
      return field instanceof StructMetaData;
    }

    @SuppressWarnings("unchecked")
    Class<? extends TBase<?, ?>> getStructClass() {
      return (Class <? extends TBase<?, ?>>)((StructMetaData)field).structClass;
    }
  }

  /**
   * Returns Pig schema for the Thrift struct.
   */
  public static Schema toSchema(Class<? extends TBase<?, ?>> tClass) {
    Schema schema = new Schema();

    try {
      for (Entry<? extends TFieldIdEnum, FieldMetaData> e : FieldMetaData.getStructMetaDataMap(tClass).entrySet()) {
        FieldMetaData meta = e.getValue();
        FieldValueMetaData field = e.getValue().valueMetaData;
        MetaData fm = new MetaData(field);
        if (fm.isStruct()) {
          schema.add(new FieldSchema(meta.fieldName, toSchema(fm.getStructClass()), DataType.TUPLE));
        } else if (fm.isEnum()) { // enums in Structs are strings (enums in containers are not, yet)
          schema.add(new FieldSchema(meta.fieldName, null, DataType.CHARARRAY));
        } else {
          if (field.type == TType.STRING) {
            // A hack to get around the fact that Thrift uses TType.STRING
            // for both binary and string.
            Class<?> fieldType = ThriftUtils.getFiedlType(tClass, meta.fieldName);
            if (fieldType == ByteBuffer.class) {
              field = new FieldValueMetaData(TTYPE_BINARY);
            }
            // This a partition work around. still need to fix the case
            // when 'binary' is used in containers.
            // This is fixed in Thrift 0.6 (field.isBinary()).
          }
          schema.add(singleFieldToFieldSchema(meta.fieldName, field));
        }
      }
    } catch (FrontendException t) {
      throw new RuntimeException(t);
    }

    return schema;
  }

  private static FieldSchema singleFieldToFieldSchema(String fieldName, FieldValueMetaData field) throws FrontendException {

    MetaData fm = new MetaData(field);

    switch (field.type) {
      case TType.LIST:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", fm.getListElem()), DataType.BAG);
      case TType.SET:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", fm.getSetElem()), DataType.BAG);
      case TType.MAP:
        // can not specify types for maps in Pig.
        return new FieldSchema(fieldName, null, DataType.MAP);
      default:
        return new FieldSchema(fieldName, null, getPigDataType(field));
    }
  }

  /**
   * Returns a schema with single tuple (for Pig bags).
   */
  private static Schema singleFieldToTupleSchema(String fieldName, FieldValueMetaData field) throws FrontendException {
    MetaData fm = new MetaData(field);
    FieldSchema fieldSchema = null;

    switch (field.type) {
      case TType.STRUCT:
        fieldSchema = new FieldSchema(fieldName, toSchema(fm.getStructClass()), DataType.TUPLE);
        break;
      case TType.LIST:
        fieldSchema = singleFieldToFieldSchema(fieldName, fm.getListElem());
        break;
      case TType.SET:
        fieldSchema = singleFieldToFieldSchema(fieldName, fm.getSetElem());
        break;
      default:
        fieldSchema = new FieldSchema(fieldName, null, getPigDataType(fm.getField()));
    }

    Schema schema = new Schema();
    schema.add(fieldSchema);
    return schema;
  }

  private static byte getPigDataType(FieldValueMetaData field) {
    switch (field.type) {
      case TType.BOOL:
      case TType.BYTE:
      case TType.I16:
      case TType.I32:
      case TType.ENUM: // will revisit this once Enums in containers are also strings.
        return DataType.INTEGER;
      case TType.I64:
        return DataType.LONG;
      case TType.DOUBLE:
        return DataType.DOUBLE;
      case TType.STRING:
        return DataType.CHARARRAY;
      case TTYPE_BINARY:
        return DataType.BYTEARRAY;
      default:
        throw new IllegalArgumentException("Unexpected type where a simple type is expected : " + field.type);
    }
  }

  /**
   * Turn a Thrift Struct into a loading schema for a pig script.
   */
  public static String toPigScript(Class<? extends TBase<?, ?>> thriftClass,
                                   Class<? extends LoadFunc> pigLoader) {
    StringBuilder sb = new StringBuilder();
    /* we are commenting out explicit schema specification. The schema is
     * included mainly to help the readers of the pig script. Pig learns the
     * schema directly from the loader.
     * If explicit schema is not commented, we might have surprising results
     * when a Thrift class (possibly in control of another team) changes,
     * but the Pig script is not updated. Commenting it out work around this.
     */
    StringBuilder prefix = new StringBuilder("       --  ");
    sb.append("raw_data = load '$INPUT_FILES' using ")
      .append(pigLoader.getName())
      .append("('")
      .append(thriftClass.getName())
      .append("');\n")
      .append(prefix)
      .append("as ");
    prefix.append("   ");

    try {
      stringifySchema(sb, toSchema(thriftClass), DataType.TUPLE, prefix);
    } catch (FrontendException e) {
      throw new RuntimeException(e);
    }

    sb.append("\n");
    return sb.toString();
  }

  /**
   * Print formatted schema. This is a modified version of
   * {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
   * with support for (indented) pretty printing.
   */
  // This is used for building up output string
  // type can only be BAG or TUPLE
  public static void stringifySchema(StringBuilder sb,
                                     Schema schema,
                                     byte type,
                                     StringBuilder prefix)
                                          throws FrontendException{
      // this is a modified version of {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
      if (type == DataType.TUPLE) {
          sb.append("(") ;
      }
      else if (type == DataType.BAG) {
          sb.append("{") ;
      }

      prefix.append("  ");
      sb.append("\n").append(prefix);

      if (schema == null) {
          sb.append("null") ;
      }
      else {
          boolean isFirst = true ;
          for (int i=0; i< schema.size() ;i++) {

              if (!isFirst) {
                  sb.append(",\n").append(prefix);
              }
              else {
                  isFirst = false ;
              }

              FieldSchema fs = schema.getField(i) ;

              if(fs == null) {
                  sb.append("null");
                  continue;
              }

              if (fs.alias != null) {
                  sb.append(fs.alias);
                  sb.append(": ");
              }

              if (DataType.isAtomic(fs.type)) {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
              else if ( (fs.type == DataType.TUPLE) ||
                        (fs.type == DataType.BAG) ) {
                  // safety net
                  if (schema != fs.schema) {
                      stringifySchema(sb, fs.schema, fs.type, prefix) ;
                  }
                  else {
                      throw new AssertionError("Schema refers to itself "
                                               + "as inner schema") ;
                  }
              } else if (fs.type == DataType.MAP) {
                  sb.append(DataType.findTypeName(fs.type) + "[ ]") ;
              } else {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
          }
      }

      prefix.setLength(prefix.length()-2);
      sb.append("\n").append(prefix);

      if (type == DataType.TUPLE) {
          sb.append(")") ;
      }
      else if (type == DataType.BAG) {
          sb.append("}") ;
      }
  }

  private TStruct getStructDesc(Class<? extends TBase<?, ?>> tClass) {
    // hack to get hold of STRUCT_DESC of a thrift class:
    // Access 'private static final' field STRUCT_DESC using reflection.
    // Bad practice, but not sure if there is a better way.
    try {
      Field f = tClass.getDeclaredField("STRUCT_DESC");
      f.setAccessible(true);
      return (TStruct) f.get(null);
    } catch (Throwable t) {
      throw new RuntimeException(t);
    }
  }

  public static void main(String[] args) throws Exception {
    if (args.length > 0) {
      Class<? extends TBase<?, ?>> tClass = ThriftUtils.getTypeRef(args[0]).getRawClass();
      System.out.println(args[0] + " : " + toSchema(tClass).toString());
      System.out.println(toPigScript(tClass, LzoThriftB64LinePigLoader.class));
    }
  }
}=======
package com.twitter.elephantbird.pig.util;

import java.nio.ByteBuffer;
import java.util.Arrays;
import java.util.Collection;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;

import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.pig.LoadFunc;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.apache.thrift.TBase;
import org.apache.thrift.protocol.TType;

import com.google.common.collect.Lists;
import com.twitter.elephantbird.pig.load.LzoThriftB64LinePigLoader;
import com.twitter.elephantbird.thrift.TStructDescriptor;
import com.twitter.elephantbird.thrift.TStructDescriptor.Field;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;

/**
 * <li> converts a Thrift struct to a Pig tuple
 * <li> utilities to provide schema for Pig loaders and Pig scripts
 */
public class ThriftToPig<M extends TBase<?, ?>> {
  public static final Logger LOG = LogManager.getLogger(ThriftToPig.class);

  private static BagFactory bagFactory = BagFactory.getInstance();
  private static TupleFactory tupleFactory  = TupleFactory.getInstance();

  private TStructDescriptor structDesc;

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(Class<M> tClass) {
    return new ThriftToPig<M>(tClass);
  }

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(TypeRef<M> typeRef) {
    return new ThriftToPig<M>(typeRef.getRawClass());
  }

  public ThriftToPig(Class<M> tClass) {
    structDesc = TStructDescriptor.getInstance(tClass);
  }

  /**
   * Converts a thrift object to Pig tuple.
   * All the fields are deserialized.
   * It might be better to use getLazyTuple() if not all fields
   * are required.
   */
  public Tuple getPigTuple(M thriftObj) {
    return toTuple(structDesc, thriftObj);
  }

  /**
   * Similar to {@link #getPigTuple(TBase)}. This delays
   * serialization of tuple contents until they are requested.
   * @param thriftObj
   * @return
   */
  public Tuple getLazyTuple(M thriftObj) {
    return new LazyTuple(thriftObj);
  }

  @SuppressWarnings("unchecked")
  private static <T extends TBase>Tuple toTuple(TStructDescriptor tDesc, T tObj) {
    int size = tDesc.getFields().size();
    Tuple tuple = tupleFactory.newTuple(size);
    for (int i=0; i<size; i++) {
      Field field = tDesc.getFieldAt(i);
      Object value = tDesc.getFieldValue(i, tObj);
      try {
        tuple.set(i, toPigObject(field, value));
      } catch (ExecException e) { // not expected
        throw new RuntimeException(e);
      }
    }
    return tuple;
  }

  @SuppressWarnings("unchecked")
  private static Object toPigObject(Field field, Object value) {
    if (value == null) {
      return null;
    }

    switch (field.getType()) {
    case TType.BOOL:
      return Integer.valueOf((Boolean)value ? 1 : 0);
    case TType.BYTE :
      return Integer.valueOf((Byte)value);
    case TType.I16 :
      return Integer.valueOf((Short)value);
    case TType.STRING:
      return stringTypeToPig(value);
    case TType.STRUCT:
      return toTuple(field.gettStructDescriptor(), (TBase<?, ?>)value);
    case TType.MAP:
      return toPigMap(field, (Map<Object, Object>)value);
    case TType.SET:
      return toPigBag(field.getSetElemField(), (Collection<Object>)value);
    case TType.LIST:
      return toPigBag(field.getListElemField(), (Collection<Object>)value);
    case TType.ENUM:
      return value.toString();
    default:
      // standard types : I32, I64, DOUBLE, etc.
      return value;
    }
  }

  /**
   * TType.STRING is a mess in Thrift. It could be byte[], ByteBuffer,
   * or even a String!.
   */
  private static Object stringTypeToPig(Object value) {
    if (value instanceof String) {
      return value;
    }
    if (value instanceof byte[]) {
      byte[] buf = (byte[])value;
      return new DataByteArray(Arrays.copyOf(buf, buf.length));
    }
    if (value instanceof ByteBuffer) {
      ByteBuffer bin = (ByteBuffer)value;
      byte[] buf = new byte[bin.remaining()];
      bin.mark();
      bin.get(buf);
      bin.reset();
      return new DataByteArray(buf);
    }
    return null;
  }

  private static Map<String, Object> toPigMap(Field field, Map<Object, Object> map) {
    // PIG map's key always a String. just use toString() and hope
    // things would work out ok.
    HashMap<String, Object> out = new HashMap<String, Object>(map.size());
    Field valueField = field.getMapValueField();
    for(Entry<Object, Object> e : map.entrySet()) {
      Object prev = out.put(e.getKey().toString(),
                            toPigObject(valueField, e.getValue()));
      if (prev != null) {
        String msg = "Duplicate keys while converting to String while "
          + " processing map " + field.getName() + " (key type : "
          + field.getMapKeyField().getType() + " value type : "
          + field.getMapValueField().getType() + ")";
        LOG.warn(msg);
        throw new RuntimeException(msg);
      }
    }
    return out;
  }

  private static DataBag toPigBag(Field field, Collection<Object> values) {
    List<Tuple> tuples = Lists.newArrayListWithExpectedSize(values.size());
    for(Object value : values) {
      Object pValue = toPigObject(field, value);
      if (pValue instanceof Tuple) { // DataBag should contain Tuples
        tuples.add((Tuple)pValue);
      } else {
        tuples.add(tupleFactory.newTuple(pValue));
      }
    }
    return bagFactory.newDefaultBag(tuples);
  }

  @SuppressWarnings("serial")
  /**
   * Delays serialization of Thrift fields until they are requested.
   */
  private class LazyTuple extends AbstractLazyTuple {
    /* NOTE : This is only a partial optimization. The other part
     * is to avoid deserialization of the Thrift fields from the
     * binary buffer.
     *
     * Currently TDeserializer allows deserializing just one field,
     * psuedo-skipping over the fields before it.
     * But if we are going deserialize 5 fields out of 20, we will be
     * skipping over same set of fields multiple times. OTOH this might
     * still be better than a full deserialization.
     *
     * We need to write our own version of TBinaryProtocol that truly skips.
     * Even TDeserializer 'skips'/ignores only after deserializing fields.
     * (e.g. Strings, Integers, buffers etc).
     */
    private M tObject;

    LazyTuple(M tObject) {
      initRealTuple(structDesc.getFields().size());
      this.tObject = tObject;
    }

    @Override
    protected Object getObjectAt(int index) {
      Field field = structDesc.getFieldAt(index);
      return toPigObject(field, structDesc.getFieldValue(index, tObject));
    }
  }

  /**
   * Returns Pig schema for the Thrift struct.
   */
  public static Schema toSchema(Class<? extends TBase<?, ?>> tClass) {
    return toSchema(TStructDescriptor.getInstance(tClass));
  }
  public static Schema toSchema(TStructDescriptor tDesc ) {
    Schema schema = new Schema();

    try {
      for(Field field : tDesc.getFields()) {
        String fieldName = field.getName();
        if (field.isStruct()) {
          schema.add(new FieldSchema(fieldName, toSchema(field.gettStructDescriptor()), DataType.TUPLE));
        } else {
          schema.add(singleFieldToFieldSchema(fieldName, field));
        }
      }
    } catch (FrontendException t) {
      throw new RuntimeException(t);
    }

    return schema;
  }

  private static FieldSchema singleFieldToFieldSchema(String fieldName, Field field) throws FrontendException {
    switch (field.getType()) {
      case TType.LIST:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", field.getListElemField()), DataType.BAG);
      case TType.SET:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", field.getSetElemField()), DataType.BAG);
      case TType.MAP:
        // can not specify types for maps in Pig.
        if (field.getMapKeyField().getType() != TType.STRING) {
          LOG.warn("Using a map with non-string key for field " + field.getName()
              + ". while converting to PIG Tuple, toString() is used for the key."
              + " It could result in incorrect maps.");
        }
        return new FieldSchema(fieldName, null, DataType.MAP);
      default:
        return new FieldSchema(fieldName, null, getPigDataType(field));
    }
  }

  /**
   * Returns a schema with single tuple (for Pig bags).
   */
  private static Schema singleFieldToTupleSchema(String fieldName, Field field) throws FrontendException {

    FieldSchema fieldSchema = null;

    switch (field.getType()) {
      case TType.STRUCT:
        fieldSchema = new FieldSchema(fieldName, toSchema(field.gettStructDescriptor()), DataType.TUPLE);
        break;
      case TType.LIST:
        fieldSchema = singleFieldToFieldSchema(fieldName, field.getListElemField());
        break;
      case TType.SET:
        fieldSchema = singleFieldToFieldSchema(fieldName, field.getSetElemField());
        break;
      default:
        fieldSchema = new FieldSchema(fieldName, null, getPigDataType(field));
    }

    Schema schema = new Schema();
    schema.add(fieldSchema);
    return schema;
  }

  private static byte getPigDataType(Field field) {
    switch (field.getType()) {
      case TType.BOOL:
      case TType.BYTE:
      case TType.I16:
      case TType.I32:
        return DataType.INTEGER;
      case TType.ENUM:
        return DataType.CHARARRAY;
      case TType.I64:
        return DataType.LONG;
      case TType.DOUBLE:
        return DataType.DOUBLE;
      case TType.STRING:
        return field.isBuffer() ? DataType.BYTEARRAY : DataType.CHARARRAY;
      default:
        throw new IllegalArgumentException("Unexpected type where a simple type is expected : " + field.getType());
    }
  }

  /**
   * Turn a Thrift Struct into a loading schema for a pig script.
   */
  public static String toPigScript(Class<? extends TBase<?, ?>> thriftClass,
                                   Class<? extends LoadFunc> pigLoader) {
    StringBuilder sb = new StringBuilder();
    /* we are commenting out explicit schema specification. The schema is
     * included mainly to help the readers of the pig script. Pig learns the
     * schema directly from the loader.
     * If explicit schema is not commented, we might have surprising results
     * when a Thrift class (possibly in control of another team) changes,
     * but the Pig script is not updated. Commenting it out avoids this.
     */
    StringBuilder prefix = new StringBuilder("       --  ");
    sb.append("raw_data = load '$INPUT_FILES' using ")
      .append(pigLoader.getName())
      .append("('")
      .append(thriftClass.getName())
      .append("');\n")
      .append(prefix)
      .append("as ");
    prefix.append("   ");

    try {
      stringifySchema(sb, toSchema(thriftClass), DataType.TUPLE, prefix);
    } catch (FrontendException e) {
      throw new RuntimeException(e);
    }

    sb.append("\n");
    return sb.toString();
  }

  /**
   * Print formatted schema. This is a modified version of
   * {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
   * with support for (indented) pretty printing.
   */
  // This is used for building up output string
  // type can only be BAG or TUPLE
  public static void stringifySchema(StringBuilder sb,
                                     Schema schema,
                                     byte type,
                                     StringBuilder prefix)
                                          throws FrontendException{
      // this is a modified version of {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
      if (type == DataType.TUPLE) {
          sb.append("(") ;
      }
      else if (type == DataType.BAG) {
          sb.append("{") ;
      }

      prefix.append("  ");
      sb.append("\n").append(prefix);

      if (schema == null) {
          sb.append("null") ;
      }
      else {
          boolean isFirst = true ;
          for (int i=0; i< schema.size() ;i++) {

              if (!isFirst) {
                  sb.append(",\n").append(prefix);
              }
              else {
                  isFirst = false ;
              }

              FieldSchema fs = schema.getField(i) ;

              if(fs == null) {
                  sb.append("null");
                  continue;
              }

              if (fs.alias != null) {
                  sb.append(fs.alias);
                  sb.append(": ");
              }

              if (DataType.isAtomic(fs.type)) {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
              else if ( (fs.type == DataType.TUPLE) ||
                        (fs.type == DataType.BAG) ) {
                  // safety net
                  if (schema != fs.schema) {
                      stringifySchema(sb, fs.schema, fs.type, prefix) ;
                  }
                  else {
                      throw new AssertionError("Schema refers to itself "
                                               + "as inner schema") ;
                  }
              } else if (fs.type == DataType.MAP) {
                  sb.append(DataType.findTypeName(fs.type) + "[ ]") ;
              } else {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
          }
      }

      prefix.setLength(prefix.length()-2);
      sb.append("\n").append(prefix);

      if (type == DataType.TUPLE) {
          sb.append(")") ;
      }
      else if (type == DataType.BAG) {
          sb.append("}") ;
      }
  }

  public static void main(String[] args) throws Exception {
    if (args.length > 0) {
      Class<? extends TBase<?, ?>> tClass = ThriftUtils.getTypeRef(args[0]).getRawClass();
      System.out.println(args[0] + " : " + toSchema(tClass).toString());
      System.out.println(toPigScript(tClass, LzoThriftB64LinePigLoader.class));
    }
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/pig/util/ProtobufTuple.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.util;

import java.io.DataInput;
import java.io.DataInputStream;
import java.io.DataOutput;
import java.io.IOException;
import java.util.List;
import java.util.Set;

import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;

import com.google.common.collect.Sets;
import com.google.protobuf.Message;
import com.google.protobuf.Descriptors.Descriptor;
import com.google.protobuf.Descriptors.FieldDescriptor;
import com.google.protobuf.Message.Builder;

/**
 * This class wraps a protocol buffer message and attempts to delay parsing until individual
 * fields are requested.
 */
public class ProtobufTuple implements Tuple {

  /**
   * Autogenerated by Eclipse.
   */
  private static final long serialVersionUID = 8468589454361280269L;
  
  private final Message msg_;
  private final Descriptor descriptor_;
  private final Tuple realTuple_;
  private final Set<Integer> materializedFieldSet_;
  private final List<FieldDescriptor> fieldDescriptors_;
  private final ProtobufToPig protoConv_;
  private final int protoSize_;
  private boolean ignoreMessage_ = false;

  public ProtobufTuple(Message msg) {
    msg_ = msg;
    descriptor_ = msg.getDescriptorForType();
    fieldDescriptors_ = descriptor_.getFields();
    protoSize_ = fieldDescriptors_.size();
    realTuple_ = TupleFactory.getInstance().newTuple(protoSize_);
    materializedFieldSet_ =  Sets.newHashSetWithExpectedSize(protoSize_);
    protoConv_ = new ProtobufToPig();
  }

  @Override
  public void append(Object obj) {
    realTuple_.append(obj);
  }

  @Override
  public Object get(int idx) throws ExecException {
    if (!ignoreMessage_ && idx < protoSize_ && !materializedFieldSet_.contains(idx)) {
      FieldDescriptor fieldDescriptor = fieldDescriptors_.get(idx);
      Object fieldValue = msg_.getField(fieldDescriptor);
      if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
        realTuple_.set(idx, protoConv_.messageToTuple(fieldDescriptor, fieldValue));
      } else {
        realTuple_.set(idx, protoConv_.singleFieldToTuple(fieldDescriptor, fieldValue));
      }
      materializedFieldSet_.add(idx);
    }
    return realTuple_.get(idx);
  }

  @Override
  public List<Object> getAll() {
    convertAll();
    return realTuple_.getAll();
  }

  @Override
  public long getMemorySize() {
    // The protobuf estimate is obviously inaccurate.
    return msg_.getSerializedSize() + realTuple_.getMemorySize();
  }

  @Override
  public byte getType(int idx) throws ExecException {
    get(idx);
    return realTuple_.getType(idx);
  }

  @Override
  public boolean isNull() {
    return realTuple_.isNull();
  }

  @Override
  public boolean isNull(int idx) throws ExecException {
    get(idx);
    return realTuple_.isNull(idx);
  }

  @Override
  public void reference(Tuple arg0) {
    realTuple_.reference(arg0);
    // Ignore the Message from now on.
    ignoreMessage_ = true;
  }

  @Override
  public void set(int idx, Object val) throws ExecException {
    realTuple_.set(idx, val);
    materializedFieldSet_.add(idx);
  }

  @Override
  public void setNull(boolean isNull) {
    realTuple_.setNull(isNull);    
  }

  @Override
  public int size() {
    return realTuple_.size();
  }

  @Override
  public String toDelimitedString(String delim) throws ExecException {
    convertAll();
    return realTuple_.toDelimitedString(delim);
  }

  @Override
  public void readFields(DataInput inp) throws IOException {
    Builder builder = msg_.newBuilderForType();
    try {
      builder.mergeDelimitedFrom((DataInputStream) inp);
    } catch (ClassCastException e) {
      throw new IOException("Provided DataInput not instance of DataInputStream.", e);
    }
    Message msg = builder.build();
    realTuple_ .reference(new ProtobufTuple(msg));
  }

  @Override
  public void write(DataOutput out) throws IOException {
    convertAll();
    realTuple_.write(out);
  }

  @SuppressWarnings("unchecked")
  @Override
  public int compareTo(Object arg0) {
    convertAll();
    return realTuple_.compareTo(arg0);
  }

  private void convertAll() {
    for (int i = 0; i < protoSize_; i++) {
      if (!materializedFieldSet_.contains(i)) {
        try {
          get(i);
        } catch (ExecException e) {
          throw new RuntimeException("Unable to process field " + i + " of the protobuf", e);
        }
      }
    }
  }
}=======
package com.twitter.elephantbird.pig.util;

import java.util.List;

import com.google.protobuf.Message;
import com.google.protobuf.Descriptors.Descriptor;
import com.google.protobuf.Descriptors.FieldDescriptor;

@SuppressWarnings("serial")
/**
 * This class wraps a protocol buffer message and attempts to delay parsing until individual
 * fields are requested.
 */
public class ProtobufTuple extends AbstractLazyTuple {

  private final Message msg_;
  private final Descriptor descriptor_;
  private final List<FieldDescriptor> fieldDescriptors_;
  private final ProtobufToPig protoConv_;
  private final int protoSize_;

  public ProtobufTuple(Message msg) {
    msg_ = msg;
    descriptor_ = msg.getDescriptorForType();
    fieldDescriptors_ = descriptor_.getFields();
    protoSize_ = fieldDescriptors_.size();
    protoConv_ = new ProtobufToPig();
    initRealTuple(protoSize_);
  }

  protected Object getObjectAt(int idx) {
    FieldDescriptor fieldDescriptor = fieldDescriptors_.get(idx);
    Object fieldValue = msg_.getField(fieldDescriptor);
    if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
      return protoConv_.messageToTuple(fieldDescriptor, fieldValue);
    } else {
      return protoConv_.singleFieldToTuple(fieldDescriptor, fieldValue);
    }
  }

  @Override
  public long getMemorySize() {
    // The protobuf estimate is obviously inaccurate.
    return msg_.getSerializedSize() + realTuple.getMemorySize();
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/pig/util/ProtobufToPig.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.util;

import java.util.List;
import java.util.Map;

import com.google.common.collect.Lists;
import com.google.common.collect.Maps;
import com.google.protobuf.Descriptors.Descriptor;
import com.google.protobuf.Descriptors.EnumValueDescriptor;
import com.google.protobuf.Descriptors.FieldDescriptor;
import com.google.protobuf.ByteString;
import com.google.protobuf.Message;
import com.sun.org.apache.xerces.internal.impl.dv.xs.SchemaDateTimeException;
import com.twitter.data.proto.Misc.CountedMap;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * A class for turning codegen'd protos into Pig Tuples and Schemas
 * for custom Pig LoadFuncs.
 * @author Kevin Weil
 */
public class ProtobufToPig {
  private static final Logger LOG = LoggerFactory.getLogger(ProtobufToPig.class);

  private static final TupleFactory tupleFactory_ = TupleFactory.getInstance();
  private static BagFactory bagFactory_ = BagFactory.getInstance();

  public enum CoercionLevel { kNoCoercion, kAllowCoercionToPigMaps }

  private final CoercionLevel coercionLevel_;

  public ProtobufToPig() {
    this(CoercionLevel.kAllowCoercionToPigMaps);
  }

  public ProtobufToPig(CoercionLevel coercionLevel) {
    coercionLevel_ = coercionLevel;
  }
  /**
   * Turn a generic message into a Tuple.  Individual fields that are enums
   * are converted into their string equivalents.  Fields that are not filled
   * out in the protobuf are set to null, unless there is a default field value in
   * which case that is used instead.
   * @param msg the protobuf message
   * @return a pig tuple representing the message.
   */
  public Tuple toTuple(Message msg) {
    if (msg == null) {
      // Pig tuples deal gracefully with nulls.
      // Also, we can be called with null here in recursive calls.
      return null;
    }

    Descriptor msgDescriptor = msg.getDescriptorForType();
    Tuple tuple = tupleFactory_.newTuple(msgDescriptor.getFields().size());
    int curField = 0;
    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        // Get the set value, or the default value, or null.
        Object fieldValue = getFieldValue(msg, fieldDescriptor);

        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          tuple.set(curField++, messageToTuple(fieldDescriptor, fieldValue));
        } else {
          tuple.set(curField++, singleFieldToTuple(fieldDescriptor, fieldValue));
        }
      }
    } catch (ExecException e) {
      LOG.warn("Could not convert msg " + msg + " to tuple", e);
    }

    return tuple;
  }

  /**
   * Translate a nested message to a tuple.  If the field is repeated, it walks the list and adds each to a bag.
   * Otherwise, it just adds the given one.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object representing fieldValue in Pig -- either a bag or a tuple.
   */
  @SuppressWarnings("unchecked")
  protected Object messageToTuple(FieldDescriptor fieldDescriptor, Object fieldValue) {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToTuple called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      // The protobuf contract is that if the field is repeated, then the object returned is actually a List
      // of the underlying datatype, which in this case is a nested message.
      List<Message> messageList = (List<Message>) (fieldValue != null ? fieldValue : Lists.newArrayList());

      // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
      // we can force the type into a pig map type.
      if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
          fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName())) {
        Map<Object, Long> map = Maps.newHashMap();
        for (Message m : messageList) {
          CountedMap cm = (CountedMap) m;
          final Long curCount = map.get(cm.getKey());
          map.put(cm.getKey(), (curCount == null ? 0L : curCount) + cm.getValue());
        }
        return map;
      } else {
        DataBag bag = bagFactory_.newDefaultBag();
        for (Message m : messageList) {
          bag.add(new ProtobufTuple(m));
        }
        return bag;
      }
    } else {
      return new ProtobufTuple((Message)fieldValue);
    }
  }

  /**
   * Translate a single field to a tuple.  If the field is repeated, it walks the list and adds each to a bag.
   * Otherwise, it just adds the given one.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object representing fieldValue in Pig -- either a bag or a single field.
   * @throws ExecException if Pig decides to.  Shouldn't happen because we won't walk off the end of a tuple's field set.
   */
  @SuppressWarnings("unchecked")
  protected Object singleFieldToTuple(FieldDescriptor fieldDescriptor, Object fieldValue) throws ExecException {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "messageToFieldSchema called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      // The protobuf contract is that if the field is repeated, then the object returned is actually a List
      // of the underlying datatype, which in this case is a "primitive" like int, float, String, etc.
      // We have to make a single-item tuple out of it to put it in the bag.
      DataBag bag = bagFactory_.newDefaultBag();
      List<Object> fieldValueList = (List<Object>) (fieldValue != null ? fieldValue : Lists.newArrayList());
      for (Object singleFieldValue : fieldValueList) {
        Object nonEnumFieldValue = coerceToPigTypes(fieldDescriptor, singleFieldValue);
        Tuple innerTuple = tupleFactory_.newTuple(1);
        innerTuple.set(0, nonEnumFieldValue);
        bag.add(innerTuple);
      }
      return bag;
    } else {
      return coerceToPigTypes(fieldDescriptor, fieldValue);
    }
  }

  /**
   * If the given field value is an enum, translate it to the enum's name as a string, since Pig cannot handle enums.
   * Also, if the given field value is a bool, translate it to 0 or 1 to avoid Pig bools, which can be sketchy.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object, unless it was from an enum field, in which case we return the name of the enum field.
   */
  private Object coerceToPigTypes(FieldDescriptor fieldDescriptor, Object fieldValue) {
    if (fieldDescriptor.getType() == FieldDescriptor.Type.ENUM && fieldValue != null) {
      EnumValueDescriptor enumValueDescriptor = (EnumValueDescriptor)fieldValue;
      return enumValueDescriptor.getName();
    } else if (fieldDescriptor.getType() == FieldDescriptor.Type.BOOL && fieldValue != null) {
      Boolean boolValue = (Boolean)fieldValue;
      return new Integer(boolValue ? 1 : 0);
    } else if (fieldDescriptor.getType() == FieldDescriptor.Type.BYTES && fieldValue != null) {
      ByteString bsValue = (ByteString)fieldValue;
      return new DataByteArray(bsValue.toByteArray());
    }
    return fieldValue;
  }

  /**
   * A utility function for getting the value of a field in a protobuf message.  It first tries the
   * literal set value in the protobuf's field list.  If the value isn't set, and the field has a default
   * value, it uses that.  Otherwise, it returns null.
   * @param msg the protobuf message
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the value of the field, or null if none can be assigned.
   */
  protected Object getFieldValue(Message msg, FieldDescriptor fieldDescriptor) {
    Object o = null;
    Map<FieldDescriptor, Object> setFields = msg.getAllFields();
    if (setFields.containsKey(fieldDescriptor)) {
      o = setFields.get(fieldDescriptor);
    } else if (fieldDescriptor.hasDefaultValue()) {
      o = fieldDescriptor.getDefaultValue();
    }

    return o;
  }

  /**
   * Turn a generic message descriptor into a Schema.  Individual fields that are enums
   * are converted into their string equivalents.
   * @param msgDescriptor the descriptor for the given message type.
   * @return a pig schema representing the message.
   */
  public Schema toSchema(Descriptor msgDescriptor) {
    Schema schema = new Schema();

    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          schema.add(messageToFieldSchema(fieldDescriptor));
        } else {
          schema.add(singleFieldToFieldSchema(fieldDescriptor));
        }
      }
    } catch (FrontendException e) {
      LOG.warn("Could not convert descriptor " + msgDescriptor + " to schema", e);
    }

    return schema;
  }

  /**
   * Turn a nested message into a Schema object.  For repeated nested messages, it generates a schema for a bag of
   * tuples.  For non-repeated nested messages, it just generates a schema for the tuple itself.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the Schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private FieldSchema messageToFieldSchema(FieldDescriptor fieldDescriptor) throws FrontendException {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToFieldSchema called with field of type " + fieldDescriptor.getType();

    // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
    // we can force the type into a pig map type.
    if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
        fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName()) && fieldDescriptor.isRepeated()) {
      return new FieldSchema(fieldDescriptor.getName(), null, DataType.MAP);
    }

    Schema innerSchema = toSchema(fieldDescriptor.getMessageType());

    if (fieldDescriptor.isRepeated()) {
      Schema tupleSchema = new Schema();
      tupleSchema.add(new FieldSchema(fieldDescriptor.getName() + "_tuple", innerSchema, DataType.TUPLE));
      return new FieldSchema(fieldDescriptor.getName(), tupleSchema, DataType.BAG);
    } else {
      return new FieldSchema(fieldDescriptor.getName(), innerSchema, DataType.TUPLE);
    }
  }

  /**
   * Turn a single field into a Schema object.  For repeated single fields, it generates a schema for a bag of single-item tuples.
   * For non-repeated fields, it just generates a standard field schema.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the Schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private FieldSchema singleFieldToFieldSchema(FieldDescriptor fieldDescriptor) throws FrontendException {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "singleFieldToFieldSchema called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      Schema itemSchema = new Schema();
      itemSchema.add(new FieldSchema(fieldDescriptor.getName(), null, getPigDataType(fieldDescriptor)));
      Schema itemTupleSchema = new Schema();
      itemTupleSchema.add(new FieldSchema(fieldDescriptor.getName() + "_tuple", itemSchema, DataType.TUPLE));

      return new FieldSchema(fieldDescriptor.getName() + "_bag", itemTupleSchema, DataType.BAG);
    } else {
      return new FieldSchema(fieldDescriptor.getName(), null, getPigDataType(fieldDescriptor));
    }
  }

  /**
   * Translate between protobuf's datatype representation and Pig's datatype representation.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the byte representing the pig datatype for the given field type.
   */
  private byte getPigDataType(FieldDescriptor fieldDescriptor) {
    switch (fieldDescriptor.getType()) {
      case INT32:
      case UINT32:
      case SINT32:
      case FIXED32:
      case SFIXED32:
      case BOOL: // We convert booleans to ints for pig output.
        return DataType.INTEGER;
      case INT64:
      case UINT64:
      case SINT64:
      case FIXED64:
      case SFIXED64:
        return DataType.LONG;
      case FLOAT:
        return DataType.FLOAT;
      case DOUBLE:
        return DataType.DOUBLE;
      case STRING:
      case ENUM: // We convert enums to strings for pig output.
        return DataType.CHARARRAY;
      case BYTES:
        return DataType.BYTEARRAY;
      case MESSAGE:
        throw new IllegalArgumentException("getPigDataType called on field " + fieldDescriptor.getFullName() + " of type message.");
      default:
        throw new IllegalArgumentException("Unexpected field type. " + fieldDescriptor.toString() + " " + fieldDescriptor.getFullName() + " " + fieldDescriptor.getType());
    }
  }

  /**
   * Turn a generic message descriptor into a loading schema for a pig script.
   * @param msgDescriptor the descriptor for the given message type.
   * @param loaderClassName the fully qualified classname of the pig loader to use.  Not
   * passed a <code>Class<? extends LoadFunc></code> because in many situations that class
   * is being generated as well, and so doesn't exist in compiled form.
   * @return a pig schema representing the message.
   */
  public String toPigScript(Descriptor msgDescriptor, String loaderClassName) {
    StringBuffer sb = new StringBuffer();
    final int initialTabOffset = 3;

    sb.append("raw_data = load '$INPUT_FILES' using " + loaderClassName + "()").append("\n");
    sb.append(tabs(initialTabOffset)).append("as (").append("\n");
    sb.append(toPigScriptInternal(msgDescriptor, initialTabOffset));
    sb.append(tabs(initialTabOffset)).append(");").append("\n").append("\n");

    return sb.toString();
  }

  /**
   * Turn a generic message descriptor into a loading schema for a pig script.  Individual fields that are enums
   * are converted into their string equivalents.
   * @param msgDescriptor the descriptor for the given message type.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return a pig schema representing the message.
   */
  private StringBuffer toPigScriptInternal(Descriptor msgDescriptor, int numTabs) {
    StringBuffer sb = new StringBuffer();
    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        // We have to add a comma after every line EXCEPT for the last, or Pig gets mad.
        boolean isLast = (fieldDescriptor == msgDescriptor.getFields().get(msgDescriptor.getFields().size() - 1));
        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          sb.append(messageToPigScript(fieldDescriptor, numTabs + 1, isLast));
        } else {
          sb.append(singleFieldToPigScript(fieldDescriptor, numTabs + 1, isLast));
        }
      }
    } catch (FrontendException e) {
      LOG.warn("Could not convert descriptor " + msgDescriptor + " to pig script", e);
    }

    return sb;
  }

  /**
   * Turn a nested message into a pig script load string.  For repeated nested messages, it generates a string for a bag of
   * tuples.  For non-repeated nested messages, it just generates a string for the tuple itself.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return the pig script load schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private StringBuffer messageToPigScript(FieldDescriptor fieldDescriptor, int numTabs, boolean isLast) throws FrontendException {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToPigScript called with field of type " + fieldDescriptor.getType();

    // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
    // we force the type into a pig map type.
    if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
        fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName()) && fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName())
          .append(": map[]").append(isLast ? "" : ",").append("\n");
    }

    if (fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": bag {").append("\n")
          .append(tabs(numTabs + 1)).append(fieldDescriptor.getName()).append("_tuple: tuple (").append("\n")
          .append(toPigScriptInternal(fieldDescriptor.getMessageType(), numTabs + 2))
          .append(tabs(numTabs + 1)).append(")").append("\n")
          .append(tabs(numTabs)).append("}").append(isLast ? "" : ",").append("\n");
    } else {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": tuple (").append("\n")
          .append(toPigScriptInternal(fieldDescriptor.getMessageType(), numTabs + 1))
          .append(tabs(numTabs)).append(")").append(isLast ? "" : ",").append("\n");
    }
  }

  /**
   * Turn a single field into a pig script load string.  For repeated single fields, it generates a string for a bag of single-item tuples.
   * For non-repeated fields, it just generates a standard single-element string.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return the pig script load string for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private StringBuffer singleFieldToPigScript(FieldDescriptor fieldDescriptor, int numTabs, boolean isLast) throws FrontendException {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "singleFieldToPigScript called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append("_bag: bag {").append("\n")
          .append(tabs(numTabs + 1)).append(fieldDescriptor.getName()).append("_tuple: tuple (").append("\n")
          .append(tabs(numTabs + 2)).append(fieldDescriptor.getName()).append(": ").append(getPigScriptDataType(fieldDescriptor)).append("\n")
          .append(tabs(numTabs + 1)).append(")").append("\n")
          .append(tabs(numTabs)).append("}").append(isLast ? "" : ",").append("\n");
    } else {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": ")
          .append(getPigScriptDataType(fieldDescriptor)).append(isLast ? "" : ",").append("\n");
    }
  }

  /**
   * Translate between protobuf's datatype representation and Pig's datatype representation.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the byte representing the pig datatype for the given field type.
   */
  private String getPigScriptDataType(FieldDescriptor fieldDescriptor) {
    switch (fieldDescriptor.getType()) {
      case INT32:
      case UINT32:
      case SINT32:
      case FIXED32:
      case SFIXED32:
      case BOOL: // We convert booleans to ints for pig output.
        return "int";
      case INT64:
      case UINT64:
      case SINT64:
      case FIXED64:
      case SFIXED64:
        return "long";
      case FLOAT:
        return "float";
      case DOUBLE:
        return "double";
      case STRING:
      case ENUM: // We convert enums to strings for pig output.
        return "chararray";
      case BYTES:
        return "bytearray";
      case MESSAGE:
        throw new IllegalArgumentException("getPigScriptDataType called on field " + fieldDescriptor.getFullName() + " of type message.");
      default:
        throw new IllegalArgumentException("Unexpected field type. " + fieldDescriptor.toString() + " " + fieldDescriptor.getFullName() + " " + fieldDescriptor.getType());
    }
  }

  private StringBuffer tabs(int numTabs) {
    StringBuffer sb = new StringBuffer();
    for (int i = 0; i < numTabs; i++) {
      sb.append("  ");
    }
    return sb;
  }  
}=======
package com.twitter.elephantbird.pig.util;

import java.util.List;
import java.util.Map;

import com.google.common.collect.Lists;
import com.google.common.collect.Maps;
import com.google.protobuf.Descriptors.Descriptor;
import com.google.protobuf.Descriptors.EnumValueDescriptor;
import com.google.protobuf.Descriptors.FieldDescriptor;
import com.google.protobuf.ByteString;
import com.google.protobuf.Message;
import com.twitter.data.proto.Misc.CountedMap;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * A class for turning codegen'd protos into Pig Tuples and Schemas
 * for custom Pig LoadFuncs.
 * @author Kevin Weil
 */
public class ProtobufToPig {
  private static final Logger LOG = LoggerFactory.getLogger(ProtobufToPig.class);

  private static final TupleFactory tupleFactory_ = TupleFactory.getInstance();
  private static BagFactory bagFactory_ = BagFactory.getInstance();

  public enum CoercionLevel { kNoCoercion, kAllowCoercionToPigMaps }

  private final CoercionLevel coercionLevel_;

  public ProtobufToPig() {
    this(CoercionLevel.kAllowCoercionToPigMaps);
  }

  public ProtobufToPig(CoercionLevel coercionLevel) {
    coercionLevel_ = coercionLevel;
  }
  /**
   * Turn a generic message into a Tuple.  Individual fields that are enums
   * are converted into their string equivalents.  Fields that are not filled
   * out in the protobuf are set to null, unless there is a default field value in
   * which case that is used instead.
   * @param msg the protobuf message
   * @return a pig tuple representing the message.
   */
  public Tuple toTuple(Message msg) {
    if (msg == null) {
      // Pig tuples deal gracefully with nulls.
      // Also, we can be called with null here in recursive calls.
      return null;
    }

    Descriptor msgDescriptor = msg.getDescriptorForType();
    Tuple tuple = tupleFactory_.newTuple(msgDescriptor.getFields().size());
    int curField = 0;
    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        // Get the set value, or the default value, or null.
        Object fieldValue = getFieldValue(msg, fieldDescriptor);

        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          tuple.set(curField++, messageToTuple(fieldDescriptor, fieldValue));
        } else {
          tuple.set(curField++, singleFieldToTuple(fieldDescriptor, fieldValue));
        }
      }
    } catch (ExecException e) {
      LOG.warn("Could not convert msg " + msg + " to tuple", e);
    }

    return tuple;
  }

  /**
   * Translate a nested message to a tuple.  If the field is repeated, it walks the list and adds each to a bag.
   * Otherwise, it just adds the given one.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object representing fieldValue in Pig -- either a bag or a tuple.
   */
  @SuppressWarnings("unchecked")
  protected Object messageToTuple(FieldDescriptor fieldDescriptor, Object fieldValue) {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToTuple called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      // The protobuf contract is that if the field is repeated, then the object returned is actually a List
      // of the underlying datatype, which in this case is a nested message.
      List<Message> messageList = (List<Message>) (fieldValue != null ? fieldValue : Lists.newArrayList());

      // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
      // we can force the type into a pig map type.
      if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
          fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName())) {
        Map<Object, Long> map = Maps.newHashMap();
        for (Message m : messageList) {
          CountedMap cm = (CountedMap) m;
          final Long curCount = map.get(cm.getKey());
          map.put(cm.getKey(), (curCount == null ? 0L : curCount) + cm.getValue());
        }
        return map;
      } else {
        DataBag bag = bagFactory_.newDefaultBag();
        for (Message m : messageList) {
          bag.add(new ProtobufTuple(m));
        }
        return bag;
      }
    } else {
      return new ProtobufTuple((Message)fieldValue);
    }
  }

  /**
   * Translate a single field to a tuple.  If the field is repeated, it walks the list and adds each to a bag.
   * Otherwise, it just adds the given one.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object representing fieldValue in Pig -- either a bag or a single field.
   * @throws ExecException if Pig decides to.  Shouldn't happen because we won't walk off the end of a tuple's field set.
   */
  @SuppressWarnings("unchecked")
  protected Object singleFieldToTuple(FieldDescriptor fieldDescriptor, Object fieldValue) {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "messageToFieldSchema called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      // The protobuf contract is that if the field is repeated, then the object returned is actually a List
      // of the underlying datatype, which in this case is a "primitive" like int, float, String, etc.
      // We have to make a single-item tuple out of it to put it in the bag.
      DataBag bag = bagFactory_.newDefaultBag();
      List<Object> fieldValueList = (List<Object>) (fieldValue != null ? fieldValue : Lists.newArrayList());
      for (Object singleFieldValue : fieldValueList) {
        Object nonEnumFieldValue = coerceToPigTypes(fieldDescriptor, singleFieldValue);
        Tuple innerTuple = tupleFactory_.newTuple(1);
        try {
          innerTuple.set(0, nonEnumFieldValue);
        } catch (ExecException e) { // not expected
          throw new RuntimeException(e);
        }
        bag.add(innerTuple);
      }
      return bag;
    } else {
      return coerceToPigTypes(fieldDescriptor, fieldValue);
    }
  }

  /**
   * If the given field value is an enum, translate it to the enum's name as a string, since Pig cannot handle enums.
   * Also, if the given field value is a bool, translate it to 0 or 1 to avoid Pig bools, which can be sketchy.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object, unless it was from an enum field, in which case we return the name of the enum field.
   */
  private Object coerceToPigTypes(FieldDescriptor fieldDescriptor, Object fieldValue) {
    if (fieldDescriptor.getType() == FieldDescriptor.Type.ENUM && fieldValue != null) {
      EnumValueDescriptor enumValueDescriptor = (EnumValueDescriptor)fieldValue;
      return enumValueDescriptor.getName();
    } else if (fieldDescriptor.getType() == FieldDescriptor.Type.BOOL && fieldValue != null) {
      Boolean boolValue = (Boolean)fieldValue;
      return new Integer(boolValue ? 1 : 0);
    } else if (fieldDescriptor.getType() == FieldDescriptor.Type.BYTES && fieldValue != null) {
      ByteString bsValue = (ByteString)fieldValue;
      return new DataByteArray(bsValue.toByteArray());
    }
    return fieldValue;
  }

  /**
   * A utility function for getting the value of a field in a protobuf message.  It first tries the
   * literal set value in the protobuf's field list.  If the value isn't set, and the field has a default
   * value, it uses that.  Otherwise, it returns null.
   * @param msg the protobuf message
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the value of the field, or null if none can be assigned.
   */
  protected Object getFieldValue(Message msg, FieldDescriptor fieldDescriptor) {
    Object o = null;
    Map<FieldDescriptor, Object> setFields = msg.getAllFields();
    if (setFields.containsKey(fieldDescriptor)) {
      o = setFields.get(fieldDescriptor);
    } else if (fieldDescriptor.hasDefaultValue()) {
      o = fieldDescriptor.getDefaultValue();
    }

    return o;
  }

  /**
   * Turn a generic message descriptor into a Schema.  Individual fields that are enums
   * are converted into their string equivalents.
   * @param msgDescriptor the descriptor for the given message type.
   * @return a pig schema representing the message.
   */
  public Schema toSchema(Descriptor msgDescriptor) {
    Schema schema = new Schema();

    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          schema.add(messageToFieldSchema(fieldDescriptor));
        } else {
          schema.add(singleFieldToFieldSchema(fieldDescriptor));
        }
      }
    } catch (FrontendException e) {
      LOG.warn("Could not convert descriptor " + msgDescriptor + " to schema", e);
    }

    return schema;
  }

  /**
   * Turn a nested message into a Schema object.  For repeated nested messages, it generates a schema for a bag of
   * tuples.  For non-repeated nested messages, it just generates a schema for the tuple itself.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the Schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private FieldSchema messageToFieldSchema(FieldDescriptor fieldDescriptor) throws FrontendException {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToFieldSchema called with field of type " + fieldDescriptor.getType();

    // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
    // we can force the type into a pig map type.
    if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
        fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName()) && fieldDescriptor.isRepeated()) {
      return new FieldSchema(fieldDescriptor.getName(), null, DataType.MAP);
    }

    Schema innerSchema = toSchema(fieldDescriptor.getMessageType());

    if (fieldDescriptor.isRepeated()) {
      Schema tupleSchema = new Schema();
      tupleSchema.add(new FieldSchema(fieldDescriptor.getName() + "_tuple", innerSchema, DataType.TUPLE));
      return new FieldSchema(fieldDescriptor.getName(), tupleSchema, DataType.BAG);
    } else {
      return new FieldSchema(fieldDescriptor.getName(), innerSchema, DataType.TUPLE);
    }
  }

  /**
   * Turn a single field into a Schema object.  For repeated single fields, it generates a schema for a bag of single-item tuples.
   * For non-repeated fields, it just generates a standard field schema.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the Schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private FieldSchema singleFieldToFieldSchema(FieldDescriptor fieldDescriptor) throws FrontendException {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "singleFieldToFieldSchema called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      Schema itemSchema = new Schema();
      itemSchema.add(new FieldSchema(fieldDescriptor.getName(), null, getPigDataType(fieldDescriptor)));
      Schema itemTupleSchema = new Schema();
      itemTupleSchema.add(new FieldSchema(fieldDescriptor.getName() + "_tuple", itemSchema, DataType.TUPLE));

      return new FieldSchema(fieldDescriptor.getName() + "_bag", itemTupleSchema, DataType.BAG);
    } else {
      return new FieldSchema(fieldDescriptor.getName(), null, getPigDataType(fieldDescriptor));
    }
  }

  /**
   * Translate between protobuf's datatype representation and Pig's datatype representation.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the byte representing the pig datatype for the given field type.
   */
  private byte getPigDataType(FieldDescriptor fieldDescriptor) {
    switch (fieldDescriptor.getType()) {
      case INT32:
      case UINT32:
      case SINT32:
      case FIXED32:
      case SFIXED32:
      case BOOL: // We convert booleans to ints for pig output.
        return DataType.INTEGER;
      case INT64:
      case UINT64:
      case SINT64:
      case FIXED64:
      case SFIXED64:
        return DataType.LONG;
      case FLOAT:
        return DataType.FLOAT;
      case DOUBLE:
        return DataType.DOUBLE;
      case STRING:
      case ENUM: // We convert enums to strings for pig output.
        return DataType.CHARARRAY;
      case BYTES:
        return DataType.BYTEARRAY;
      case MESSAGE:
        throw new IllegalArgumentException("getPigDataType called on field " + fieldDescriptor.getFullName() + " of type message.");
      default:
        throw new IllegalArgumentException("Unexpected field type. " + fieldDescriptor.toString() + " " + fieldDescriptor.getFullName() + " " + fieldDescriptor.getType());
    }
  }

  /**
   * Turn a generic message descriptor into a loading schema for a pig script.
   * @param msgDescriptor the descriptor for the given message type.
   * @param loaderClassName the fully qualified classname of the pig loader to use.  Not
   * passed a <code>Class<? extends LoadFunc></code> because in many situations that class
   * is being generated as well, and so doesn't exist in compiled form.
   * @return a pig schema representing the message.
   */
  public String toPigScript(Descriptor msgDescriptor, String loaderClassName) {
    StringBuffer sb = new StringBuffer();
    final int initialTabOffset = 3;

    sb.append("raw_data = load '$INPUT_FILES' using " + loaderClassName + "()").append("\n");
    sb.append(tabs(initialTabOffset)).append("as (").append("\n");
    sb.append(toPigScriptInternal(msgDescriptor, initialTabOffset));
    sb.append(tabs(initialTabOffset)).append(");").append("\n").append("\n");

    return sb.toString();
  }

  /**
   * Turn a generic message descriptor into a loading schema for a pig script.  Individual fields that are enums
   * are converted into their string equivalents.
   * @param msgDescriptor the descriptor for the given message type.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return a pig schema representing the message.
   */
  private StringBuffer toPigScriptInternal(Descriptor msgDescriptor, int numTabs) {
    StringBuffer sb = new StringBuffer();
    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        // We have to add a comma after every line EXCEPT for the last, or Pig gets mad.
        boolean isLast = (fieldDescriptor == msgDescriptor.getFields().get(msgDescriptor.getFields().size() - 1));
        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          sb.append(messageToPigScript(fieldDescriptor, numTabs + 1, isLast));
        } else {
          sb.append(singleFieldToPigScript(fieldDescriptor, numTabs + 1, isLast));
        }
      }
    } catch (FrontendException e) {
      LOG.warn("Could not convert descriptor " + msgDescriptor + " to pig script", e);
    }

    return sb;
  }

  /**
   * Turn a nested message into a pig script load string.  For repeated nested messages, it generates a string for a bag of
   * tuples.  For non-repeated nested messages, it just generates a string for the tuple itself.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return the pig script load schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private StringBuffer messageToPigScript(FieldDescriptor fieldDescriptor, int numTabs, boolean isLast) throws FrontendException {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToPigScript called with field of type " + fieldDescriptor.getType();

    // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
    // we force the type into a pig map type.
    if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
        fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName()) && fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName())
          .append(": map[]").append(isLast ? "" : ",").append("\n");
    }

    if (fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": bag {").append("\n")
          .append(tabs(numTabs + 1)).append(fieldDescriptor.getName()).append("_tuple: tuple (").append("\n")
          .append(toPigScriptInternal(fieldDescriptor.getMessageType(), numTabs + 2))
          .append(tabs(numTabs + 1)).append(")").append("\n")
          .append(tabs(numTabs)).append("}").append(isLast ? "" : ",").append("\n");
    } else {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": tuple (").append("\n")
          .append(toPigScriptInternal(fieldDescriptor.getMessageType(), numTabs + 1))
          .append(tabs(numTabs)).append(")").append(isLast ? "" : ",").append("\n");
    }
  }

  /**
   * Turn a single field into a pig script load string.  For repeated single fields, it generates a string for a bag of single-item tuples.
   * For non-repeated fields, it just generates a standard single-element string.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return the pig script load string for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private StringBuffer singleFieldToPigScript(FieldDescriptor fieldDescriptor, int numTabs, boolean isLast) throws FrontendException {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "singleFieldToPigScript called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append("_bag: bag {").append("\n")
          .append(tabs(numTabs + 1)).append(fieldDescriptor.getName()).append("_tuple: tuple (").append("\n")
          .append(tabs(numTabs + 2)).append(fieldDescriptor.getName()).append(": ").append(getPigScriptDataType(fieldDescriptor)).append("\n")
          .append(tabs(numTabs + 1)).append(")").append("\n")
          .append(tabs(numTabs)).append("}").append(isLast ? "" : ",").append("\n");
    } else {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": ")
          .append(getPigScriptDataType(fieldDescriptor)).append(isLast ? "" : ",").append("\n");
    }
  }

  /**
   * Translate between protobuf's datatype representation and Pig's datatype representation.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the byte representing the pig datatype for the given field type.
   */
  private String getPigScriptDataType(FieldDescriptor fieldDescriptor) {
    switch (fieldDescriptor.getType()) {
      case INT32:
      case UINT32:
      case SINT32:
      case FIXED32:
      case SFIXED32:
      case BOOL: // We convert booleans to ints for pig output.
        return "int";
      case INT64:
      case UINT64:
      case SINT64:
      case FIXED64:
      case SFIXED64:
        return "long";
      case FLOAT:
        return "float";
      case DOUBLE:
        return "double";
      case STRING:
      case ENUM: // We convert enums to strings for pig output.
        return "chararray";
      case BYTES:
        return "bytearray";
      case MESSAGE:
        throw new IllegalArgumentException("getPigScriptDataType called on field " + fieldDescriptor.getFullName() + " of type message.");
      default:
        throw new IllegalArgumentException("Unexpected field type. " + fieldDescriptor.toString() + " " + fieldDescriptor.getFullName() + " " + fieldDescriptor.getType());
    }
  }

  private StringBuffer tabs(int numTabs) {
    StringBuffer sb = new StringBuffer();
    for (int i = 0; i < numTabs; i++) {
      sb.append("  ");
    }
    return sb;
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/pig/util/PigCounterHelper.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.util;

import java.util.Map;

import com.google.common.collect.Maps;
import org.apache.hadoop.mapred.Reporter;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger;
import org.apache.pig.impl.util.Pair;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * A helper class to deal with Hadoop counters in Pig.  They are stored within the singleton
 * PigHadoopLogger instance, but are null for some period of time at job startup, even after
 * Pig has been invoked.  This class buffers counters, trying each time to get a valid Reporter and flushing
 * stored counters each time it does.
 */
public class PigCounterHelper {
  private Map<Pair<String, String>, Long> counterStringMap_ = Maps.newHashMap();
  private Map<Enum<?>, Long> counterEnumMap_ = Maps.newHashMap();
  private Reporter reporter_ = null;

  /**
   * Mocks the Reporter.incrCounter, but adds buffering.
   * See org.apache.hadoop.mapred.Reporter's incrCounter.
   */
  public void incrCounter(String group, String counter, long incr) {
    if (getReporter() != null) { // common case
      getReporter().incrCounter(group, counter, incr);
      if (counterStringMap_.size() > 0) {
        for (Map.Entry<Pair<String, String>, Long> entry : counterStringMap_.entrySet()) {
          getReporter().incrCounter(entry.getKey().first, entry.getKey().second, entry.getValue());
        }
        counterStringMap_.clear();
      }
    } else { // buffer the increments.
      Pair<String, String> key = new Pair<String, String>(group, counter);
      Long currentValue = counterStringMap_.get(key);
      counterStringMap_.put(key, (currentValue == null ? 0 : currentValue) + incr);
    }
  }

  /**
   * Mocks the Reporter.incrCounter, but adds buffering.
   * See org.apache.hadoop.mapred.Reporter's incrCounter.
   */
  public void incrCounter(Enum<?> key, long incr) {
    if (getReporter() != null) {
      getReporter().incrCounter(key, incr);
      if (counterEnumMap_.size() > 0) {
        for (Map.Entry<Enum<?>, Long> entry : counterEnumMap_.entrySet()) {
          getReporter().incrCounter(entry.getKey(), entry.getValue());
        }
        counterEnumMap_.clear();
      }
    } else { // buffer the increments
      Long currentValue = counterEnumMap_.get(key);
      counterEnumMap_.put(key, (currentValue == null ? 0 : currentValue) + incr);
    }
  }

  /**
   * Try for the Reporter object if it hasn't been initialized yet, otherwise just return it.
   * @return the job's reporter object, or null if it isn't retrievable yet.
   */
  private Reporter getReporter() {
    if (reporter_ == null) {
      reporter_ = PigHadoopLogger.getInstance().getReporter();
    }
    return reporter_;
  }
}=======
package com.twitter.elephantbird.pig.util;

import java.util.Map;

import com.google.common.collect.Maps;
import org.apache.hadoop.mapred.Reporter;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger;
import org.apache.pig.impl.util.Pair;

/**
 * A helper class to deal with Hadoop counters in Pig.  They are stored within the singleton
 * PigHadoopLogger instance, but are null for some period of time at job startup, even after
 * Pig has been invoked.  This class buffers counters, trying each time to get a valid Reporter and flushing
 * stored counters each time it does.
 */
public class PigCounterHelper {
  private Map<Pair<String, String>, Long> counterStringMap_ = Maps.newHashMap();
  private Map<Enum<?>, Long> counterEnumMap_ = Maps.newHashMap();
  private PigHadoopLogger pigLogger_ = null;

  /**
   * Mocks the Reporter.incrCounter, but adds buffering.
   * See org.apache.hadoop.mapred.Reporter's incrCounter.
   */
  public void incrCounter(String group, String counter, long incr) {
    if (getReporter() != null) { // common case
      getReporter().incrCounter(group, counter, incr);
      if (counterStringMap_.size() > 0) {
        for (Map.Entry<Pair<String, String>, Long> entry : counterStringMap_.entrySet()) {
          getReporter().incrCounter(entry.getKey().first, entry.getKey().second, entry.getValue());
        }
        counterStringMap_.clear();
      }
    } else { // buffer the increments.
      Pair<String, String> key = new Pair<String, String>(group, counter);
      Long currentValue = counterStringMap_.get(key);
      counterStringMap_.put(key, (currentValue == null ? 0 : currentValue) + incr);
    }
  }

  /**
   * Mocks the Reporter.incrCounter, but adds buffering.
   * See org.apache.hadoop.mapred.Reporter's incrCounter.
   */
  public void incrCounter(Enum<?> key, long incr) {
    Reporter reporter = getReporter();
    if (reporter != null) {
      reporter.incrCounter(key, incr);
      if (counterEnumMap_.size() > 0) {
        for (Map.Entry<Enum<?>, Long> entry : counterEnumMap_.entrySet()) {
          getReporter().incrCounter(entry.getKey(), entry.getValue());
        }
        counterEnumMap_.clear();
      }
    } else { // buffer the increments
      Long currentValue = counterEnumMap_.get(key);
      counterEnumMap_.put(key, (currentValue == null ? 0 : currentValue) + incr);
    }
  }

  /**
   * Try for the Reporter object if it hasn't been initialized yet, otherwise just return it.
   * @return the job's reporter object, or null if it isn't retrievable yet.
   */
  private Reporter getReporter() {
    if (pigLogger_ == null) {
      pigLogger_ = PigHadoopLogger.getInstance();
    }
    return pigLogger_.getReporter();
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/pig/load/LzoThriftB64LinePigLoader.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.nio.charset.Charset;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.apache.thrift.TBase;
import org.apache.thrift.TException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.twitter.elephantbird.mapreduce.io.ThriftConverter;
import com.twitter.elephantbird.pig.util.ThriftToPig;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;

public class LzoThriftB64LinePigLoader<M extends TBase<?, ?>> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoThriftB64LinePigLoader.class);

  private final TypeRef<M> typeRef_;
  private final ThriftConverter<M> converter_;
  private final Base64 base64_ = new Base64();
  private final ThriftToPig<M> thriftToPig_;

  private static final Charset UTF8 = Charset.forName("UTF-8");
  private static final byte RECORD_DELIMITER = (byte)'\n';

  private Pair<String, String> linesRead;
  private Pair<String, String> thriftStructsRead;
  private Pair<String, String> thriftErrors;

  public LzoThriftB64LinePigLoader(String thriftClassName) {
    typeRef_ = ThriftUtils.getTypeRef(thriftClassName);
    converter_ = ThriftConverter.newInstance(typeRef_);
    thriftToPig_ =  ThriftToPig.newInstance(typeRef_);

    String group = "LzoB64Lines of " + typeRef_.getRawClass().getName();
    linesRead = new Pair<String, String>(group, "Lines Read");
    thriftStructsRead = new Pair<String, String>(group, "Thrift Structs");
    thriftErrors = new Pair<String, String>(group, "Errors");

    setLoaderSpec(getClass(), new String[]{thriftClassName});
  }

  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // Since we are not block aligned we throw away the first record of each split and count on a different
    // instance to read it.  The only split this doesn't work for is the first.
    if (!atFirstRecord) {
      getNext();
    }
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    String line;
    Tuple t = null;
    while ((line = is_.readLine(UTF8, RECORD_DELIMITER)) != null) {
      incrCounter(linesRead, 1L);
      M value = converter_.fromBytes(base64_.decode(line.getBytes("UTF-8")));
      if (value != null) {
        try {
          t = thriftToPig_.getPigTuple(value);
          incrCounter(thriftStructsRead, 1L);
          break;
        } catch (TException e) {
          incrCounter(thriftErrors, 1L);
          LOG.warn("ThriftToTuple error :", e); // may be struct mismatch
        }
      }
    }

    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return ThriftToPig.toSchema(typeRef_.getRawClass());
  }
}=======
package com.twitter.elephantbird.pig.load;

import java.io.IOException;

import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.thrift.TBase;

import com.twitter.elephantbird.mapreduce.io.BinaryConverter;
import com.twitter.elephantbird.mapreduce.io.ThriftConverter;
import com.twitter.elephantbird.pig.util.PigUtil;
import com.twitter.elephantbird.pig.util.ThriftToPig;
import com.twitter.elephantbird.util.TypeRef;

/**
 * Loader for LZO files with line-oriented base64 encoded Thrift objects.
 */
public class LzoThriftB64LinePigLoader<M extends TBase<?, ?>> extends LzoBinaryB64LinePigLoader {

  private final TypeRef<M> typeRef_;
  private ThriftConverter<M> converter_;
  private final ThriftToPig<M> thriftToPig_;

  public LzoThriftB64LinePigLoader(String thriftClassName) {
    typeRef_ = PigUtil.getThriftTypeRef(thriftClassName);
    converter_ = ThriftConverter.newInstance(typeRef_);
    thriftToPig_ =  ThriftToPig.newInstance(typeRef_);

    BinaryConverter<Tuple> tupleConverter = new BinaryConverter<Tuple>() {
      public Tuple fromBytes(byte[] messageBuffer) {
        M value = converter_.fromBytes(messageBuffer);
        if (value != null) {
          return thriftToPig_.getLazyTuple(value);
        }
        return null;
      }

      public byte[] toBytes(Tuple message) {
        throw new RuntimeException("not implemented");
      }
    };

    init(thriftClassName, tupleConverter);
    setLoaderSpec(getClass(), new String[]{thriftClassName});
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return ThriftToPig.toSchema(typeRef_.getRawClass());
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/pig/load/LzoThriftBlockPigLoader.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.load;

import java.io.IOException;

import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.apache.thrift.TBase;
import org.apache.thrift.TException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.twitter.elephantbird.mapreduce.io.ThriftBlockReader;
import com.twitter.elephantbird.pig.util.ThriftToPig;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;


public class LzoThriftBlockPigLoader<M extends TBase<?, ?>> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoThriftBlockPigLoader.class);

  private final TypeRef<M> typeRef_;
  private final ThriftToPig<M> thriftToPig_;
  private ThriftBlockReader<M> reader_;

  private Pair<String, String> thriftStructsRead;
  private Pair<String, String> thriftErrors;

  public LzoThriftBlockPigLoader(String thriftClassName) {
    typeRef_ = ThriftUtils.getTypeRef(thriftClassName);
    thriftToPig_ =  ThriftToPig.newInstance(typeRef_);

    String group = "LzoBlocks of " + typeRef_.getRawClass().getName();
    thriftStructsRead = new Pair<String, String>(group, "Thrift Structs Read");
    thriftErrors = new Pair<String, String>(group, "Errors");

    setLoaderSpec(getClass(), new String[]{thriftClassName});
  }

  @Override
  public void postBind() throws IOException {
    reader_ = new ThriftBlockReader<M>(is_, typeRef_);
  }

  @Override
  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // We want to explicitly not do any special syncing here, because the reader_
    // handles this automatically.
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    // If we are past the end of the file split, tell the reader not to read any more new blocks.
    // Then continue reading until the last of the reader's already-parsed values are used up.
    // The next split will start at the next sync point and no records will be missed.
    if (is_.getPosition() > end_) {
      reader_.markNoMoreNewBlocks();
    }

    M value;
    while ((value = reader_.readNext()) != null) {
      try {
        Tuple t = thriftToPig_.getPigTuple(value);
        incrCounter(thriftStructsRead, 1L);
        return t;
      } catch (TException e) {
        incrCounter(thriftErrors, 1L);
        LOG.warn("ThriftToTuple error :", e); // may be corrupt data.
        // try next
      }
    }
    return null;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return ThriftToPig.toSchema(typeRef_.getRawClass());
  }
}=======
package com.twitter.elephantbird.pig.load;

import java.io.IOException;

import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.apache.thrift.TBase;

import com.twitter.elephantbird.mapreduce.io.ThriftBlockReader;
import com.twitter.elephantbird.pig.util.PigUtil;
import com.twitter.elephantbird.pig.util.ThriftToPig;
import com.twitter.elephantbird.util.TypeRef;


public class LzoThriftBlockPigLoader<M extends TBase<?, ?>> extends LzoBaseLoadFunc {

  private final TypeRef<M> typeRef_;
  private final ThriftToPig<M> thriftToPig_;
  private ThriftBlockReader<M> reader_;

  private Pair<String, String> thriftStructsRead;

  public LzoThriftBlockPigLoader(String thriftClassName) {
    typeRef_ = PigUtil.getThriftTypeRef(thriftClassName);
    thriftToPig_ =  ThriftToPig.newInstance(typeRef_);

    String group = "LzoBlocks of " + typeRef_.getRawClass().getName();
    thriftStructsRead = new Pair<String, String>(group, "Thrift Structs Read");

    setLoaderSpec(getClass(), new String[]{thriftClassName});
  }

  @Override
  public void postBind() throws IOException {
    reader_ = new ThriftBlockReader<M>(is_, typeRef_);
  }

  @Override
  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // We want to explicitly not do any special syncing here, because the reader_
    // handles this automatically.
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    // If we are past the end of the file split, tell the reader not to read any more new blocks.
    // Then continue reading until the last of the reader's already-parsed values are used up.
    // The next split will start at the next sync point and no records will be missed.
    if (is_.getPosition() > end_) {
      reader_.markNoMoreNewBlocks();
    }

    M value;
    while ((value = reader_.readNext()) != null) {
      incrCounter(thriftStructsRead, 1L);
      return thriftToPig_.getLazyTuple(value);
    }
    return null;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return ThriftToPig.toSchema(typeRef_.getRawClass());
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.load;

import java.io.IOException;

import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.Message;
import com.twitter.elephantbird.mapreduce.io.ProtobufBlockReader;
import com.twitter.elephantbird.mapreduce.io.ProtobufWritable;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;


public class LzoProtobufBlockPigLoader<M extends Message> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoProtobufBlockPigLoader.class);

  private ProtobufBlockReader<M> reader_ = null;
  private ProtobufWritable<M> value_ = null;
  private TypeRef<M> typeRef_ = null;
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  private Pair<String, String> protobufsRead;
  private Pair<String, String> protobufErrors;

  public LzoProtobufBlockPigLoader() {
    LOG.info("LzoProtobufBlockLoader zero-parameter creation");
  }

  public LzoProtobufBlockPigLoader(String protoClassName) {
    TypeRef<M> typeRef = Protobufs.getTypeRef(protoClassName);
    setTypeRef(typeRef);
    setLoaderSpec(getClass(), new String[]{protoClassName});
  }

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called before getNext!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    value_ = new ProtobufWritable<M>(typeRef_);
    String group = "LzoBlocks of " + typeRef_.getRawClass().getName();
    protobufsRead = new Pair<String, String>(group, "Protobufs Read");
    protobufErrors = new Pair<String, String>(group, "Errors");
  }

  @Override
  public void postBind() throws IOException {
    reader_ = new ProtobufBlockReader<M>(is_, typeRef_);
  }

  @Override
  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // We want to explicitly not do any special syncing here, because the reader_
    // handles this automatically.
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }


  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    // If we are past the end of the file split, tell the reader not to read any more new blocks.
    // Then continue reading until the last of the reader's already-parsed values are used up.
    // The next split will start at the next sync point and no records will be missed.
    if (is_.getPosition() > end_) {
      reader_.markNoMoreNewBlocks();
    }

    Tuple t = null;
    if (reader_.readProtobuf(value_)) {
      if (value_.get() == null) {
        incrCounter(protobufErrors, 1);
      }
      t = new ProtobufTuple(value_.get());
      incrCounter(protobufsRead, 1L);
    }
    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}=======
package com.twitter.elephantbird.pig.load;

import java.io.IOException;

import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.Message;
import com.twitter.elephantbird.mapreduce.io.ProtobufBlockReader;
import com.twitter.elephantbird.mapreduce.io.ProtobufWritable;
import com.twitter.elephantbird.pig.util.PigUtil;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;


public class LzoProtobufBlockPigLoader<M extends Message> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoProtobufBlockPigLoader.class);

  private ProtobufBlockReader<M> reader_ = null;
  private ProtobufWritable<M> value_ = null;
  private TypeRef<M> typeRef_ = null;
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  private Pair<String, String> protobufsRead;
  private Pair<String, String> protobufErrors;

  public LzoProtobufBlockPigLoader() {
    LOG.info("LzoProtobufBlockLoader zero-parameter creation");
  }

  public LzoProtobufBlockPigLoader(String protoClassName) {
    TypeRef<M> typeRef = PigUtil.getProtobufTypeRef(protoClassName);
    setTypeRef(typeRef);
    setLoaderSpec(getClass(), new String[]{protoClassName});
  }

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called before getNext!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    value_ = new ProtobufWritable<M>(typeRef_);
    String group = "LzoBlocks of " + typeRef_.getRawClass().getName();
    protobufsRead = new Pair<String, String>(group, "Protobufs Read");
    protobufErrors = new Pair<String, String>(group, "Errors");
  }

  @Override
  public void postBind() throws IOException {
    reader_ = new ProtobufBlockReader<M>(is_, typeRef_);
  }

  @Override
  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // We want to explicitly not do any special syncing here, because the reader_
    // handles this automatically.
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }


  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    // If we are past the end of the file split, tell the reader not to read any more new blocks.
    // Then continue reading until the last of the reader's already-parsed values are used up.
    // The next split will start at the next sync point and no records will be missed.
    if (is_.getPosition() > end_) {
      reader_.markNoMoreNewBlocks();
    }

    Tuple t = null;
    if (reader_.readProtobuf(value_)) {
      if (value_.get() == null) {
        incrCounter(protobufErrors, 1);
      }
      t = new ProtobufTuple(value_.get());
      incrCounter(protobufsRead, 1L);
    }
    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.nio.charset.Charset;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.Pair;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.base.Function;
import com.google.protobuf.Message;
import com.twitter.elephantbird.mapreduce.io.ProtobufConverter;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * This is the base class for all base64 encoded, line-oriented protocol buffer based pig loaders.
 * Data is expected to be one base64 encoded serialized protocol buffer per line. The specific
 * protocol buffer is a template parameter, generally specified by a codegen'd derived class.
 * See com.twitter.elephantbird.proto.HadoopProtoCodeGenerator.
 */

public class LzoProtobufB64LinePigLoader<M extends Message> extends LzoBaseLoadFunc {
  private static final Logger LOG = LoggerFactory.getLogger(LzoProtobufB64LinePigLoader.class);

  private TypeRef<M> typeRef_ = null;
  private ProtobufConverter<M> protoConverter_ = null;
  private final Base64 base64_ = new Base64();
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  private static final Charset UTF8 = Charset.forName("UTF-8");
  private static final byte RECORD_DELIMITER = (byte)'\n';

  private Pair<String, String> linesRead;
  private Pair<String, String> protobufsRead;
  private Pair<String, String> protobufErrors;

  public LzoProtobufB64LinePigLoader() {
    LOG.info("LzoProtobufB64LineLoader zero-parameter creation");
  }

  public LzoProtobufB64LinePigLoader(String protoClassName) {
    TypeRef<M> typeRef = Protobufs.getTypeRef(protoClassName);
    setTypeRef(typeRef);
    setLoaderSpec(getClass(), new String[]{protoClassName});
  }

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called before getNext!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    protoConverter_ = ProtobufConverter.newInstance(typeRef);
    String group = "LzoB64Lines of " + typeRef_.getRawClass().getName();
    linesRead = new Pair<String, String>(group, "Lines Read");
    protobufsRead = new Pair<String, String>(group, "Protobufs Read");
    protobufErrors = new Pair<String, String>(group, "Errors");
  }

  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // Since we are not block aligned we throw away the first record of each split and count on a different
    // instance to read it.  The only split this doesn't work for is the first.
    if (!atFirstRecord) {
      getNext();
    }
  }

  @Override
  protected boolean verifyStream() throws IOException {
    return is_ != null;
  }

  /**
   * Return every non-null line as a single-element tuple to Pig.
   */
  public Tuple getNext() throws IOException {
    if (!verifyStream()) {
      return null;
    }

    String line;
    Tuple t = null;
    while ((line = is_.readLine(UTF8, RECORD_DELIMITER)) != null) {
      incrCounter(linesRead, 1L);
      M protoValue = protoConverter_.fromBytes(base64_.decode(line.getBytes("UTF-8")));
      if (protoValue != null) {
        t = new ProtobufTuple(protoValue);
        incrCounter(protobufsRead, 1L);
        break;
      } else {
        incrCounter(protobufErrors, 1L);
      }
    }

    return t;
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}=======
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import org.apache.pig.ExecType;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.Message;
import com.twitter.elephantbird.mapreduce.io.BinaryConverter;
import com.twitter.elephantbird.mapreduce.io.ProtobufConverter;
import com.twitter.elephantbird.pig.util.PigUtil;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * This is the class for all base64 encoded, line-oriented protocol buffer based pig loaders.
 * Data is expected to be one base64 encoded serialized protocol buffer per line. The specific
 * protocol buffer is a template parameter, generally specified by a codegen'd derived class.
 * See com.twitter.elephantbird.proto.HadoopProtoCodeGenerator.
 */

public class LzoProtobufB64LinePigLoader<M extends Message> extends LzoBinaryB64LinePigLoader {
  private static final Logger LOG = LoggerFactory.getLogger(LzoProtobufB64LinePigLoader.class);

  private TypeRef<M> typeRef_ = null;
  private ProtobufConverter<M> protoConverter_ = null;

  public LzoProtobufB64LinePigLoader() {
    LOG.info("LzoProtobufB64LineLoader zero-parameter creation");
  }

  public LzoProtobufB64LinePigLoader(String protoClassName) {
    TypeRef<M> typeRef = PigUtil.getProtobufTypeRef(protoClassName);
    setTypeRef(typeRef);
    setLoaderSpec(getClass(), new String[]{protoClassName});
  }

  /**
   * Set the type parameter so it doesn't get erased by Java.  Must be called before getNext!
   *
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    protoConverter_ = ProtobufConverter.newInstance(typeRef);
    BinaryConverter<Tuple> converter = new BinaryConverter<Tuple>() {
      public Tuple fromBytes(byte[] messageBuffer) {
        Message message = protoConverter_.fromBytes(messageBuffer);
        if (message != null) {
          return new ProtobufTuple(message);
        }
        return null;
      }

      public byte[] toBytes(Tuple message) {
        throw new RuntimeException("not implemented");
      }
    };

    init(typeRef_.getRawClass().getName(), converter);
  }

  @Override
  public Schema determineSchema(String filename, ExecType execType, DataStorage store) throws IOException {
    return new ProtobufToPig().toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/pig/piggybank/ThriftBytesToTuple.java;<<<<<<< MINE
import com.twitter.elephantbird.pig8.util.ThriftToPig;
import com.twitter.elephantbird.util.ThriftUtils;
||||||| BASE
import com.twitter.elephantbird.pig.util.ThriftToPig;
import com.twitter.elephantbird.util.ThriftUtils;
=======
import com.twitter.elephantbird.pig.util.PigUtil;
import com.twitter.elephantbird.pig.util.ThriftToPig;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/java/com/twitter/elephantbird/pig/piggybank/ProtobufBytesToTuple.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.piggybank;

import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;

import com.google.protobuf.Message;
import com.twitter.elephantbird.mapreduce.io.ProtobufConverter;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * The base class for a Pig UDF that takes as input a tuple containing a single element, the
 * bytes of a serialized protocol buffer as a DataByteArray.  It outputs the protobuf in
 * expanded form.  The specific protocol buffer is a template parameter, generally specified by a
 * codegen'd derived class. See com.twitter.elephantbird.proto.HadoopProtoCodeGenerator.
 * Alternatly, full class name could be passed to the constructor in Pig:
 * <pre>
 *   DEFINE PersonProtobufBytesToTuple com.twitter.elephantbird.pig.piggybank.ProtobufBytesToTuple('com.twitter.elephantbird.proto.Person');
 *   persons = FOREACH protobufs GENERATE PersonProtobufBytesToTuple($0);
 * </pre>
 */
public class ProtobufBytesToTuple<M extends Message> extends EvalFunc<Tuple> {
  private TypeRef<M> typeRef_ = null;
  private ProtobufConverter<M> protoConverter_ = null;
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  public ProtobufBytesToTuple() {}

  public ProtobufBytesToTuple(String protoClassName) {
    TypeRef<M> typeRef = Protobufs.getTypeRef(protoClassName);
    setTypeRef(typeRef);
  }

  /**
   * Set the type parameter so it doesn't get erased by Java. Must be called during
   * initialization.
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    protoConverter_ = ProtobufConverter.newInstance(typeRef);
  }

  @Override
  public Tuple exec(Tuple input) throws IOException {
    if (input == null || input.size() < 1) return null;
    try {
      DataByteArray bytes = (DataByteArray) input.get(0);
      M value_ = protoConverter_.fromBytes(bytes.get());
      return new ProtobufTuple(value_);
    } catch (IOException e) {
      return null;
    }
  }

  @Override
  public Schema outputSchema(Schema input) {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}=======
package com.twitter.elephantbird.pig.piggybank;

import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;

import com.google.protobuf.Message;
import com.twitter.elephantbird.mapreduce.io.ProtobufConverter;
import com.twitter.elephantbird.pig.util.PigUtil;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
import com.twitter.elephantbird.pig.util.ProtobufTuple;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * The base class for a Pig UDF that takes as input a tuple containing a single element, the
 * bytes of a serialized protocol buffer as a DataByteArray.  It outputs the protobuf in
 * expanded form.  The specific protocol buffer is a template parameter, generally specified by a
 * codegen'd derived class. See com.twitter.elephantbird.proto.HadoopProtoCodeGenerator.
 * Alternatly, full class name could be passed to the constructor in Pig:
 * <pre>
 *   DEFINE PersonProtobufBytesToTuple com.twitter.elephantbird.pig.piggybank.ProtobufBytesToTuple('com.twitter.elephantbird.proto.Person');
 *   persons = FOREACH protobufs GENERATE PersonProtobufBytesToTuple($0);
 * </pre>
 */
public class ProtobufBytesToTuple<M extends Message> extends EvalFunc<Tuple> {
  private TypeRef<M> typeRef_ = null;
  private ProtobufConverter<M> protoConverter_ = null;
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();

  public ProtobufBytesToTuple() {}

  public ProtobufBytesToTuple(String protoClassName) {
    TypeRef<M> typeRef = PigUtil.getProtobufTypeRef(protoClassName);
    setTypeRef(typeRef);
  }

  /**
   * Set the type parameter so it doesn't get erased by Java. Must be called during
   * initialization.
   * @param typeRef
   */
  public void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    protoConverter_ = ProtobufConverter.newInstance(typeRef);
  }

  @Override
  public Tuple exec(Tuple input) throws IOException {
    if (input == null || input.size() < 1) return null;
    try {
      DataByteArray bytes = (DataByteArray) input.get(0);
      M value_ = protoConverter_.fromBytes(bytes.get());
      return new ProtobufTuple(value_);
    } catch (IOException e) {
      return null;
    }
  }

  @Override
  public Schema outputSchema(Schema input) {
    return protoToPig_.toSchema(Protobufs.getMessageDescriptor(typeRef_.getRawClass()));
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_27ceb0c_4353485/rev_27ceb0c-4353485/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;<<<<<<< MINE
import com.twitter.elephantbird.pig8.util.ThriftToPig;
||||||| BASE
import com.twitter.elephantbird.pig.util.ThriftToPig;
=======
import com.twitter.elephantbird.pig.util.PigToThrift;
import com.twitter.elephantbird.pig.util.ThriftToPig;
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_a64b868_6e11c79/rev_a64b868-6e11c79/ribbon-core/src/main/java/com/netflix/client/AbstractLoadBalancerAwareClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_a64b868_6e11c79/rev_a64b868-6e11c79/ribbon-core/src/main/java/com/netflix/client/AbstractLoadBalancerAwareClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_a64b868_6e11c79/rev_a64b868-6e11c79/ribbon-core/src/main/java/com/netflix/client/AbstractLoadBalancerAwareClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_a64b868_6e11c79/rev_a64b868-6e11c79/ribbon-core/src/main/java/com/netflix/client/AbstractLoadBalancerAwareClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_a64b868_6e11c79/rev_a64b868-6e11c79/ribbon-httpclient/src/main/java/com/netflix/niws/client/http/RestClient.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/main/java/retrofit/http/HttpProfiler.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/main/java/retrofit/http/Fetcher.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/main/java/retrofit/http/Fetcher.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/main/java/retrofit/http/RestAdapter.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/main/java/retrofit/http/RestAdapter.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/main/java/retrofit/http/RestAdapter.java;<<<<<<< MINE
import javax.inject.Inject;
import javax.inject.Provider;
import javax.inject.Singleton;
import org.apache.http.Header;
import org.apache.http.HttpEntity;
import org.apache.http.HttpResponse;
import org.apache.http.client.HttpClient;
import org.apache.http.client.ResponseHandler;
import org.apache.http.client.methods.HttpEntityEnclosingRequestBase;
import org.apache.http.client.methods.HttpUriRequest;
import retrofit.core.Callback;
import retrofit.core.MainThread;
||||||| BASE
=======
import javax.inject.Inject;
import javax.inject.Provider;
import javax.inject.Singleton;
import org.apache.http.HttpEntity;
import org.apache.http.HttpResponse;
import org.apache.http.client.HttpClient;
import org.apache.http.client.ResponseHandler;
import org.apache.http.client.methods.HttpEntityEnclosingRequestBase;
import org.apache.http.client.methods.HttpUriRequest;
import retrofit.core.Callback;
import retrofit.core.MainThread;
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/main/java/retrofit/http/RestAdapter.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/main/java/retrofit/http/RestAdapter.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/main/java/retrofit/http/RestAdapter.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/main/java/retrofit/http/RestAdapter.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/main/java/retrofit/http/RestAdapter.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/main/java/retrofit/http/RestAdapter.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/test/java/retrofit/http/FetcherTest.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/test/java/retrofit/http/FetcherTest.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/test/java/retrofit/http/RestAdapterTest.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/test/java/retrofit/http/RestAdapterTest.java;null
/home/taes/taes/projects/retrofit/revisions/rev_35b7257_838711b/rev_35b7257-838711b/http/src/test/java/retrofit/http/RestAdapterTest.java;null
/home/taes/taes/projects/atlas/revisions/rev_9749e85_ff2918c/rev_9749e85-ff2918c/atlas-demo/AtlasDemo/buildSrc/src/main/java/com/taobao/android/builder/extension/AtlasExtension.java;<<<<<<< MINE

||||||| BASE
=======
import com.taobao.android.builder.extension.factory.MultiDexConfigFactory;
>>>>>>> YOURS
/home/taes/taes/projects/atlas/revisions/rev_9749e85_ff2918c/rev_9749e85-ff2918c/atlas-demo/AtlasDemo/buildSrc/src/main/java/com/taobao/android/builder/extension/AtlasExtension.java;<<<<<<< MINE

    public Logger getLogger() {
        return logger;
    }

    public Project getProject() {
        return project;
    }
||||||| BASE
=======

    public NamedDomainObjectContainer<MultiDexConfig> getMultiDexConfigs() {
        return multiDexConfigs;
    }

    public void setMultiDexConfigs(
        NamedDomainObjectContainer<MultiDexConfig> multiDexConfigs) {
        this.multiDexConfigs = multiDexConfigs;
    }
>>>>>>> YOURS
/home/taes/taes/projects/atlas/revisions/rev_9749e85_ff2918c/rev_9749e85-ff2918c/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/extension/AtlasExtension.java;<<<<<<< MINE

||||||| BASE
=======
import com.taobao.android.builder.extension.factory.MultiDexConfigFactory;
>>>>>>> YOURS
/home/taes/taes/projects/atlas/revisions/rev_9749e85_ff2918c/rev_9749e85-ff2918c/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/extension/AtlasExtension.java;<<<<<<< MINE

    public Logger getLogger() {
        return logger;
    }

    public Project getProject() {
        return project;
    }
||||||| BASE
=======

    public NamedDomainObjectContainer<MultiDexConfig> getMultiDexConfigs() {
        return multiDexConfigs;
    }

    public void setMultiDexConfigs(
        NamedDomainObjectContainer<MultiDexConfig> multiDexConfigs) {
        this.multiDexConfigs = multiDexConfigs;
    }
>>>>>>> YOURS
/home/taes/taes/projects/archaius/revisions/rev_07ee5dc_bfb68e9/rev_07ee5dc-bfb68e9/archaius-core/src/main/java/com/netflix/config/DynamicSetProperty.java;<<<<<<< MINE
        
||||||| BASE

    public static final String DEFAULT_DELIMITER = ",";
        
=======

    public static final String DEFAULT_DELIMITER = ",";

>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_ed6d50d_1f0c02e/rev_ed6d50d-1f0c02e/src/java/com/twitter/elephantbird/util/Codecs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ed6d50d_1f0c02e/rev_ed6d50d-1f0c02e/src/java/com/twitter/elephantbird/util/Codecs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ed6d50d_1f0c02e/rev_ed6d50d-1f0c02e/src/java/com/twitter/elephantbird/util/Codecs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ed6d50d_1f0c02e/rev_ed6d50d-1f0c02e/src/java/com/twitter/elephantbird/util/Codecs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ed6d50d_1f0c02e/rev_ed6d50d-1f0c02e/src/java/com/twitter/elephantbird/pig/store/LzoProtobufB64LinePigStorage.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_d48ed68_da570d2/rev_d48ed68-da570d2/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufBlockPigletGenerator.java;<<<<<<< MINE
import com.twitter.elephantbird.pig8.util.ProtobufToPig;
||||||| BASE
import com.twitter.elephantbird.pig.util.ProtobufToPig;
=======
import com.twitter.elephantbird.pig.load.LzoProtobufBlockPigLoader;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_d48ed68_da570d2/rev_d48ed68-da570d2/src/java/com/twitter/elephantbird/proto/codegen/LzoProtobufB64LinePigletGenerator.java;<<<<<<< MINE
import com.twitter.elephantbird.pig8.util.ProtobufToPig;
||||||| BASE
import com.twitter.elephantbird.pig.util.ProtobufToPig;
=======
import com.twitter.elephantbird.pig.load.LzoProtobufB64LinePigLoader;
import com.twitter.elephantbird.pig.util.ProtobufToPig;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_d48ed68_da570d2/rev_d48ed68-da570d2/src/java/com/twitter/elephantbird/pig/store/LzoProtobufB64LinePigStorage.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.store;

import java.io.IOException;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.data.Tuple;
import org.omg.IOP.Codec;

import com.google.protobuf.Message;
import com.twitter.elephantbird.pig.util.PigToProtobuf;
import com.twitter.elephantbird.pig.util.PigUtil;
import com.twitter.elephantbird.util.Codecs;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * Serializes Pig Tuples into Base-64 encoded, line-delimited protocol buffers.
 * The fields in the pig tuple must correspond exactly to the fields in the protobuf, as
 * no name-matching is performed (names of the tuple fields are not currently accessible to
 * a StoreFunc. It will be in 0.7, so something more flexible will be possible)
 *
 * @param <M> Protocol Buffer Message class being serialized
 */
public class LzoProtobufB64LinePigStorage<M extends Message> extends LzoBaseStoreFunc {

  private TypeRef<M> typeRef_;
  private Base64 base64_ = Codecs.createStandardBase64();
  private Message msgObj; // for newBuilder()

  protected LzoProtobufB64LinePigStorage(){}

  public LzoProtobufB64LinePigStorage(String protoClassName) {
    TypeRef<M> typeRef = PigUtil.getProtobufTypeRef(protoClassName);
    setTypeRef(typeRef);
  }

  protected void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    msgObj =  Protobufs.getMessageBuilder(typeRef_.getRawClass()).build();
  }

  public void putNext(Tuple f) throws IOException {
    if (f == null) return;
    Message message = PigToProtobuf.tupleToMessage(msgObj.newBuilderForType(), f);
    os_.write(base64_.encode(message.toByteArray()));
    os_.write(Protobufs.NEWLINE_UTF8_BYTE);
  }

}=======
package com.twitter.elephantbird.pig.store;

import java.io.IOException;

import org.apache.commons.codec.binary.Base64;
import org.apache.pig.data.Tuple;

import com.google.protobuf.Message;
import com.twitter.elephantbird.pig.util.PigToProtobuf;
import com.twitter.elephantbird.pig.util.PigUtil;
import com.twitter.elephantbird.util.Codecs;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * Serializes Pig Tuples into Base-64 encoded, line-delimited protocol buffers.
 * The fields in the pig tuple must correspond exactly to the fields in the protobuf, as
 * no name-matching is performed (names of the tuple fields are not currently accessible to
 * a StoreFunc. It will be in 0.7, so something more flexible will be possible)
 *
 * @param <M> Protocol Buffer Message class being serialized
 */
public class LzoProtobufB64LinePigStorage<M extends Message> extends LzoBaseStoreFunc {

  private TypeRef<M> typeRef_;
  private Base64 base64_ = Codecs.createStandardBase64();
  private Message msgObj; // for newBuilder()

  protected LzoProtobufB64LinePigStorage(){}

  public LzoProtobufB64LinePigStorage(String protoClassName) {
    TypeRef<M> typeRef = PigUtil.getProtobufTypeRef(protoClassName);
    setTypeRef(typeRef);
  }

  protected void setTypeRef(TypeRef<M> typeRef) {
    typeRef_ = typeRef;
    msgObj =  Protobufs.getMessageBuilder(typeRef_.getRawClass()).build();
  }

  public void putNext(Tuple f) throws IOException {
    if (f == null) return;
    Message message = PigToProtobuf.tupleToMessage(msgObj.newBuilderForType(), f);
    os_.write(base64_.encode(message.toByteArray()));
    os_.write(Protobufs.NEWLINE_UTF8_BYTE);
  }

}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_d48ed68_da570d2/rev_d48ed68-da570d2/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.util;

import java.nio.ByteBuffer;
import java.util.Arrays;
import java.util.Collection;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;

import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.pig.LoadFunc;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.apache.thrift.TBase;
import org.apache.thrift.protocol.TType;

import com.google.common.collect.Lists;
import com.twitter.elephantbird.pig.load.LzoThriftB64LinePigLoader;
import com.twitter.elephantbird.thrift.TStructDescriptor;
import com.twitter.elephantbird.thrift.TStructDescriptor.Field;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;

/**
 * <li> converts a Thrift struct to a Pig tuple
 * <li> utilities to provide schema for Pig loaders and Pig scripts
 */
public class ThriftToPig<M extends TBase<?, ?>> {
  public static final Logger LOG = LogManager.getLogger(ThriftToPig.class);

  private static BagFactory bagFactory = BagFactory.getInstance();
  private static TupleFactory tupleFactory  = TupleFactory.getInstance();

  private TStructDescriptor structDesc;

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(Class<M> tClass) {
    return new ThriftToPig<M>(tClass);
  }

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(TypeRef<M> typeRef) {
    return new ThriftToPig<M>(typeRef.getRawClass());
  }

  public ThriftToPig(Class<M> tClass) {
    structDesc = TStructDescriptor.getInstance(tClass);
  }

  /**
   * Converts a thrift object to Pig tuple.
   * All the fields are deserialized.
   * It might be better to use getLazyTuple() if not all fields
   * are required.
   */
  public Tuple getPigTuple(M thriftObj) {
    return toTuple(structDesc, thriftObj);
  }

  /**
   * Similar to {@link #getPigTuple(TBase)}. This delays
   * serialization of tuple contents until they are requested.
   * @param thriftObj
   * @return
   */
  public Tuple getLazyTuple(M thriftObj) {
    return new LazyTuple(thriftObj);
  }

  @SuppressWarnings("unchecked")
  private static <T extends TBase>Tuple toTuple(TStructDescriptor tDesc, T tObj) {
    int size = tDesc.getFields().size();
    Tuple tuple = tupleFactory.newTuple(size);
    for (int i=0; i<size; i++) {
      Field field = tDesc.getFieldAt(i);
      Object value = tDesc.getFieldValue(i, tObj);
      try {
        tuple.set(i, toPigObject(field, value));
      } catch (ExecException e) { // not expected
        throw new RuntimeException(e);
      }
    }
    return tuple;
  }

  @SuppressWarnings("unchecked")
  private static Object toPigObject(Field field, Object value) {
    if (value == null) {
      return null;
    }

    switch (field.getType()) {
    case TType.BOOL:
      return Integer.valueOf((Boolean)value ? 1 : 0);
    case TType.BYTE :
      return Integer.valueOf((Byte)value);
    case TType.I16 :
      return Integer.valueOf((Short)value);
    case TType.STRING:
      return stringTypeToPig(value);
    case TType.STRUCT:
      return toTuple(field.gettStructDescriptor(), (TBase<?, ?>)value);
    case TType.MAP:
      return toPigMap(field, (Map<Object, Object>)value);
    case TType.SET:
      return toPigBag(field.getSetElemField(), (Collection<Object>)value);
    case TType.LIST:
      return toPigBag(field.getListElemField(), (Collection<Object>)value);
    case TType.ENUM:
      return value.toString();
    default:
      // standard types : I32, I64, DOUBLE, etc.
      return value;
    }
  }

  /**
   * TType.STRING is a mess in Thrift. It could be byte[], ByteBuffer,
   * or even a String!.
   */
  private static Object stringTypeToPig(Object value) {
    if (value instanceof String) {
      return value;
    }
    if (value instanceof byte[]) {
      byte[] buf = (byte[])value;
      return new DataByteArray(Arrays.copyOf(buf, buf.length));
    }
    if (value instanceof ByteBuffer) {
      ByteBuffer bin = (ByteBuffer)value;
      byte[] buf = new byte[bin.remaining()];
      bin.mark();
      bin.get(buf);
      bin.reset();
      return new DataByteArray(buf);
    }
    return null;
  }

  private static Map<String, Object> toPigMap(Field field, Map<Object, Object> map) {
    // PIG map's key always a String. just use toString() and hope
    // things would work out ok.
    HashMap<String, Object> out = new HashMap<String, Object>(map.size());
    Field valueField = field.getMapValueField();
    for(Entry<Object, Object> e : map.entrySet()) {
      Object prev = out.put(e.getKey().toString(),
                            toPigObject(valueField, e.getValue()));
      if (prev != null) {
        String msg = "Duplicate keys while converting to String while "
          + " processing map " + field.getName() + " (key type : "
          + field.getMapKeyField().getType() + " value type : "
          + field.getMapValueField().getType() + ")";
        LOG.warn(msg);
        throw new RuntimeException(msg);
      }
    }
    return out;
  }

  private static DataBag toPigBag(Field field, Collection<Object> values) {
    List<Tuple> tuples = Lists.newArrayListWithExpectedSize(values.size());
    for(Object value : values) {
      Object pValue = toPigObject(field, value);
      if (pValue instanceof Tuple) { // DataBag should contain Tuples
        tuples.add((Tuple)pValue);
      } else {
        tuples.add(tupleFactory.newTuple(pValue));
      }
    }
    return bagFactory.newDefaultBag(tuples);
  }

  @SuppressWarnings("serial")
  /**
   * Delays serialization of Thrift fields until they are requested.
   */
  private class LazyTuple extends AbstractLazyTuple {
    /* NOTE : This is only a partial optimization. The other part
     * is to avoid deserialization of the Thrift fields from the
     * binary buffer.
     *
     * Currently TDeserializer allows deserializing just one field,
     * psuedo-skipping over the fields before it.
     * But if we are going deserialize 5 fields out of 20, we will be
     * skipping over same set of fields multiple times. OTOH this might
     * still be better than a full deserialization.
     *
     * We need to write our own version of TBinaryProtocol that truly skips.
     * Even TDeserializer 'skips'/ignores only after deserializing fields.
     * (e.g. Strings, Integers, buffers etc).
     */
    private M tObject;

    LazyTuple(M tObject) {
      initRealTuple(structDesc.getFields().size());
      this.tObject = tObject;
    }

    @Override
    protected Object getObjectAt(int index) {
      Field field = structDesc.getFieldAt(index);
      return toPigObject(field, structDesc.getFieldValue(index, tObject));
    }
  }

  /**
   * Returns Pig schema for the Thrift struct.
   */
  public static Schema toSchema(Class<? extends TBase<?, ?>> tClass) {
    return toSchema(TStructDescriptor.getInstance(tClass));
  }
  public static Schema toSchema(TStructDescriptor tDesc ) {
    Schema schema = new Schema();

    try {
      for(Field field : tDesc.getFields()) {
        String fieldName = field.getName();
        if (field.isStruct()) {
          schema.add(new FieldSchema(fieldName, toSchema(field.gettStructDescriptor()), DataType.TUPLE));
        } else {
          schema.add(singleFieldToFieldSchema(fieldName, field));
        }
      }
    } catch (FrontendException t) {
      throw new RuntimeException(t);
    }

    return schema;
  }

  private static FieldSchema singleFieldToFieldSchema(String fieldName, Field field) throws FrontendException {
    switch (field.getType()) {
      case TType.LIST:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", field.getListElemField()), DataType.BAG);
      case TType.SET:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", field.getSetElemField()), DataType.BAG);
      case TType.MAP:
        // can not specify types for maps in Pig.
        if (field.getMapKeyField().getType() != TType.STRING) {
          LOG.warn("Using a map with non-string key for field " + field.getName()
              + ". while converting to PIG Tuple, toString() is used for the key."
              + " It could result in incorrect maps.");
        }
        return new FieldSchema(fieldName, null, DataType.MAP);
      default:
        return new FieldSchema(fieldName, null, getPigDataType(field));
    }
  }

  /**
   * Returns a schema with single tuple (for Pig bags).
   */
  private static Schema singleFieldToTupleSchema(String fieldName, Field field) throws FrontendException {

    FieldSchema fieldSchema = null;

    switch (field.getType()) {
      case TType.STRUCT:
        fieldSchema = new FieldSchema(fieldName, toSchema(field.gettStructDescriptor()), DataType.TUPLE);
        break;
      case TType.LIST:
        fieldSchema = singleFieldToFieldSchema(fieldName, field.getListElemField());
        break;
      case TType.SET:
        fieldSchema = singleFieldToFieldSchema(fieldName, field.getSetElemField());
        break;
      default:
        fieldSchema = new FieldSchema(fieldName, null, getPigDataType(field));
    }

    Schema schema = new Schema();
    schema.add(fieldSchema);
    return schema;
  }

  private static byte getPigDataType(Field field) {
    switch (field.getType()) {
      case TType.BOOL:
      case TType.BYTE:
      case TType.I16:
      case TType.I32:
        return DataType.INTEGER;
      case TType.ENUM:
        return DataType.CHARARRAY;
      case TType.I64:
        return DataType.LONG;
      case TType.DOUBLE:
        return DataType.DOUBLE;
      case TType.STRING:
        return field.isBuffer() ? DataType.BYTEARRAY : DataType.CHARARRAY;
      default:
        throw new IllegalArgumentException("Unexpected type where a simple type is expected : " + field.getType());
    }
  }

  /**
   * Turn a Thrift Struct into a loading schema for a pig script.
   */
  public static String toPigScript(Class<? extends TBase<?, ?>> thriftClass,
                                   Class<? extends LoadFunc> pigLoader) {
    StringBuilder sb = new StringBuilder();
    /* we are commenting out explicit schema specification. The schema is
     * included mainly to help the readers of the pig script. Pig learns the
     * schema directly from the loader.
     * If explicit schema is not commented, we might have surprising results
     * when a Thrift class (possibly in control of another team) changes,
     * but the Pig script is not updated. Commenting it out avoids this.
     */
    StringBuilder prefix = new StringBuilder("       --  ");
    sb.append("raw_data = load '$INPUT_FILES' using ")
      .append(pigLoader.getName())
      .append("('")
      .append(thriftClass.getName())
      .append("');\n")
      .append(prefix)
      .append("as ");
    prefix.append("   ");

    try {
      stringifySchema(sb, toSchema(thriftClass), DataType.TUPLE, prefix);
    } catch (FrontendException e) {
      throw new RuntimeException(e);
    }

    sb.append("\n");
    return sb.toString();
  }

  /**
   * Print formatted schema. This is a modified version of
   * {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
   * with support for (indented) pretty printing.
   */
  // This is used for building up output string
  // type can only be BAG or TUPLE
  public static void stringifySchema(StringBuilder sb,
                                     Schema schema,
                                     byte type,
                                     StringBuilder prefix)
                                          throws FrontendException{
      // this is a modified version of {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
      if (type == DataType.TUPLE) {
          sb.append("(") ;
      }
      else if (type == DataType.BAG) {
          sb.append("{") ;
      }

      prefix.append("  ");
      sb.append("\n").append(prefix);

      if (schema == null) {
          sb.append("null") ;
      }
      else {
          boolean isFirst = true ;
          for (int i=0; i< schema.size() ;i++) {

              if (!isFirst) {
                  sb.append(",\n").append(prefix);
              }
              else {
                  isFirst = false ;
              }

              FieldSchema fs = schema.getField(i) ;

              if(fs == null) {
                  sb.append("null");
                  continue;
              }

              if (fs.alias != null) {
                  sb.append(fs.alias);
                  sb.append(": ");
              }

              if (DataType.isAtomic(fs.type)) {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
              else if ( (fs.type == DataType.TUPLE) ||
                        (fs.type == DataType.BAG) ) {
                  // safety net
                  if (schema != fs.schema) {
                      stringifySchema(sb, fs.schema, fs.type, prefix) ;
                  }
                  else {
                      throw new AssertionError("Schema refers to itself "
                                               + "as inner schema") ;
                  }
              } else if (fs.type == DataType.MAP) {
                  sb.append(DataType.findTypeName(fs.type) + "[ ]") ;
              } else {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
          }
      }

      prefix.setLength(prefix.length()-2);
      sb.append("\n").append(prefix);

      if (type == DataType.TUPLE) {
          sb.append(")") ;
      }
      else if (type == DataType.BAG) {
          sb.append("}") ;
      }
  }

  public static void main(String[] args) throws Exception {
    if (args.length > 0) {
      Class<? extends TBase<?, ?>> tClass = ThriftUtils.getTypeRef(args[0]).getRawClass();
      System.out.println(args[0] + " : " + toSchema(tClass).toString());
      System.out.println(toPigScript(tClass, LzoThriftB64LinePigLoader.class));
    }
  }
}=======
package com.twitter.elephantbird.pig.util;

import java.nio.ByteBuffer;
import java.util.Arrays;
import java.util.Collection;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;

import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.pig.LoadFunc;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.apache.thrift.TBase;
import org.apache.thrift.protocol.TType;

import com.google.common.collect.Lists;
import com.twitter.elephantbird.pig.load.LzoThriftB64LinePigLoader;
import com.twitter.elephantbird.thrift.TStructDescriptor;
import com.twitter.elephantbird.thrift.TStructDescriptor.Field;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;

/**
 * <li> converts a Thrift struct to a Pig tuple
 * <li> utilities to provide schema for Pig loaders and Pig scripts
 */
public class ThriftToPig<M extends TBase<?, ?>> {
  public static final Logger LOG = LogManager.getLogger(ThriftToPig.class);

  private static BagFactory bagFactory = BagFactory.getInstance();
  private static TupleFactory tupleFactory  = TupleFactory.getInstance();

  private TStructDescriptor structDesc;

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(Class<M> tClass) {
    return new ThriftToPig<M>(tClass);
  }

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(TypeRef<M> typeRef) {
    return new ThriftToPig<M>(typeRef.getRawClass());
  }

  public ThriftToPig(Class<M> tClass) {
    structDesc = TStructDescriptor.getInstance(tClass);
  }

  /**
   * Converts a thrift object to Pig tuple.
   * All the fields are deserialized.
   * It might be better to use getLazyTuple() if not all fields
   * are required.
   */
  public Tuple getPigTuple(M thriftObj) {
    return toTuple(structDesc, thriftObj);
  }

  /**
   * Similar to {@link #getPigTuple(TBase)}. This delays
   * serialization of tuple contents until they are requested.
   * @param thriftObj
   * @return
   */
  public Tuple getLazyTuple(M thriftObj) {
    return new LazyTuple(thriftObj);
  }

  @SuppressWarnings("unchecked")
  private static <T extends TBase>Tuple toTuple(TStructDescriptor tDesc, T tObj) {
    int size = tDesc.getFields().size();
    Tuple tuple = tupleFactory.newTuple(size);
    for (int i=0; i<size; i++) {
      Field field = tDesc.getFieldAt(i);
      Object value = tDesc.getFieldValue(i, tObj);
      try {
        tuple.set(i, toPigObject(field, value));
      } catch (ExecException e) { // not expected
        throw new RuntimeException(e);
      }
    }
    return tuple;
  }

  @SuppressWarnings("unchecked")
  private static Object toPigObject(Field field, Object value) {
    if (value == null) {
      return null;
    }

    switch (field.getType()) {
    case TType.BOOL:
      return Integer.valueOf((Boolean)value ? 1 : 0);
    case TType.BYTE :
      return Integer.valueOf((Byte)value);
    case TType.I16 :
      return Integer.valueOf((Short)value);
    case TType.STRING:
      return stringTypeToPig(value);
    case TType.STRUCT:
      return toTuple(field.gettStructDescriptor(), (TBase<?, ?>)value);
    case TType.MAP:
      return toPigMap(field, (Map<Object, Object>)value);
    case TType.SET:
      return toPigBag(field.getSetElemField(), (Collection<Object>)value);
    case TType.LIST:
      return toPigBag(field.getListElemField(), (Collection<Object>)value);
    case TType.ENUM:
      return value.toString();
    default:
      // standard types : I32, I64, DOUBLE, etc.
      return value;
    }
  }

  /**
   * TType.STRING is a mess in Thrift. It could be byte[], ByteBuffer,
   * or even a String!.
   */
  private static Object stringTypeToPig(Object value) {
    if (value instanceof String) {
      return value;
    }
    if (value instanceof byte[]) {
      byte[] buf = (byte[])value;
      return new DataByteArray(Arrays.copyOf(buf, buf.length));
    }
    if (value instanceof ByteBuffer) {
      ByteBuffer bin = (ByteBuffer)value;
      byte[] buf = new byte[bin.remaining()];
      bin.mark();
      bin.get(buf);
      bin.reset();
      return new DataByteArray(buf);
    }
    return null;
  }

  private static Map<String, Object> toPigMap(Field field, Map<Object, Object> map) {
    // PIG map's key always a String. just use toString() and hope
    // things would work out ok.
    HashMap<String, Object> out = new HashMap<String, Object>(map.size());
    Field valueField = field.getMapValueField();
    for(Entry<Object, Object> e : map.entrySet()) {
      Object prev = out.put(e.getKey().toString(),
                            toPigObject(valueField, e.getValue()));
      if (prev != null) {
        String msg = "Duplicate keys while converting to String while "
          + " processing map " + field.getName() + " (key type : "
          + field.getMapKeyField().getType() + " value type : "
          + field.getMapValueField().getType() + ")";
        LOG.warn(msg);
        throw new RuntimeException(msg);
      }
    }
    return out;
  }

  private static DataBag toPigBag(Field field, Collection<Object> values) {
    List<Tuple> tuples = Lists.newArrayListWithExpectedSize(values.size());
    for(Object value : values) {
      Object pValue = toPigObject(field, value);
      if (pValue instanceof Tuple) { // DataBag should contain Tuples
        tuples.add((Tuple)pValue);
      } else {
        tuples.add(tupleFactory.newTuple(pValue));
      }
    }
    return bagFactory.newDefaultBag(tuples);
  }

  @SuppressWarnings("serial")
  /**
   * Delays serialization of Thrift fields until they are requested.
   */
  private class LazyTuple extends AbstractLazyTuple {
    /* NOTE : This is only a partial optimization. The other part
     * is to avoid deserialization of the Thrift fields from the
     * binary buffer.
     *
     * Currently TDeserializer allows deserializing just one field,
     * psuedo-skipping over the fields before it.
     * But if we are going deserialize 5 fields out of 20, we will be
     * skipping over same set of fields multiple times. OTOH this might
     * still be better than a full deserialization.
     *
     * We need to write our own version of TBinaryProtocol that truly skips.
     * Even TDeserializer 'skips'/ignores only after deserializing fields.
     * (e.g. Strings, Integers, buffers etc).
     */
    private M tObject;

    LazyTuple(M tObject) {
      initRealTuple(structDesc.getFields().size());
      this.tObject = tObject;
    }

    @Override
    protected Object getObjectAt(int index) {
      Field field = structDesc.getFieldAt(index);
      return toPigObject(field, structDesc.getFieldValue(index, tObject));
    }
  }

  /**
   * Returns Pig schema for the Thrift struct.
   */
  public static Schema toSchema(Class<? extends TBase<?, ?>> tClass) {
    return toSchema(TStructDescriptor.getInstance(tClass));
  }
  public static Schema toSchema(TStructDescriptor tDesc ) {
    Schema schema = new Schema();

    try {
      for(Field field : tDesc.getFields()) {
        String fieldName = field.getName();
        if (field.isStruct()) {
          schema.add(new FieldSchema(fieldName, toSchema(field.gettStructDescriptor()), DataType.TUPLE));
        } else {
          schema.add(singleFieldToFieldSchema(fieldName, field));
        }
      }
    } catch (FrontendException t) {
      throw new RuntimeException(t);
    }

    return schema;
  }

  private static FieldSchema singleFieldToFieldSchema(String fieldName, Field field) throws FrontendException {
    switch (field.getType()) {
      case TType.LIST:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", field.getListElemField()), DataType.BAG);
      case TType.SET:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", field.getSetElemField()), DataType.BAG);
      case TType.MAP:
        // can not specify types for maps in Pig.
        if (field.getMapKeyField().getType() != TType.STRING) {
          LOG.warn("Using a map with non-string key for field " + field.getName()
              + ". while converting to PIG Tuple, toString() is used for the key."
              + " It could result in incorrect maps.");
        }
        return new FieldSchema(fieldName, null, DataType.MAP);
      default:
        return new FieldSchema(fieldName, null, getPigDataType(field));
    }
  }

  /**
   * Returns a schema with single tuple (for Pig bags).
   */
  private static Schema singleFieldToTupleSchema(String fieldName, Field field) throws FrontendException {

    FieldSchema fieldSchema = null;

    switch (field.getType()) {
      case TType.STRUCT:
        // wrapping STRUCT in a FieldSchema makes it impossible to
        // access fields in PIG script (causes runtime error).
        return toSchema(field.gettStructDescriptor());
      case TType.LIST:
        fieldSchema = singleFieldToFieldSchema(fieldName, field.getListElemField());
        break;
      case TType.SET:
        fieldSchema = singleFieldToFieldSchema(fieldName, field.getSetElemField());
        break;
      default:
        fieldSchema = new FieldSchema(fieldName, null, getPigDataType(field));
    }

    Schema schema = new Schema();
    schema.add(fieldSchema);
    return schema;
  }

  private static byte getPigDataType(Field field) {
    switch (field.getType()) {
      case TType.BOOL:
      case TType.BYTE:
      case TType.I16:
      case TType.I32:
        return DataType.INTEGER;
      case TType.ENUM:
        return DataType.CHARARRAY;
      case TType.I64:
        return DataType.LONG;
      case TType.DOUBLE:
        return DataType.DOUBLE;
      case TType.STRING:
        return field.isBuffer() ? DataType.BYTEARRAY : DataType.CHARARRAY;
      default:
        throw new IllegalArgumentException("Unexpected type where a simple type is expected : " + field.getType());
    }
  }

  /**
   * Turn a Thrift Struct into a loading schema for a pig script.
   */
  public static String toPigScript(Class<? extends TBase<?, ?>> thriftClass,
                                   Class<? extends LoadFunc> pigLoader) {
    StringBuilder sb = new StringBuilder();
    /* we are commenting out explicit schema specification. The schema is
     * included mainly to help the readers of the pig script. Pig learns the
     * schema directly from the loader.
     * If explicit schema is not commented, we might have surprising results
     * when a Thrift class (possibly in control of another team) changes,
     * but the Pig script is not updated. Commenting it out avoids this.
     */
    StringBuilder prefix = new StringBuilder("       --  ");
    sb.append("raw_data = load '$INPUT_FILES' using ")
      .append(pigLoader.getName())
      .append("('")
      .append(thriftClass.getName())
      .append("');\n")
      .append(prefix)
      .append("as ");
    prefix.append("   ");

    try {
      stringifySchema(sb, toSchema(thriftClass), DataType.TUPLE, prefix);
    } catch (FrontendException e) {
      throw new RuntimeException(e);
    }

    sb.append("\n");
    return sb.toString();
  }

  /**
   * Print formatted schema. This is a modified version of
   * {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
   * with support for (indented) pretty printing.
   */
  // This is used for building up output string
  // type can only be BAG or TUPLE
  public static void stringifySchema(StringBuilder sb,
                                     Schema schema,
                                     byte type,
                                     StringBuilder prefix)
                                          throws FrontendException{
      // this is a modified version of {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
      if (type == DataType.TUPLE) {
          sb.append("(") ;
      }
      else if (type == DataType.BAG) {
          sb.append("{") ;
      }

      prefix.append("  ");
      sb.append("\n").append(prefix);

      if (schema == null) {
          sb.append("null") ;
      }
      else {
          boolean isFirst = true ;
          for (int i=0; i< schema.size() ;i++) {

              if (!isFirst) {
                  sb.append(",\n").append(prefix);
              }
              else {
                  isFirst = false ;
              }

              FieldSchema fs = schema.getField(i) ;

              if(fs == null) {
                  sb.append("null");
                  continue;
              }

              if (fs.alias != null) {
                  sb.append(fs.alias);
                  sb.append(": ");
              }

              if (DataType.isAtomic(fs.type)) {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
              else if ( (fs.type == DataType.TUPLE) ||
                        (fs.type == DataType.BAG) ) {
                  // safety net
                  if (schema != fs.schema) {
                      stringifySchema(sb, fs.schema, fs.type, prefix) ;
                  }
                  else {
                      throw new AssertionError("Schema refers to itself "
                                               + "as inner schema") ;
                  }
              } else if (fs.type == DataType.MAP) {
                  sb.append(DataType.findTypeName(fs.type) + "[ ]") ;
              } else {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
          }
      }

      prefix.setLength(prefix.length()-2);
      sb.append("\n").append(prefix);

      if (type == DataType.TUPLE) {
          sb.append(")") ;
      }
      else if (type == DataType.BAG) {
          sb.append("}") ;
      }
  }

  public static void main(String[] args) throws Exception {
    if (args.length > 0) {
      Class<? extends TBase<?, ?>> tClass = ThriftUtils.getTypeRef(args[0]).getRawClass();
      System.out.println(args[0] + " : " + toSchema(tClass).toString());
      System.out.println(toPigScript(tClass, LzoThriftB64LinePigLoader.class));
    }
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_d48ed68_da570d2/rev_d48ed68-da570d2/src/java/com/twitter/elephantbird/pig/util/ProtobufToPig.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.util;

import java.util.List;
import java.util.Map;

import com.google.common.collect.Lists;
import com.google.common.collect.Maps;
import com.google.protobuf.Descriptors.Descriptor;
import com.google.protobuf.Descriptors.EnumValueDescriptor;
import com.google.protobuf.Descriptors.FieldDescriptor;
import com.google.protobuf.ByteString;
import com.google.protobuf.Message;
import com.twitter.data.proto.Misc.CountedMap;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * A class for turning codegen'd protos into Pig Tuples and Schemas
 * for custom Pig LoadFuncs.
 * @author Kevin Weil
 */
public class ProtobufToPig {
  private static final Logger LOG = LoggerFactory.getLogger(ProtobufToPig.class);

  private static final TupleFactory tupleFactory_ = TupleFactory.getInstance();
  private static BagFactory bagFactory_ = BagFactory.getInstance();

  public enum CoercionLevel { kNoCoercion, kAllowCoercionToPigMaps }

  private final CoercionLevel coercionLevel_;

  public ProtobufToPig() {
    this(CoercionLevel.kAllowCoercionToPigMaps);
  }

  public ProtobufToPig(CoercionLevel coercionLevel) {
    coercionLevel_ = coercionLevel;
  }
  /**
   * Turn a generic message into a Tuple.  Individual fields that are enums
   * are converted into their string equivalents.  Fields that are not filled
   * out in the protobuf are set to null, unless there is a default field value in
   * which case that is used instead.
   * @param msg the protobuf message
   * @return a pig tuple representing the message.
   */
  public Tuple toTuple(Message msg) {
    if (msg == null) {
      // Pig tuples deal gracefully with nulls.
      // Also, we can be called with null here in recursive calls.
      return null;
    }

    Descriptor msgDescriptor = msg.getDescriptorForType();
    Tuple tuple = tupleFactory_.newTuple(msgDescriptor.getFields().size());
    int curField = 0;
    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        // Get the set value, or the default value, or null.
        Object fieldValue = getFieldValue(msg, fieldDescriptor);

        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          tuple.set(curField++, messageToTuple(fieldDescriptor, fieldValue));
        } else {
          tuple.set(curField++, singleFieldToTuple(fieldDescriptor, fieldValue));
        }
      }
    } catch (ExecException e) {
      LOG.warn("Could not convert msg " + msg + " to tuple", e);
    }

    return tuple;
  }

  /**
   * Translate a nested message to a tuple.  If the field is repeated, it walks the list and adds each to a bag.
   * Otherwise, it just adds the given one.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object representing fieldValue in Pig -- either a bag or a tuple.
   */
  @SuppressWarnings("unchecked")
  protected Object messageToTuple(FieldDescriptor fieldDescriptor, Object fieldValue) {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToTuple called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      // The protobuf contract is that if the field is repeated, then the object returned is actually a List
      // of the underlying datatype, which in this case is a nested message.
      List<Message> messageList = (List<Message>) (fieldValue != null ? fieldValue : Lists.newArrayList());

      // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
      // we can force the type into a pig map type.
      if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
          fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName())) {
        Map<Object, Long> map = Maps.newHashMap();
        for (Message m : messageList) {
          CountedMap cm = (CountedMap) m;
          final Long curCount = map.get(cm.getKey());
          map.put(cm.getKey(), (curCount == null ? 0L : curCount) + cm.getValue());
        }
        return map;
      } else {
        DataBag bag = bagFactory_.newDefaultBag();
        for (Message m : messageList) {
          bag.add(new ProtobufTuple(m));
        }
        return bag;
      }
    } else {
      return new ProtobufTuple((Message)fieldValue);
    }
  }

  /**
   * Translate a single field to a tuple.  If the field is repeated, it walks the list and adds each to a bag.
   * Otherwise, it just adds the given one.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object representing fieldValue in Pig -- either a bag or a single field.
   * @throws ExecException if Pig decides to.  Shouldn't happen because we won't walk off the end of a tuple's field set.
   */
  @SuppressWarnings("unchecked")
  protected Object singleFieldToTuple(FieldDescriptor fieldDescriptor, Object fieldValue) {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "messageToFieldSchema called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      // The protobuf contract is that if the field is repeated, then the object returned is actually a List
      // of the underlying datatype, which in this case is a "primitive" like int, float, String, etc.
      // We have to make a single-item tuple out of it to put it in the bag.
      DataBag bag = bagFactory_.newDefaultBag();
      List<Object> fieldValueList = (List<Object>) (fieldValue != null ? fieldValue : Lists.newArrayList());
      for (Object singleFieldValue : fieldValueList) {
        Object nonEnumFieldValue = coerceToPigTypes(fieldDescriptor, singleFieldValue);
        Tuple innerTuple = tupleFactory_.newTuple(1);
        try {
          innerTuple.set(0, nonEnumFieldValue);
        } catch (ExecException e) { // not expected
          throw new RuntimeException(e);
        }
        bag.add(innerTuple);
      }
      return bag;
    } else {
      return coerceToPigTypes(fieldDescriptor, fieldValue);
    }
  }

  /**
   * If the given field value is an enum, translate it to the enum's name as a string, since Pig cannot handle enums.
   * Also, if the given field value is a bool, translate it to 0 or 1 to avoid Pig bools, which can be sketchy.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object, unless it was from an enum field, in which case we return the name of the enum field.
   */
  private Object coerceToPigTypes(FieldDescriptor fieldDescriptor, Object fieldValue) {
    if (fieldDescriptor.getType() == FieldDescriptor.Type.ENUM && fieldValue != null) {
      EnumValueDescriptor enumValueDescriptor = (EnumValueDescriptor)fieldValue;
      return enumValueDescriptor.getName();
    } else if (fieldDescriptor.getType() == FieldDescriptor.Type.BOOL && fieldValue != null) {
      Boolean boolValue = (Boolean)fieldValue;
      return new Integer(boolValue ? 1 : 0);
    } else if (fieldDescriptor.getType() == FieldDescriptor.Type.BYTES && fieldValue != null) {
      ByteString bsValue = (ByteString)fieldValue;
      return new DataByteArray(bsValue.toByteArray());
    }
    return fieldValue;
  }

  /**
   * A utility function for getting the value of a field in a protobuf message.  It first tries the
   * literal set value in the protobuf's field list.  If the value isn't set, and the field has a default
   * value, it uses that.  Otherwise, it returns null.
   * @param msg the protobuf message
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the value of the field, or null if none can be assigned.
   */
  protected Object getFieldValue(Message msg, FieldDescriptor fieldDescriptor) {
    Object o = null;
    Map<FieldDescriptor, Object> setFields = msg.getAllFields();
    if (setFields.containsKey(fieldDescriptor)) {
      o = setFields.get(fieldDescriptor);
    } else if (fieldDescriptor.hasDefaultValue()) {
      o = fieldDescriptor.getDefaultValue();
    }

    return o;
  }

  /**
   * Turn a generic message descriptor into a Schema.  Individual fields that are enums
   * are converted into their string equivalents.
   * @param msgDescriptor the descriptor for the given message type.
   * @return a pig schema representing the message.
   */
  public Schema toSchema(Descriptor msgDescriptor) {
    Schema schema = new Schema();

    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          schema.add(messageToFieldSchema(fieldDescriptor));
        } else {
          schema.add(singleFieldToFieldSchema(fieldDescriptor));
        }
      }
    } catch (FrontendException e) {
      LOG.warn("Could not convert descriptor " + msgDescriptor + " to schema", e);
    }

    return schema;
  }

  /**
   * Turn a nested message into a Schema object.  For repeated nested messages, it generates a schema for a bag of
   * tuples.  For non-repeated nested messages, it just generates a schema for the tuple itself.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the Schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private FieldSchema messageToFieldSchema(FieldDescriptor fieldDescriptor) throws FrontendException {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToFieldSchema called with field of type " + fieldDescriptor.getType();

    // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
    // we can force the type into a pig map type.
    if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
        fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName()) && fieldDescriptor.isRepeated()) {
      return new FieldSchema(fieldDescriptor.getName(), null, DataType.MAP);
    }

    Schema innerSchema = toSchema(fieldDescriptor.getMessageType());

    if (fieldDescriptor.isRepeated()) {
      Schema tupleSchema = new Schema();
      tupleSchema.add(new FieldSchema(fieldDescriptor.getName() + "_tuple", innerSchema, DataType.TUPLE));
      return new FieldSchema(fieldDescriptor.getName(), tupleSchema, DataType.BAG);
    } else {
      return new FieldSchema(fieldDescriptor.getName(), innerSchema, DataType.TUPLE);
    }
  }

  /**
   * Turn a single field into a Schema object.  For repeated single fields, it generates a schema for a bag of single-item tuples.
   * For non-repeated fields, it just generates a standard field schema.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the Schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private FieldSchema singleFieldToFieldSchema(FieldDescriptor fieldDescriptor) throws FrontendException {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "singleFieldToFieldSchema called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      Schema itemSchema = new Schema();
      itemSchema.add(new FieldSchema(fieldDescriptor.getName(), null, getPigDataType(fieldDescriptor)));
      Schema itemTupleSchema = new Schema();
      itemTupleSchema.add(new FieldSchema(fieldDescriptor.getName() + "_tuple", itemSchema, DataType.TUPLE));

      return new FieldSchema(fieldDescriptor.getName() + "_bag", itemTupleSchema, DataType.BAG);
    } else {
      return new FieldSchema(fieldDescriptor.getName(), null, getPigDataType(fieldDescriptor));
    }
  }

  /**
   * Translate between protobuf's datatype representation and Pig's datatype representation.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the byte representing the pig datatype for the given field type.
   */
  private byte getPigDataType(FieldDescriptor fieldDescriptor) {
    switch (fieldDescriptor.getType()) {
      case INT32:
      case UINT32:
      case SINT32:
      case FIXED32:
      case SFIXED32:
      case BOOL: // We convert booleans to ints for pig output.
        return DataType.INTEGER;
      case INT64:
      case UINT64:
      case SINT64:
      case FIXED64:
      case SFIXED64:
        return DataType.LONG;
      case FLOAT:
        return DataType.FLOAT;
      case DOUBLE:
        return DataType.DOUBLE;
      case STRING:
      case ENUM: // We convert enums to strings for pig output.
        return DataType.CHARARRAY;
      case BYTES:
        return DataType.BYTEARRAY;
      case MESSAGE:
        throw new IllegalArgumentException("getPigDataType called on field " + fieldDescriptor.getFullName() + " of type message.");
      default:
        throw new IllegalArgumentException("Unexpected field type. " + fieldDescriptor.toString() + " " + fieldDescriptor.getFullName() + " " + fieldDescriptor.getType());
    }
  }

  /**
   * Turn a generic message descriptor into a loading schema for a pig script.
   * @param msgDescriptor the descriptor for the given message type.
   * @param loaderClassName the fully qualified classname of the pig loader to use.  Not
   * passed a <code>Class<? extends LoadFunc></code> because in many situations that class
   * is being generated as well, and so doesn't exist in compiled form.
   * @return a pig schema representing the message.
   */
  public String toPigScript(Descriptor msgDescriptor, String loaderClassName) {
    StringBuffer sb = new StringBuffer();
    final int initialTabOffset = 3;

    sb.append("raw_data = load '$INPUT_FILES' using " + loaderClassName + "()").append("\n");
    sb.append(tabs(initialTabOffset)).append("as (").append("\n");
    sb.append(toPigScriptInternal(msgDescriptor, initialTabOffset));
    sb.append(tabs(initialTabOffset)).append(");").append("\n").append("\n");

    return sb.toString();
  }

  /**
   * Turn a generic message descriptor into a loading schema for a pig script.  Individual fields that are enums
   * are converted into their string equivalents.
   * @param msgDescriptor the descriptor for the given message type.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return a pig schema representing the message.
   */
  private StringBuffer toPigScriptInternal(Descriptor msgDescriptor, int numTabs) {
    StringBuffer sb = new StringBuffer();
    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        // We have to add a comma after every line EXCEPT for the last, or Pig gets mad.
        boolean isLast = (fieldDescriptor == msgDescriptor.getFields().get(msgDescriptor.getFields().size() - 1));
        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          sb.append(messageToPigScript(fieldDescriptor, numTabs + 1, isLast));
        } else {
          sb.append(singleFieldToPigScript(fieldDescriptor, numTabs + 1, isLast));
        }
      }
    } catch (FrontendException e) {
      LOG.warn("Could not convert descriptor " + msgDescriptor + " to pig script", e);
    }

    return sb;
  }

  /**
   * Turn a nested message into a pig script load string.  For repeated nested messages, it generates a string for a bag of
   * tuples.  For non-repeated nested messages, it just generates a string for the tuple itself.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return the pig script load schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private StringBuffer messageToPigScript(FieldDescriptor fieldDescriptor, int numTabs, boolean isLast) throws FrontendException {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToPigScript called with field of type " + fieldDescriptor.getType();

    // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
    // we force the type into a pig map type.
    if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
        fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName()) && fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName())
          .append(": map[]").append(isLast ? "" : ",").append("\n");
    }

    if (fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": bag {").append("\n")
          .append(tabs(numTabs + 1)).append(fieldDescriptor.getName()).append("_tuple: tuple (").append("\n")
          .append(toPigScriptInternal(fieldDescriptor.getMessageType(), numTabs + 2))
          .append(tabs(numTabs + 1)).append(")").append("\n")
          .append(tabs(numTabs)).append("}").append(isLast ? "" : ",").append("\n");
    } else {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": tuple (").append("\n")
          .append(toPigScriptInternal(fieldDescriptor.getMessageType(), numTabs + 1))
          .append(tabs(numTabs)).append(")").append(isLast ? "" : ",").append("\n");
    }
  }

  /**
   * Turn a single field into a pig script load string.  For repeated single fields, it generates a string for a bag of single-item tuples.
   * For non-repeated fields, it just generates a standard single-element string.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return the pig script load string for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private StringBuffer singleFieldToPigScript(FieldDescriptor fieldDescriptor, int numTabs, boolean isLast) throws FrontendException {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "singleFieldToPigScript called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append("_bag: bag {").append("\n")
          .append(tabs(numTabs + 1)).append(fieldDescriptor.getName()).append("_tuple: tuple (").append("\n")
          .append(tabs(numTabs + 2)).append(fieldDescriptor.getName()).append(": ").append(getPigScriptDataType(fieldDescriptor)).append("\n")
          .append(tabs(numTabs + 1)).append(")").append("\n")
          .append(tabs(numTabs)).append("}").append(isLast ? "" : ",").append("\n");
    } else {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": ")
          .append(getPigScriptDataType(fieldDescriptor)).append(isLast ? "" : ",").append("\n");
    }
  }

  /**
   * Translate between protobuf's datatype representation and Pig's datatype representation.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the byte representing the pig datatype for the given field type.
   */
  private String getPigScriptDataType(FieldDescriptor fieldDescriptor) {
    switch (fieldDescriptor.getType()) {
      case INT32:
      case UINT32:
      case SINT32:
      case FIXED32:
      case SFIXED32:
      case BOOL: // We convert booleans to ints for pig output.
        return "int";
      case INT64:
      case UINT64:
      case SINT64:
      case FIXED64:
      case SFIXED64:
        return "long";
      case FLOAT:
        return "float";
      case DOUBLE:
        return "double";
      case STRING:
      case ENUM: // We convert enums to strings for pig output.
        return "chararray";
      case BYTES:
        return "bytearray";
      case MESSAGE:
        throw new IllegalArgumentException("getPigScriptDataType called on field " + fieldDescriptor.getFullName() + " of type message.");
      default:
        throw new IllegalArgumentException("Unexpected field type. " + fieldDescriptor.toString() + " " + fieldDescriptor.getFullName() + " " + fieldDescriptor.getType());
    }
  }

  private StringBuffer tabs(int numTabs) {
    StringBuffer sb = new StringBuffer();
    for (int i = 0; i < numTabs; i++) {
      sb.append("  ");
    }
    return sb;
  }
}=======
package com.twitter.elephantbird.pig.util;

import java.util.List;
import java.util.Map;

import com.google.common.base.Joiner;
import com.google.common.collect.Lists;
import com.google.common.collect.Maps;
import com.google.protobuf.DescriptorProtos.DescriptorProto;
import com.google.protobuf.Descriptors.Descriptor;
import com.google.protobuf.Descriptors.EnumValueDescriptor;
import com.google.protobuf.Descriptors.FieldDescriptor;
import com.google.protobuf.ByteString;
import com.google.protobuf.Message;
import com.twitter.data.proto.Misc.CountedMap;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * A class for turning codegen'd protos into Pig Tuples and Schemas
 * for custom Pig LoadFuncs.
 * @author Kevin Weil
 */
public class ProtobufToPig {
  private static final Logger LOG = LoggerFactory.getLogger(ProtobufToPig.class);

  private static final TupleFactory tupleFactory_ = TupleFactory.getInstance();
  private static BagFactory bagFactory_ = BagFactory.getInstance();

  public enum CoercionLevel { kNoCoercion, kAllowCoercionToPigMaps }

  private final CoercionLevel coercionLevel_;

  public ProtobufToPig() {
    this(CoercionLevel.kAllowCoercionToPigMaps);
  }

  public ProtobufToPig(CoercionLevel coercionLevel) {
    coercionLevel_ = coercionLevel;
  }
  /**
   * Turn a generic message into a Tuple.  Individual fields that are enums
   * are converted into their string equivalents.  Fields that are not filled
   * out in the protobuf are set to null, unless there is a default field value in
   * which case that is used instead.
   * @param msg the protobuf message
   * @return a pig tuple representing the message.
   */
  public Tuple toTuple(Message msg) {
    if (msg == null) {
      // Pig tuples deal gracefully with nulls.
      // Also, we can be called with null here in recursive calls.
      return null;
    }

    Descriptor msgDescriptor = msg.getDescriptorForType();
    Tuple tuple = tupleFactory_.newTuple(msgDescriptor.getFields().size());
    int curField = 0;
    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        // Get the set value, or the default value, or null.
        Object fieldValue = getFieldValue(msg, fieldDescriptor);

        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          tuple.set(curField++, messageToTuple(fieldDescriptor, fieldValue));
        } else {
          tuple.set(curField++, singleFieldToTuple(fieldDescriptor, fieldValue));
        }
      }
    } catch (ExecException e) {
      LOG.warn("Could not convert msg " + msg + " to tuple", e);
    }

    return tuple;
  }

  /**
   * Translate a nested message to a tuple.  If the field is repeated, it walks the list and adds each to a bag.
   * Otherwise, it just adds the given one.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object representing fieldValue in Pig -- either a bag or a tuple.
   */
  @SuppressWarnings("unchecked")
  protected Object messageToTuple(FieldDescriptor fieldDescriptor, Object fieldValue) {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToTuple called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      // The protobuf contract is that if the field is repeated, then the object returned is actually a List
      // of the underlying datatype, which in this case is a nested message.
      List<Message> messageList = (List<Message>) (fieldValue != null ? fieldValue : Lists.newArrayList());

      // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
      // we can force the type into a pig map type.
      if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
          fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName())) {
        Map<Object, Long> map = Maps.newHashMap();
        for (Message m : messageList) {
          CountedMap cm = (CountedMap) m;
          final Long curCount = map.get(cm.getKey());
          map.put(cm.getKey(), (curCount == null ? 0L : curCount) + cm.getValue());
        }
        return map;
      } else {
        DataBag bag = bagFactory_.newDefaultBag();
        for (Message m : messageList) {
          bag.add(new ProtobufTuple(m));
        }
        return bag;
      }
    } else {
      return new ProtobufTuple((Message)fieldValue);
    }
  }

  /**
   * Translate a single field to a tuple.  If the field is repeated, it walks the list and adds each to a bag.
   * Otherwise, it just adds the given one.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object representing fieldValue in Pig -- either a bag or a single field.
   * @throws ExecException if Pig decides to.  Shouldn't happen because we won't walk off the end of a tuple's field set.
   */
  @SuppressWarnings("unchecked")
  protected Object singleFieldToTuple(FieldDescriptor fieldDescriptor, Object fieldValue) {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "messageToFieldSchema called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      // The protobuf contract is that if the field is repeated, then the object returned is actually a List
      // of the underlying datatype, which in this case is a "primitive" like int, float, String, etc.
      // We have to make a single-item tuple out of it to put it in the bag.
      DataBag bag = bagFactory_.newDefaultBag();
      List<Object> fieldValueList = (List<Object>) (fieldValue != null ? fieldValue : Lists.newArrayList());
      for (Object singleFieldValue : fieldValueList) {
        Object nonEnumFieldValue = coerceToPigTypes(fieldDescriptor, singleFieldValue);
        Tuple innerTuple = tupleFactory_.newTuple(1);
        try {
          innerTuple.set(0, nonEnumFieldValue);
        } catch (ExecException e) { // not expected
          throw new RuntimeException(e);
        }
        bag.add(innerTuple);
      }
      return bag;
    } else {
      return coerceToPigTypes(fieldDescriptor, fieldValue);
    }
  }

  /**
   * If the given field value is an enum, translate it to the enum's name as a string, since Pig cannot handle enums.
   * Also, if the given field value is a bool, translate it to 0 or 1 to avoid Pig bools, which can be sketchy.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param fieldValue the object representing the value of this field, possibly null.
   * @return the object, unless it was from an enum field, in which case we return the name of the enum field.
   */
  private Object coerceToPigTypes(FieldDescriptor fieldDescriptor, Object fieldValue) {
    if (fieldDescriptor.getType() == FieldDescriptor.Type.ENUM && fieldValue != null) {
      EnumValueDescriptor enumValueDescriptor = (EnumValueDescriptor)fieldValue;
      return enumValueDescriptor.getName();
    } else if (fieldDescriptor.getType() == FieldDescriptor.Type.BOOL && fieldValue != null) {
      Boolean boolValue = (Boolean)fieldValue;
      return new Integer(boolValue ? 1 : 0);
    } else if (fieldDescriptor.getType() == FieldDescriptor.Type.BYTES && fieldValue != null) {
      ByteString bsValue = (ByteString)fieldValue;
      return new DataByteArray(bsValue.toByteArray());
    }
    return fieldValue;
  }

  /**
   * A utility function for getting the value of a field in a protobuf message.  It first tries the
   * literal set value in the protobuf's field list.  If the value isn't set, and the field has a default
   * value, it uses that.  Otherwise, it returns null.
   * @param msg the protobuf message
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the value of the field, or null if none can be assigned.
   */
  protected Object getFieldValue(Message msg, FieldDescriptor fieldDescriptor) {
    Object o = null;
    Map<FieldDescriptor, Object> setFields = msg.getAllFields();
    if (setFields.containsKey(fieldDescriptor)) {
      o = setFields.get(fieldDescriptor);
    } else if (fieldDescriptor.hasDefaultValue()) {
      o = fieldDescriptor.getDefaultValue();
    }

    return o;
  }

  /**
   * Turn a generic message descriptor into a Schema.  Individual fields that are enums
   * are converted into their string equivalents.
   * @param msgDescriptor the descriptor for the given message type.
   * @return a pig schema representing the message.
   */
  public Schema toSchema(Descriptor msgDescriptor) {
    Schema schema = new Schema();

    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          schema.add(messageToFieldSchema(fieldDescriptor));
        } else {
          schema.add(singleFieldToFieldSchema(fieldDescriptor));
        }
      }
    } catch (FrontendException e) {
      LOG.warn("Could not convert descriptor " + msgDescriptor + " to schema", e);
    }

    return schema;
  }

  /**
   * Turn a nested message into a Schema object.  For repeated nested messages, it generates a schema for a bag of
   * tuples.  For non-repeated nested messages, it just generates a schema for the tuple itself.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the Schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private FieldSchema messageToFieldSchema(FieldDescriptor fieldDescriptor) throws FrontendException {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToFieldSchema called with field of type " + fieldDescriptor.getType();

    // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
    // we can force the type into a pig map type.
    if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
        fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName()) && fieldDescriptor.isRepeated()) {
      return new FieldSchema(fieldDescriptor.getName(), null, DataType.MAP);
    }

    Schema innerSchema = toSchema(fieldDescriptor.getMessageType());

    if (fieldDescriptor.isRepeated()) {
      Schema tupleSchema = new Schema();
      tupleSchema.add(new FieldSchema(fieldDescriptor.getName() + "_tuple", innerSchema, DataType.TUPLE));
      return new FieldSchema(fieldDescriptor.getName(), tupleSchema, DataType.BAG);
    } else {
      return new FieldSchema(fieldDescriptor.getName(), innerSchema, DataType.TUPLE);
    }
  }

  /**
   * Turn a single field into a Schema object.  For repeated single fields, it generates a schema for a bag of single-item tuples.
   * For non-repeated fields, it just generates a standard field schema.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the Schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private FieldSchema singleFieldToFieldSchema(FieldDescriptor fieldDescriptor) throws FrontendException {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "singleFieldToFieldSchema called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      Schema itemSchema = new Schema();
      itemSchema.add(new FieldSchema(fieldDescriptor.getName(), null, getPigDataType(fieldDescriptor)));
      Schema itemTupleSchema = new Schema();
      itemTupleSchema.add(new FieldSchema(fieldDescriptor.getName() + "_tuple", itemSchema, DataType.TUPLE));

      return new FieldSchema(fieldDescriptor.getName() + "_bag", itemTupleSchema, DataType.BAG);
    } else {
      return new FieldSchema(fieldDescriptor.getName(), null, getPigDataType(fieldDescriptor));
    }
  }

  /**
   * Translate between protobuf's datatype representation and Pig's datatype representation.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the byte representing the pig datatype for the given field type.
   */
  private byte getPigDataType(FieldDescriptor fieldDescriptor) {
    switch (fieldDescriptor.getType()) {
      case INT32:
      case UINT32:
      case SINT32:
      case FIXED32:
      case SFIXED32:
      case BOOL: // We convert booleans to ints for pig output.
        return DataType.INTEGER;
      case INT64:
      case UINT64:
      case SINT64:
      case FIXED64:
      case SFIXED64:
        return DataType.LONG;
      case FLOAT:
        return DataType.FLOAT;
      case DOUBLE:
        return DataType.DOUBLE;
      case STRING:
      case ENUM: // We convert enums to strings for pig output.
        return DataType.CHARARRAY;
      case BYTES:
        return DataType.BYTEARRAY;
      case MESSAGE:
        throw new IllegalArgumentException("getPigDataType called on field " + fieldDescriptor.getFullName() + " of type message.");
      default:
        throw new IllegalArgumentException("Unexpected field type. " + fieldDescriptor.toString() + " " + fieldDescriptor.getFullName() + " " + fieldDescriptor.getType());
    }
  }

  /**
   * Turn a generic message descriptor into a loading schema for a pig script.
   * @param msgDescriptor the descriptor for the given message type.
   * @param loaderClassName the fully qualified classname of the pig loader to use.  Not
   * passed a <code>Class<? extends LoadFunc></code> because in many situations that class
   * is being generated as well, and so doesn't exist in compiled form.
   * @return a pig script that can load the given message.
   */
  public String toPigScript(Descriptor msgDescriptor, String loaderClassName) {
    StringBuffer sb = new StringBuffer();
    final int initialTabOffset = 3;

    sb.append("raw_data = load '$INPUT_FILES' using " + loaderClassName + "()").append("\n");
    sb.append(tabs(initialTabOffset)).append("as (").append("\n");
    sb.append(toPigScriptInternal(msgDescriptor, initialTabOffset));
    sb.append(tabs(initialTabOffset)).append(");").append("\n").append("\n");

    return sb.toString();
  }

  /**
   * Same as toPigScript(Descriptor, String) but allows parameters for the loader.
   *
   * @param msgDescriptor
   * @param loaderClassName
   * @param params
   * @return a pig script that can load the given message.
   */
  public String toPigScript(Descriptor msgDescriptor, String loaderClassName, String... params) {
    StringBuffer sb = new StringBuffer();
    final int initialTabOffset = 3;

    sb.append("raw_data = load '$INPUT_FILES' using ")
    .append(loaderClassName)
    .append("(");
    String paramString = "";
    if (params.length > 0) {
      paramString = "'" + Joiner.on(",'").join(params) + "'";
    }
    sb.append(paramString).append(")").append("\n");
    sb.append("/**\n");
    sb.append(tabs(initialTabOffset)).append("as (").append("\n");
    sb.append(toPigScriptInternal(msgDescriptor, initialTabOffset));
    sb.append(tabs(initialTabOffset)).append(")").append("\n").append("\n");
    sb.append("**/\n;\n");
    return sb.toString();
  }

  /**
   * Turn a generic message descriptor into a loading schema for a pig script.  Individual fields that are enums
   * are converted into their string equivalents.
   * @param msgDescriptor the descriptor for the given message type.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return a pig schema representing the message.
   */
  private StringBuffer toPigScriptInternal(Descriptor msgDescriptor, int numTabs) {
    StringBuffer sb = new StringBuffer();
    try {
      // Walk through all the possible fields in the message.
      for (FieldDescriptor fieldDescriptor : msgDescriptor.getFields()) {
        // We have to add a comma after every line EXCEPT for the last, or Pig gets mad.
        boolean isLast = (fieldDescriptor == msgDescriptor.getFields().get(msgDescriptor.getFields().size() - 1));
        if (fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE) {
          sb.append(messageToPigScript(fieldDescriptor, numTabs + 1, isLast));
        } else {
          sb.append(singleFieldToPigScript(fieldDescriptor, numTabs + 1, isLast));
        }
      }
    } catch (FrontendException e) {
      LOG.warn("Could not convert descriptor " + msgDescriptor + " to pig script", e);
    }

    return sb;
  }

  /**
   * Turn a nested message into a pig script load string.  For repeated nested messages, it generates a string for a bag of
   * tuples.  For non-repeated nested messages, it just generates a string for the tuple itself.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return the pig script load schema for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private StringBuffer messageToPigScript(FieldDescriptor fieldDescriptor, int numTabs, boolean isLast) throws FrontendException {
    assert fieldDescriptor.getType() == FieldDescriptor.Type.MESSAGE : "messageToPigScript called with field of type " + fieldDescriptor.getType();

    // Since protobufs do not have a map type, we use CountedMap to fake it.  Whenever the protobuf has a repeated CountedMap in it,
    // we force the type into a pig map type.
    if (coercionLevel_ == CoercionLevel.kAllowCoercionToPigMaps &&
        fieldDescriptor.getMessageType().getName().equals(CountedMap.getDescriptor().getName()) && fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName())
          .append(": map[]").append(isLast ? "" : ",").append("\n");
    }

    if (fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": bag {").append("\n")
          .append(tabs(numTabs + 1)).append(fieldDescriptor.getName()).append("_tuple: tuple (").append("\n")
          .append(toPigScriptInternal(fieldDescriptor.getMessageType(), numTabs + 2))
          .append(tabs(numTabs + 1)).append(")").append("\n")
          .append(tabs(numTabs)).append("}").append(isLast ? "" : ",").append("\n");
    } else {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": tuple (").append("\n")
          .append(toPigScriptInternal(fieldDescriptor.getMessageType(), numTabs + 1))
          .append(tabs(numTabs)).append(")").append(isLast ? "" : ",").append("\n");
    }
  }

  /**
   * Turn a single field into a pig script load string.  For repeated single fields, it generates a string for a bag of single-item tuples.
   * For non-repeated fields, it just generates a standard single-element string.
   * @param fieldDescriptor the descriptor object for the given field.
   * @param numTabs the tab depth at the current point in the recursion, for pretty printing.
   * @return the pig script load string for the nested message.
   * @throws FrontendException if Pig decides to.
   */
  private StringBuffer singleFieldToPigScript(FieldDescriptor fieldDescriptor, int numTabs, boolean isLast) throws FrontendException {
    assert fieldDescriptor.getType() != FieldDescriptor.Type.MESSAGE : "singleFieldToPigScript called with field of type " + fieldDescriptor.getType();

    if (fieldDescriptor.isRepeated()) {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append("_bag: bag {").append("\n")
          .append(tabs(numTabs + 1)).append(fieldDescriptor.getName()).append("_tuple: tuple (").append("\n")
          .append(tabs(numTabs + 2)).append(fieldDescriptor.getName()).append(": ").append(getPigScriptDataType(fieldDescriptor)).append("\n")
          .append(tabs(numTabs + 1)).append(")").append("\n")
          .append(tabs(numTabs)).append("}").append(isLast ? "" : ",").append("\n");
    } else {
      return new StringBuffer().append(tabs(numTabs)).append(fieldDescriptor.getName()).append(": ")
          .append(getPigScriptDataType(fieldDescriptor)).append(isLast ? "" : ",").append("\n");
    }
  }

  /**
   * Translate between protobuf's datatype representation and Pig's datatype representation.
   * @param fieldDescriptor the descriptor object for the given field.
   * @return the byte representing the pig datatype for the given field type.
   */
  private String getPigScriptDataType(FieldDescriptor fieldDescriptor) {
    switch (fieldDescriptor.getType()) {
      case INT32:
      case UINT32:
      case SINT32:
      case FIXED32:
      case SFIXED32:
      case BOOL: // We convert booleans to ints for pig output.
        return "int";
      case INT64:
      case UINT64:
      case SINT64:
      case FIXED64:
      case SFIXED64:
        return "long";
      case FLOAT:
        return "float";
      case DOUBLE:
        return "double";
      case STRING:
      case ENUM: // We convert enums to strings for pig output.
        return "chararray";
      case BYTES:
        return "bytearray";
      case MESSAGE:
        throw new IllegalArgumentException("getPigScriptDataType called on field " + fieldDescriptor.getFullName() + " of type message.");
      default:
        throw new IllegalArgumentException("Unexpected field type. " + fieldDescriptor.toString() + " " + fieldDescriptor.getFullName() + " " + fieldDescriptor.getType());
    }
  }

  private StringBuffer tabs(int numTabs) {
    StringBuffer sb = new StringBuffer();
    for (int i = 0; i < numTabs; i++) {
      sb.append("  ");
    }
    return sb;
  }

}>>>>>>> YOURS
/home/taes/taes/projects/atlas/revisions/rev_06b2d1f_53cd9d2/rev_06b2d1f-53cd9d2/atlas-demo/AtlasDemo/buildSrc/src/main/java/com/taobao/android/builder/tools/diff/DiffResExtractor.java;<<<<<<< MINE
||||||| BASE
/*
 *
 *
 *
 *                                   Apache License
 *                             Version 2.0, January 2004
 *                          http://www.apache.org/licenses/
 *
 *     TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
 *
 *     1. Definitions.
 *
 *        "License" shall mean the terms and conditions for use, reproduction,
 *        and distribution as defined by Sections 1 through 9 of this document.
 *
 *        "Licensor" shall mean the copyright owner or entity authorized by
 *        the copyright owner that is granting the License.
 *
 *        "Legal Entity" shall mean the union of the acting entity and all
 *        other entities that control, are controlled by, or are under common
 *        control with that entity. For the purposes of this definition,
 *        "control" means (i) the power, direct or indirect, to cause the
 *        direction or management of such entity, whether by contract or
 *        otherwise, or (ii) ownership of fifty percent (50%) or more of the
 *        outstanding shares, or (iii) beneficial ownership of such entity.
 *
 *        "You" (or "Your") shall mean an individual or Legal Entity
 *        exercising permissions granted by this License.
 *
 *        "Source" form shall mean the preferred form for making modifications,
 *        including but not limited to software source code, documentation
 *        source, and configuration files.
 *
 *        "Object" form shall mean any form resulting from mechanical
 *        transformation or translation of a Source form, including but
 *        not limited to compiled object code, generated documentation,
 *        and conversions to other media types.
 *
 *        "Work" shall mean the work of authorship, whether in Source or
 *        Object form, made available under the License, as indicated by a
 *        copyright notice that is included in or attached to the work
 *        (an example is provided in the Appendix below).
 *
 *        "Derivative Works" shall mean any work, whether in Source or Object
 *        form, that is based on (or derived from) the Work and for which the
 *        editorial revisions, annotations, elaborations, or other modifications
 *        represent, as a whole, an original work of authorship. For the purposes
 *        of this License, Derivative Works shall not include works that remain
 *        separable from, or merely link (or bind by name) to the interfaces of,
 *        the Work and Derivative Works thereof.
 *
 *        "Contribution" shall mean any work of authorship, including
 *        the original version of the Work and any modifications or additions
 *        to that Work or Derivative Works thereof, that is intentionally
 *        submitted to Licensor for inclusion in the Work by the copyright owner
 *        or by an individual or Legal Entity authorized to submit on behalf of
 *        the copyright owner. For the purposes of this definition, "submitted"
 *        means any form of electronic, verbal, or written communication sent
 *        to the Licensor or its representatives, including but not limited to
 *        communication on electronic mailing lists, source code control systems,
 *        and issue tracking systems that are managed by, or on behalf of, the
 *        Licensor for the purpose of discussing and improving the Work, but
 *        excluding communication that is conspicuously marked or otherwise
 *        designated in writing by the copyright owner as "Not a Contribution."
 *
 *        "Contributor" shall mean Licensor and any individual or Legal Entity
 *        on behalf of whom a Contribution has been received by Licensor and
 *        subsequently incorporated within the Work.
 *
 *     2. Grant of Copyright License. Subject to the terms and conditions of
 *        this License, each Contributor hereby grants to You a perpetual,
 *        worldwide, non-exclusive, no-charge, royalty-free, irrevocable
 *        copyright license to reproduce, prepare Derivative Works of,
 *        publicly display, publicly perform, sublicense, and distribute the
 *        Work and such Derivative Works in Source or Object form.
 *
 *     3. Grant of Patent License. Subject to the terms and conditions of
 *        this License, each Contributor hereby grants to You a perpetual,
 *        worldwide, non-exclusive, no-charge, royalty-free, irrevocable
 *        (except as stated in this section) patent license to make, have made,
 *        use, offer to sell, sell, import, and otherwise transfer the Work,
 *        where such license applies only to those patent claims licensable
 *        by such Contributor that are necessarily infringed by their
 *        Contribution(s) alone or by combination of their Contribution(s)
 *        with the Work to which such Contribution(s) was submitted. If You
 *        institute patent litigation against any entity (including a
 *        cross-claim or counterclaim in a lawsuit) alleging that the Work
 *        or a Contribution incorporated within the Work constitutes direct
 *        or contributory patent infringement, then any patent licenses
 *        granted to You under this License for that Work shall terminate
 *        as of the date such litigation is filed.
 *
 *     4. Redistribution. You may reproduce and distribute copies of the
 *        Work or Derivative Works thereof in any medium, with or without
 *        modifications, and in Source or Object form, provided that You
 *        meet the following conditions:
 *
 *        (a) You must give any other recipients of the Work or
 *            Derivative Works a copy of this License; and
 *
 *        (b) You must cause any modified files to carry prominent notices
 *            stating that You changed the files; and
 *
 *        (c) You must retain, in the Source form of any Derivative Works
 *            that You distribute, all copyright, patent, trademark, and
 *            attribution notices from the Source form of the Work,
 *            excluding those notices that do not pertain to any part of
 *            the Derivative Works; and
 *
 *        (d) If the Work includes a "NOTICE" text file as part of its
 *            distribution, then any Derivative Works that You distribute must
 *            include a readable copy of the attribution notices contained
 *            within such NOTICE file, excluding those notices that do not
 *            pertain to any part of the Derivative Works, in at least one
 *            of the following places: within a NOTICE text file distributed
 *            as part of the Derivative Works; within the Source form or
 *            documentation, if provided along with the Derivative Works; or,
 *            within a display generated by the Derivative Works, if and
 *            wherever such third-party notices normally appear. The contents
 *            of the NOTICE file are for informational purposes only and
 *            do not modify the License. You may add Your own attribution
 *            notices within Derivative Works that You distribute, alongside
 *            or as an addendum to the NOTICE text from the Work, provided
 *            that such additional attribution notices cannot be construed
 *            as modifying the License.
 *
 *        You may add Your own copyright statement to Your modifications and
 *        may provide additional or different license terms and conditions
 *        for use, reproduction, or distribution of Your modifications, or
 *        for any such Derivative Works as a whole, provided Your use,
 *        reproduction, and distribution of the Work otherwise complies with
 *        the conditions stated in this License.
 *
 *     5. Submission of Contributions. Unless You explicitly state otherwise,
 *        any Contribution intentionally submitted for inclusion in the Work
 *        by You to the Licensor shall be under the terms and conditions of
 *        this License, without any additional terms or conditions.
 *        Notwithstanding the above, nothing herein shall supersede or modify
 *        the terms of any separate license agreement you may have executed
 *        with Licensor regarding such Contributions.
 *
 *     6. Trademarks. This License does not grant permission to use the trade
 *        names, trademarks, service marks, or product names of the Licensor,
 *        except as required for reasonable and customary use in describing the
 *        origin of the Work and reproducing the content of the NOTICE file.
 *
 *     7. Disclaimer of Warranty. Unless required by applicable law or
 *        agreed to in writing, Licensor provides the Work (and each
 *        Contributor provides its Contributions) on an "AS IS" BASIS,
 *        WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
 *        implied, including, without limitation, any warranties or conditions
 *        of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
 *        PARTICULAR PURPOSE. You are solely responsible for determining the
 *        appropriateness of using or redistributing the Work and assume any
 *        risks associated with Your exercise of permissions under this License.
 *
 *     8. Limitation of Liability. In no event and under no legal theory,
 *        whether in tort (including negligence), contract, or otherwise,
 *        unless required by applicable law (such as deliberate and grossly
 *        negligent acts) or agreed to in writing, shall any Contributor be
 *        liable to You for damages, including any direct, indirect, special,
 *        incidental, or consequential damages of any character arising as a
 *        result of this License or out of the use or inability to use the
 *        Work (including but not limited to damages for loss of goodwill,
 *        work stoppage, computer failure or malfunction, or any and all
 *        other commercial damages or losses), even if such Contributor
 *        has been advised of the possibility of such damages.
 *
 *     9. Accepting Warranty or Additional Liability. While redistributing
 *        the Work or Derivative Works thereof, You may choose to offer,
 *        and charge a fee for, acceptance of support, warranty, indemnity,
 *        or other liability obligations and/or rights consistent with this
 *        License. However, in accepting such obligations, You may act only
 *        on Your own behalf and on Your sole responsibility, not on behalf
 *        of any other Contributor, and only if You agree to indemnify,
 *        defend, and hold each Contributor harmless for any liability
 *        incurred by, or claims asserted against, such Contributor by reason
 *        of your accepting any such warranty or additional liability.
 *
 *     END OF TERMS AND CONDITIONS
 *
 *     APPENDIX: How to apply the Apache License to your work.
 *
 *        To apply the Apache License to your work, attach the following
 *        boilerplate notice, with the fields enclosed by brackets "[]"
 *        replaced with your own identifying information. (Don't include
 *        the brackets!)  The text should be enclosed in the appropriate
 *        comment syntax for the file format. We also recommend that a
 *        file or class name and description of purpose be included on the
 *        same "printed page" as the copyright notice for easier
 *        identification within third-party archives.
 *
 *     Copyright 2016 Alibaba Group
 *
 *     Licensed under the Apache License, Version 2.0 (the "License");
 *     you may not use this file except in compliance with the License.
 *     You may obtain a copy of the License at
 *
 *         http://www.apache.org/licenses/LICENSE-2.0
 *
 *     Unless required by applicable law or agreed to in writing, software
 *     distributed under the License is distributed on an "AS IS" BASIS,
 *     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *     See the License for the specific language governing permissions and
 *     limitations under the License.
 *
 *
 */

package com.taobao.android.builder.tools.diff;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.Set;
import java.util.UUID;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import com.taobao.android.builder.tools.MD5Util;
import com.taobao.android.builder.tools.zip.ZipUtils;
import org.apache.commons.io.FileUtils;
import org.apache.commons.io.filefilter.TrueFileFilter;

/**
 * Created by wuzhong on 2016/9/30.
 */
public class DiffResExtractor {

    /**
     * assets : apk
     * res : diffResFiles  apk 
     *
     * @param diffResFiles
     * @param currentApk
     * @param baseApk
     * @param fullResDir
     * @param destDir
     * @throws IOException
     */
    public static void extractDiff(Set<String> diffResFiles, File currentApk, File baseApk, File fullResDir, File destDir) throws IOException {

        if (!currentApk.exists() || !baseApk.exists() || !fullResDir.exists()) {
            return;
        }


        FileUtils.deleteDirectory(destDir);
        destDir.mkdirs();

        File tmpFolder = new File(destDir.getParentFile(), "tmp-diffRes");
        FileUtils.deleteDirectory(tmpFolder);
        tmpFolder.mkdirs();


        File apkDir = new File(tmpFolder, "newApkDir");
        File baseApkDir = new File(tmpFolder, "baseApkDir");

        ZipUtils.unzip(currentApk, apkDir.getAbsolutePath());
        ZipUtils.unzip(baseApk, baseApkDir.getAbsolutePath());

        //compare res and assets
        Collection<File> files = FileUtils.listFiles(apkDir, TrueFileFilter.INSTANCE, TrueFileFilter.INSTANCE);

        int basePathLength = apkDir.getAbsolutePath().length();

        List<String> diffResPath = new ArrayList<String>();

        //assets
        for (File file : files) {

            String relativePath = file.getAbsolutePath().substring(basePathLength);

            if (!relativePath.startsWith("/assets/")) {
                continue;
            }

            File baseFile = new File(baseApkDir, relativePath);
            if (!baseFile.exists()) {
                diffResPath.add(relativePath);
                continue;
            }

            if (!MD5Util.getFileMD5(file).equals(MD5Util.getFileMD5(baseFile))) {

                File rawFile = new File(apkDir, relativePath);
                FileUtils.copyFile(rawFile, new File(destDir, relativePath));

            }
        }

        //res
        for (String diffFile : diffResFiles) {

            File baseFile = new File(baseApkDir, diffFile);
            File currentFile = new File(apkDir, diffFile);

            if (baseFile.exists() && currentFile.exists() && MD5Util.getFileMD5(baseFile).equals(MD5Util.getFileMD5(currentFile))) {
                continue;
            }

            //copy file
            File rawFile = new File(fullResDir, diffFile);
            if (rawFile.exists()) {
                FileUtils.copyFile(rawFile, new File(destDir, diffFile));
            }

        }

        //resource.arsc
        File resDir = new File(destDir, "res");
        if (!resDir.exists()) {
            File valuesDir = new File(resDir, "values");
            FileUtils.forceMkdir(valuesDir);
            File stringsFile = new File(valuesDir, "strings.xml");
            UUID uuid = UUID.randomUUID();
            FileUtils.writeStringToFile(stringsFile, String.format("<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<resources>\n    <string name=\"%s\">%s</string>\n</resources>\n", uuid, uuid), "UTF-8", false);

        }

        final Pattern densityOnlyPattern = Pattern.compile("[a-zA-Z]+-[a-zA-Z]+dpi");
        if (resDir.exists()) {
            File[] resDirs = resDir.listFiles();
            if (resDirs != null) {
                for (File file : resDirs) {
                    Matcher m = densityOnlyPattern.matcher(file.getName());
                    if (m.matches()) {
                        FileUtils.moveDirectory(file, new File(file.getAbsolutePath() + "-v4"));
                    }
                }
            }
        }

    }

}=======
/*
 *
 *
 *
 *                                   Apache License
 *                             Version 2.0, January 2004
 *                          http://www.apache.org/licenses/
 *
 *     TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
 *
 *     1. Definitions.
 *
 *        "License" shall mean the terms and conditions for use, reproduction,
 *        and distribution as defined by Sections 1 through 9 of this document.
 *
 *        "Licensor" shall mean the copyright owner or entity authorized by
 *        the copyright owner that is granting the License.
 *
 *        "Legal Entity" shall mean the union of the acting entity and all
 *        other entities that control, are controlled by, or are under common
 *        control with that entity. For the purposes of this definition,
 *        "control" means (i) the power, direct or indirect, to cause the
 *        direction or management of such entity, whether by contract or
 *        otherwise, or (ii) ownership of fifty percent (50%) or more of the
 *        outstanding shares, or (iii) beneficial ownership of such entity.
 *
 *        "You" (or "Your") shall mean an individual or Legal Entity
 *        exercising permissions granted by this License.
 *
 *        "Source" form shall mean the preferred form for making modifications,
 *        including but not limited to software source code, documentation
 *        source, and configuration files.
 *
 *        "Object" form shall mean any form resulting from mechanical
 *        transformation or translation of a Source form, including but
 *        not limited to compiled object code, generated documentation,
 *        and conversions to other media types.
 *
 *        "Work" shall mean the work of authorship, whether in Source or
 *        Object form, made available under the License, as indicated by a
 *        copyright notice that is included in or attached to the work
 *        (an example is provided in the Appendix below).
 *
 *        "Derivative Works" shall mean any work, whether in Source or Object
 *        form, that is based on (or derived from) the Work and for which the
 *        editorial revisions, annotations, elaborations, or other modifications
 *        represent, as a whole, an original work of authorship. For the purposes
 *        of this License, Derivative Works shall not include works that remain
 *        separable from, or merely link (or bind by name) to the interfaces of,
 *        the Work and Derivative Works thereof.
 *
 *        "Contribution" shall mean any work of authorship, including
 *        the original version of the Work and any modifications or additions
 *        to that Work or Derivative Works thereof, that is intentionally
 *        submitted to Licensor for inclusion in the Work by the copyright owner
 *        or by an individual or Legal Entity authorized to submit on behalf of
 *        the copyright owner. For the purposes of this definition, "submitted"
 *        means any form of electronic, verbal, or written communication sent
 *        to the Licensor or its representatives, including but not limited to
 *        communication on electronic mailing lists, source code control systems,
 *        and issue tracking systems that are managed by, or on behalf of, the
 *        Licensor for the purpose of discussing and improving the Work, but
 *        excluding communication that is conspicuously marked or otherwise
 *        designated in writing by the copyright owner as "Not a Contribution."
 *
 *        "Contributor" shall mean Licensor and any individual or Legal Entity
 *        on behalf of whom a Contribution has been received by Licensor and
 *        subsequently incorporated within the Work.
 *
 *     2. Grant of Copyright License. Subject to the terms and conditions of
 *        this License, each Contributor hereby grants to You a perpetual,
 *        worldwide, non-exclusive, no-charge, royalty-free, irrevocable
 *        copyright license to reproduce, prepare Derivative Works of,
 *        publicly display, publicly perform, sublicense, and distribute the
 *        Work and such Derivative Works in Source or Object form.
 *
 *     3. Grant of Patent License. Subject to the terms and conditions of
 *        this License, each Contributor hereby grants to You a perpetual,
 *        worldwide, non-exclusive, no-charge, royalty-free, irrevocable
 *        (except as stated in this section) patent license to make, have made,
 *        use, offer to sell, sell, import, and otherwise transfer the Work,
 *        where such license applies only to those patent claims licensable
 *        by such Contributor that are necessarily infringed by their
 *        Contribution(s) alone or by combination of their Contribution(s)
 *        with the Work to which such Contribution(s) was submitted. If You
 *        institute patent litigation against any entity (including a
 *        cross-claim or counterclaim in a lawsuit) alleging that the Work
 *        or a Contribution incorporated within the Work constitutes direct
 *        or contributory patent infringement, then any patent licenses
 *        granted to You under this License for that Work shall terminate
 *        as of the date such litigation is filed.
 *
 *     4. Redistribution. You may reproduce and distribute copies of the
 *        Work or Derivative Works thereof in any medium, with or without
 *        modifications, and in Source or Object form, provided that You
 *        meet the following conditions:
 *
 *        (a) You must give any other recipients of the Work or
 *            Derivative Works a copy of this License; and
 *
 *        (b) You must cause any modified files to carry prominent notices
 *            stating that You changed the files; and
 *
 *        (c) You must retain, in the Source form of any Derivative Works
 *            that You distribute, all copyright, patent, trademark, and
 *            attribution notices from the Source form of the Work,
 *            excluding those notices that do not pertain to any part of
 *            the Derivative Works; and
 *
 *        (d) If the Work includes a "NOTICE" text file as part of its
 *            distribution, then any Derivative Works that You distribute must
 *            include a readable copy of the attribution notices contained
 *            within such NOTICE file, excluding those notices that do not
 *            pertain to any part of the Derivative Works, in at least one
 *            of the following places: within a NOTICE text file distributed
 *            as part of the Derivative Works; within the Source form or
 *            documentation, if provided along with the Derivative Works; or,
 *            within a display generated by the Derivative Works, if and
 *            wherever such third-party notices normally appear. The contents
 *            of the NOTICE file are for informational purposes only and
 *            do not modify the License. You may add Your own attribution
 *            notices within Derivative Works that You distribute, alongside
 *            or as an addendum to the NOTICE text from the Work, provided
 *            that such additional attribution notices cannot be construed
 *            as modifying the License.
 *
 *        You may add Your own copyright statement to Your modifications and
 *        may provide additional or different license terms and conditions
 *        for use, reproduction, or distribution of Your modifications, or
 *        for any such Derivative Works as a whole, provided Your use,
 *        reproduction, and distribution of the Work otherwise complies with
 *        the conditions stated in this License.
 *
 *     5. Submission of Contributions. Unless You explicitly state otherwise,
 *        any Contribution intentionally submitted for inclusion in the Work
 *        by You to the Licensor shall be under the terms and conditions of
 *        this License, without any additional terms or conditions.
 *        Notwithstanding the above, nothing herein shall supersede or modify
 *        the terms of any separate license agreement you may have executed
 *        with Licensor regarding such Contributions.
 *
 *     6. Trademarks. This License does not grant permission to use the trade
 *        names, trademarks, service marks, or product names of the Licensor,
 *        except as required for reasonable and customary use in describing the
 *        origin of the Work and reproducing the content of the NOTICE file.
 *
 *     7. Disclaimer of Warranty. Unless required by applicable law or
 *        agreed to in writing, Licensor provides the Work (and each
 *        Contributor provides its Contributions) on an "AS IS" BASIS,
 *        WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
 *        implied, including, without limitation, any warranties or conditions
 *        of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
 *        PARTICULAR PURPOSE. You are solely responsible for determining the
 *        appropriateness of using or redistributing the Work and assume any
 *        risks associated with Your exercise of permissions under this License.
 *
 *     8. Limitation of Liability. In no event and under no legal theory,
 *        whether in tort (including negligence), contract, or otherwise,
 *        unless required by applicable law (such as deliberate and grossly
 *        negligent acts) or agreed to in writing, shall any Contributor be
 *        liable to You for damages, including any direct, indirect, special,
 *        incidental, or consequential damages of any character arising as a
 *        result of this License or out of the use or inability to use the
 *        Work (including but not limited to damages for loss of goodwill,
 *        work stoppage, computer failure or malfunction, or any and all
 *        other commercial damages or losses), even if such Contributor
 *        has been advised of the possibility of such damages.
 *
 *     9. Accepting Warranty or Additional Liability. While redistributing
 *        the Work or Derivative Works thereof, You may choose to offer,
 *        and charge a fee for, acceptance of support, warranty, indemnity,
 *        or other liability obligations and/or rights consistent with this
 *        License. However, in accepting such obligations, You may act only
 *        on Your own behalf and on Your sole responsibility, not on behalf
 *        of any other Contributor, and only if You agree to indemnify,
 *        defend, and hold each Contributor harmless for any liability
 *        incurred by, or claims asserted against, such Contributor by reason
 *        of your accepting any such warranty or additional liability.
 *
 *     END OF TERMS AND CONDITIONS
 *
 *     APPENDIX: How to apply the Apache License to your work.
 *
 *        To apply the Apache License to your work, attach the following
 *        boilerplate notice, with the fields enclosed by brackets "[]"
 *        replaced with your own identifying information. (Don't include
 *        the brackets!)  The text should be enclosed in the appropriate
 *        comment syntax for the file format. We also recommend that a
 *        file or class name and description of purpose be included on the
 *        same "printed page" as the copyright notice for easier
 *        identification within third-party archives.
 *
 *     Copyright 2016 Alibaba Group
 *
 *     Licensed under the Apache License, Version 2.0 (the "License");
 *     you may not use this file except in compliance with the License.
 *     You may obtain a copy of the License at
 *
 *         http://www.apache.org/licenses/LICENSE-2.0
 *
 *     Unless required by applicable law or agreed to in writing, software
 *     distributed under the License is distributed on an "AS IS" BASIS,
 *     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *     See the License for the specific language governing permissions and
 *     limitations under the License.
 *
 *
 */

package com.taobao.android.builder.tools.diff;

import java.io.File;
import java.io.IOException;
import java.util.Collection;
import java.util.Set;
import java.util.UUID;

import com.taobao.android.builder.tools.MD5Util;
import com.taobao.android.builder.tools.zip.ZipUtils;
import org.apache.commons.io.FileUtils;
import org.apache.commons.io.filefilter.TrueFileFilter;

/**
 * Created by wuzhong on 2016/9/30.
 */
public class DiffResExtractor {

    /**
     * assets : apk
     * res : diffResFiles  apk 
     *
     * @param diffResFiles
     * @param currentApk
     * @param baseApk
     * @param fullResDir
     * @param destDir
     * @throws IOException
     */
    public static void extractDiff(Set<String> diffResFiles, File currentApk, File baseApk, File fullResDir,
                                   File destDir) throws IOException {

        if (!currentApk.exists() || !baseApk.exists() || !fullResDir.exists()) {
            return;
        }

        FileUtils.deleteDirectory(destDir);
        destDir.mkdirs();

        File tmpFolder = new File(destDir.getParentFile(), "tmp-diffRes");
        FileUtils.deleteDirectory(tmpFolder);
        tmpFolder.mkdirs();

        File apkDir = new File(tmpFolder, "newApkDir");
        File baseApkDir = new File(tmpFolder, "baseApkDir");

        ZipUtils.unzip(currentApk, apkDir.getAbsolutePath());
        ZipUtils.unzip(baseApk, baseApkDir.getAbsolutePath());

        //compare res and assets
        Collection<File> files = FileUtils.listFiles(apkDir, TrueFileFilter.INSTANCE, TrueFileFilter.INSTANCE);

        int basePathLength = apkDir.getAbsolutePath().length();

        //List<String> diffResPath = new ArrayList<String>();

        //assets
        for (File file : files) {

            String relativePath = file.getAbsolutePath().substring(basePathLength);

            if (!relativePath.startsWith("/assets/")) {
                continue;
            }

            File baseFile = new File(baseApkDir, relativePath);
            if (!baseFile.exists() || !MD5Util.getFileMD5(file).equals(MD5Util.getFileMD5(baseFile))) {
                FileUtils.copyFile(file, new File(destDir, relativePath));
            }

        }

        //res
        for (String diffFile : diffResFiles) {

            File baseFile = new File(baseApkDir, diffFile);
            File currentFile = new File(apkDir, diffFile);

            if (baseFile.exists() && currentFile.exists() && MD5Util.getFileMD5(baseFile).equals(
                MD5Util.getFileMD5(currentFile))) {
                continue;
            }

            //copy file
            File rawFile = new File(fullResDir, diffFile);
            if (rawFile.exists()) {
                FileUtils.copyFile(rawFile, new File(destDir, diffFile));
            }

        }

        //resource.arsc
        File resDir = new File(destDir, "res");
        if (!resDir.exists()) {
            File valuesDir = new File(resDir, "values");
            FileUtils.forceMkdir(valuesDir);
            File stringsFile = new File(valuesDir, "strings.xml");
            UUID uuid = UUID.randomUUID();
            FileUtils.writeStringToFile(stringsFile, String.format(
                "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<resources>\n    <string "
                    + "name=\"%s\">%s</string>\n</resources>\n",
                uuid, uuid), "UTF-8", false);

        }

        //final Pattern densityOnlyPattern = Pattern.compile("[a-zA-Z]+-[a-zA-Z]+dpi");
        //if (resDir.exists()) {
        //    File[] resDirs = resDir.listFiles();
        //    if (resDirs != null) {
        //        for (File file : resDirs) {
        //            Matcher m = densityOnlyPattern.matcher(file.getName());
        //            if (m.matches()) {
        //                FileUtils.moveDirectory(file, new File(file.getAbsolutePath() + "-v4"));
        //            }
        //        }
        //    }
        //}

    }

}>>>>>>> YOURS
/home/taes/taes/projects/atlas/revisions/rev_a504c52_dd98861/rev_a504c52-dd98861/atlas-core/src/main/java/android/taobao/atlas/framework/bundlestorage/BundleArchive.java;null
/home/taes/taes/projects/atlas/revisions/rev_a504c52_dd98861/rev_a504c52-dd98861/atlas-core/src/main/java/android/taobao/atlas/framework/bundlestorage/BundleArchive.java;null
/home/taes/taes/projects/atlas/revisions/rev_a504c52_dd98861/rev_a504c52-dd98861/atlas-core/src/main/java/android/taobao/atlas/framework/bundlestorage/BundleArchive.java;null
/home/taes/taes/projects/atlas/revisions/rev_a504c52_dd98861/rev_a504c52-dd98861/atlas-core/src/main/java/android/taobao/atlas/framework/bundlestorage/BundleArchive.java;null
/home/taes/taes/projects/atlas/revisions/rev_a504c52_dd98861/rev_a504c52-dd98861/atlas-core/src/main/java/android/taobao/atlas/framework/bundlestorage/BundleArchive.java;null
/home/taes/taes/projects/atlas/revisions/rev_a504c52_dd98861/rev_a504c52-dd98861/atlas-core/src/main/java/android/taobao/atlas/framework/BundleClassLoader.java;null
/home/taes/taes/projects/atlas/revisions/rev_a504c52_dd98861/rev_a504c52-dd98861/atlas-core/src/main/java/android/taobao/atlas/bundleInfo/BundleListing.java;null
/home/taes/taes/projects/atlas/revisions/rev_a504c52_dd98861/rev_a504c52-dd98861/atlas-core/src/main/java/android/taobao/atlas/bundleInfo/BundleListing.java;null
/home/taes/taes/projects/atlas/revisions/rev_a504c52_dd98861/rev_a504c52-dd98861/atlas-core/src/main/java/android/taobao/atlas/bundleInfo/BundleListing.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_f06548b_513c57a/rev_f06548b-513c57a/src/java/com/twitter/elephantbird/pig/load/LzoSlice.java;<<<<<<< MINE
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.util.Arrays;
import java.util.HashSet;
import java.util.Set;

import com.twitter.elephantbird.pig.util.LzoBufferedPositionedInputStream;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.BlockLocation;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionCodecFactory;
import org.apache.hadoop.io.compress.CompressionInputStream;
import org.apache.pig.FuncSpec;
import org.apache.pig.Slice;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.backend.datastorage.SeekableInputStream;
import org.apache.pig.backend.datastorage.SeekableInputStream.FLAGS;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.PigContext;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This class implements the Pig Slice interface, meaning it both represents an input split
 * and begins processing on a mapper by binding a loader to each input split.
 */
public class LzoSlice implements Slice {
  private static final Logger LOG = LoggerFactory.getLogger(LzoSlice.class);

  // Generated by Eclipse.
  private static final long serialVersionUID = 4921280606900935966L;

  private final String filename_;
  private long start_;
  private long length_;
  private CompressionInputStream is_;
  private SeekableInputStream fsis_;
  private LzoBaseLoadFunc loader_;
  private FuncSpec loadFuncSpec_;

  /**
   * Construct a slice with the given specifications.
   * @param filename the filename from which the block comes.
   * @param start the byte offset of the start of the block.  If this is zero, it's necessary to read the
   *        file header before real data reading starts.  If this is nonzero, it's assumed to be at the beginning
   *        of an LZO block boundary, otherwise decompression will fail.
   * @param length the length of the block.
   * @param loadFuncSpec the spec for invoking the loader function.
   */
  public LzoSlice(String filename, long start, long length, FuncSpec loadFuncSpec) {
    LOG.debug("LzoSlice::LzoSlice, file = " + filename + ", start = " + start + ", length = " + length);
    filename_ = filename;
    start_ = start;
    length_ = length;
    loadFuncSpec_ = loadFuncSpec;
  }

  public void close() throws IOException {
    if (is_ != null) {
      is_.close();
    }
  }

  public long getStart() {
    return start_;
  }

  public long getLength() {
    return length_;
  }

  /*
   * return the filename being operated on
   */
  public String getFilename() {
    return filename_;
  }

  /**
   * Return the set of servers which contain any blocks of the given file.
   */
  public String[] getLocations() {
    try {
      FileSystem fs = FileSystem.get(new Configuration());
      Set<String> locations = new HashSet<String>();

      FileStatus status = fs.getFileStatus(new Path(filename_));
      BlockLocation[] b = fs.getFileBlockLocations(status, getStart(), getLength());
      for (int i = 0; i < b.length; i++) {
        locations.addAll(Arrays.asList(b[i].getHosts()));
      }
      return locations.toArray(new String[locations.size()]);
    } catch (IOException e) {
      LOG.error("Caught exception: " + e);
      return null;
    }
  }

  public long getPos() throws IOException {
    return fsis_.tell();
  }

  public float getProgress() throws IOException {
    float progress = getPos() - start_;
    float finish = getLength();
    return progress / finish;
  }

  /**
   * Set up the slice by creating the compressed input stream around the given Hadoop file input stream.
   * @param store the Pig storage object.
   */
  public void init(DataStorage store) throws IOException {

    LOG.info("LzoSlice::LzoSlice, file = " + filename_ + ", start = " + start_ + ", length = " + length_);

    fsis_ = store.asElement(store.getActiveContainer(), filename_).sopen();

    CompressionCodecFactory compressionCodecs = new CompressionCodecFactory(new Configuration());
    final CompressionCodec codec = compressionCodecs.getCodec(new Path(filename_));
    is_ = codec.createInputStream(fsis_, codec.createDecompressor());
    // At this point, is_ will already be a nonzero number of bytes into the file, because
    // the Lzop codec reads the header upon opening the stream.
    boolean beginsAtHeader = false;
    if (start_ != 0) {
      // If start_ is nonzero, seek there to begin reading, using SEEK_SET per above.
      fsis_.seek(start_, FLAGS.SEEK_SET);
    } else {
      // If start_ is zero, then it's actually at the header offset. Adjust based on this.
      start_ = fsis_.tell();
      length_ -= start_;
      beginsAtHeader = true;
    }

    LOG.info("Creating constructor for class " + loadFuncSpec_);
    // Use instantiateFuncFromSpec to maintain the arguments passed in from the Pig script.
    loader_ = (LzoBaseLoadFunc) PigContext.instantiateFuncFromSpec(loadFuncSpec_);
    loader_.setBeginsAtHeader(beginsAtHeader);
    // Wrap Pig's BufferedPositionedInputStream with our own, which gives positions based on the number
    // of compressed bytes read rather than the number of uncompressed bytes read.
    loader_.bindTo(filename_, new LzoBufferedPositionedInputStream(is_, start_), start_, start_ + length_);
  }

  /**
   * Get the next tuple by delegating to the loader.
   * @param tuple the tuple to be filled out
   * @return true if the load should continue, i.e. if the tuple was filled out this round.
   */
  public boolean next(Tuple tuple) throws IOException {
    // Delegate to the loader.
    Tuple t = loader_.getNext();
    if (t == null) {
      return false;
    }
    tuple.reference(t);
    return true;
  }


}||||||| BASE
package com.twitter.elephantbird.pig.load;

import java.io.IOException;
import java.util.Arrays;
import java.util.HashSet;
import java.util.Set;

import com.twitter.elephantbird.pig.util.LzoBufferedPositionedInputStream;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.BlockLocation;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionCodecFactory;
import org.apache.hadoop.io.compress.CompressionInputStream;
import org.apache.pig.FuncSpec;
import org.apache.pig.Slice;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.backend.datastorage.SeekableInputStream;
import org.apache.pig.backend.datastorage.SeekableInputStream.FLAGS;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.PigContext;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This class implements the Pig Slice interface, meaning it both represents an input split
 * and begins processing on a mapper by binding a loader to each input split.
 */
public class LzoSlice implements Slice {
  private static final Logger LOG = LoggerFactory.getLogger(LzoSlice.class);

  // Generated by Eclipse.
  private static final long serialVersionUID = 4921280606900935966L;

  private final String filename_;
  private long start_;
  private long length_;
  private CompressionInputStream is_;
  private SeekableInputStream fsis_;
  private LzoBaseLoadFunc loader_;
  private FuncSpec loadFuncSpec_;

  /**
   * Construct a slice with the given specifications.
   * @param filename the filename from which the block comes.
   * @param start the byte offset of the start of the block.  If this is zero, it's necessary to read the
   *        file header before real data reading starts.  If this is nonzero, it's assumed to be at the beginning
   *        of an LZO block boundary, otherwise decompression will fail.
   * @param length the length of the block.
   * @param loadFuncSpec the spec for invoking the loader function.
   */
  public LzoSlice(String filename, long start, long length, FuncSpec loadFuncSpec) {
    LOG.debug("LzoSlice::LzoSlice, file = " + filename + ", start = " + start + ", length = " + length);
    filename_ = filename;
    start_ = start;
    length_ = length;
    loadFuncSpec_ = loadFuncSpec;
  }

  public void close() throws IOException {
    if (is_ != null) {
      is_.close();
    }
  }

  public long getStart() {
    return start_;
  }

  public long getLength() {
    return length_;
  }

  /*
   * return the filename being operated on
   */
  public String getFilename() {
    return filename_;
  }

  /**
   * Return the set of servers which contain any blocks of the given file.
   */
  public String[] getLocations() {
    try {
      FileSystem fs = FileSystem.get(new Configuration());
      Set<String> locations = new HashSet<String>();

      FileStatus status = fs.getFileStatus(new Path(filename_));
      BlockLocation[] b = fs.getFileBlockLocations(status, getStart(), getLength());
      for (int i = 0; i < b.length; i++) {
        locations.addAll(Arrays.asList(b[i].getHosts()));
      }
      return locations.toArray(new String[locations.size()]);
    } catch (IOException e) {
      LOG.error("Caught exception: " + e);
      return null;
    }
  }

  public long getPos() throws IOException {
    return fsis_.tell();
  }

  public float getProgress() throws IOException {
    float progress = getPos() - start_;
    float finish = getLength();
    return progress / finish;
  }

  /**
   * Set up the slice by creating the compressed input stream around the given Hadoop file input stream.
   * @param store the Pig storage object.
   */
  public void init(DataStorage store) throws IOException {
    fsis_ = store.asElement(store.getActiveContainer(), filename_).sopen();

    CompressionCodecFactory compressionCodecs = new CompressionCodecFactory(new Configuration());
    final CompressionCodec codec = compressionCodecs.getCodec(new Path(filename_));
    is_ = codec.createInputStream(fsis_, codec.createDecompressor());
    // At this point, is_ will already be a nonzero number of bytes into the file, because
    // the Lzop codec reads the header upon opening the stream.
    boolean beginsAtHeader = false;
    if (start_ != 0) {
      // If start_ is nonzero, seek there to begin reading, using SEEK_SET per above.
      fsis_.seek(start_, FLAGS.SEEK_SET);
    } else {
      // If start_ is zero, then it's actually at the header offset. Adjust based on this.
      start_ = fsis_.tell();
      length_ -= start_;
      beginsAtHeader = true;
    }

    LOG.info("Creating constructor for class " + loadFuncSpec_);
    // Use instantiateFuncFromSpec to maintain the arguments passed in from the Pig script.
    loader_ = (LzoBaseLoadFunc) PigContext.instantiateFuncFromSpec(loadFuncSpec_);
    loader_.setBeginsAtHeader(beginsAtHeader);
    // Wrap Pig's BufferedPositionedInputStream with our own, which gives positions based on the number
    // of compressed bytes read rather than the number of uncompressed bytes read.
    loader_.bindTo(filename_, new LzoBufferedPositionedInputStream(is_, start_), start_, start_ + length_);
  }

  /**
   * Get the next tuple by delegating to the loader.
   * @param tuple the tuple to be filled out
   * @return true if the load should continue, i.e. if the tuple was filled out this round.
   */
  public boolean next(Tuple tuple) throws IOException {
    // Delegate to the loader.
    Tuple t = loader_.getNext();
    if (t == null) {
      return false;
    }
    tuple.reference(t);
    return true;
  }


}=======
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_f06548b_513c57a/rev_f06548b-513c57a/src/java/com/twitter/elephantbird/pig/load/LzoJsonLoader.java;<<<<<<< MINE
  public LzoJsonLoader() {
    LOG.debug("LzoJsonLoader creation");
  }

  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // Since we are not block aligned we throw away the first record of each split and count on a different
    // instance to read it.  The only split this doesn't work for is the first.
    if (!atFirstRecord) {
      getNext();
    }
  }
||||||| BASE
  public LzoJsonLoader() {
    LOG.info("LzoJsonLoader creation");
  }

  public void skipToNextSyncPoint(boolean atFirstRecord) throws IOException {
    // Since we are not block aligned we throw away the first record of each split and count on a different
    // instance to read it.  The only split this doesn't work for is the first.
    if (!atFirstRecord) {
      getNext();
    }
  }
=======
  public LzoJsonLoader() {}
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_f06548b_513c57a/rev_f06548b-513c57a/src/java/com/twitter/elephantbird/pig/load/LzoJsonLoader.java;<<<<<<< MINE

||||||| BASE
  
=======

  @Override
  public void setLocation(String location, Job job)
  throws IOException {
	  FileInputFormat.setInputPaths(job, location);
  }

>>>>>>> YOURS
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/aapt/src/main/java/com/taobao/android/AaptLib.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/proguard/AtlasProguardConstants.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/android/build/gradle/internal/api/AwbTransform.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/android/build/gradle/internal/api/AwbTransform.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/extension/MultiDexConfig.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/extension/MultiDexConfig.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/extension/MultiDexConfig.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/manager/AtlasAppTaskManager.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/manager/AtlasAppTaskManager.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/manager/AtlasAppTaskManager.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/manager/AtlasAppTaskManager.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/app/merge/MergeManifestAwbsConfigAction.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffApkBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffApkBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffApkBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/bundleinfo/BundleItemRunner.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/FileNameUtils.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/cache/FileLockUtils.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/cache/SimpleLocalCache.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/cache/SimpleNetworkCache.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/cache/FileCacheCenter.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/cache/Cache.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/cache/FileCacheException.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/log/LogOutputListener.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/Profiler.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/Profiler.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/multidex/FileComparator.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/proguard/dump/AbstractClasslVisitor.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/proguard/dump/BundleProguardDumper.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/proguard/dump/ClassDetailVisitor.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/proguard/dump/utils/ReflectUtils.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/proguard/dump/ClassStructVisitor.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/proguard/domain/Result.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/proguard/KeepOnlyConfigurationParser.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/test/java/proguard/KeepOnlyConfigurationParserTest.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/test/java/com/taobao/atlas/LogTest.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/test/java/com/taobao/atlas/cache/FileLockTest.java;null
/home/taes/taes/projects/atlas/revisions/rev_8d5a4e8_cc2b800/rev_8d5a4e8-cc2b800/atlas-gradle-plugin/atlas-plugin/src/test/java/com/taobao/atlas/dex/FastDexMergeTest.java;null
/home/taes/taes/projects/archaius/revisions/rev_f720103_8e7f408/rev_f720103-8e7f408/archaius-core/src/main/java/com/netflix/config/DerivedStringProperty.java;null
/home/taes/taes/projects/archaius/revisions/rev_f720103_8e7f408/rev_f720103-8e7f408/archaius-core/src/test/java/com/netflix/config/DerivedStringPropertyTest.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_50ad2a6_c42b56e/rev_50ad2a6-c42b56e/src/java/com/twitter/elephantbird/mapreduce/input/LzoInputFormat.java;<<<<<<< MINE
  private final Map<Path, LzoIndex> indexes_ = new HashMap<Path, LzoIndex>();

  private final PathFilter hiddenPathFilter = new PathFilter() {
    // avoid hidden files and directories.
    @Override
    public boolean accept(Path path) {
      String name = path.getName();
      return !name.startsWith(".") &&
             !name.startsWith("_");
    }
  };

||||||| BASE
  private final Map<Path, LzoIndex> indexes_ = new HashMap<Path, LzoIndex>();

=======
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-core/src/main/java/com/netflix/client/config/CommonClientConfigKey.java;<<<<<<< MINE
import javax.annotation.Nullable;
||||||| BASE
    AppName("AppName"),
    Version("Version"),
    Port("Port"),
    SecurePort("SecurePort"),
    VipAddress("VipAddress"),
    DeploymentContextBasedVipAddresses("DeploymentContextBasedVipAddresses"),
    MaxAutoRetries("MaxAutoRetries"),
    MaxAutoRetriesNextServer("MaxAutoRetriesNextServer"),
    OkToRetryOnAllOperations("OkToRetryOnAllOperations"),
    RequestSpecificRetryOn("RequestSpecificRetryOn"),
    ReceiveBuffferSize("ReceiveBuffferSize"),
    EnablePrimeConnections("EnablePrimeConnections"),
    PrimeConnectionsClassName("PrimeConnectionsClassName"),
    MaxRetriesPerServerPrimeConnection("MaxRetriesPerServerPrimeConnection"),
    MaxTotalTimeToPrimeConnections("MaxTotalTimeToPrimeConnections"),
    MinPrimeConnectionsRatio("MinPrimeConnectionsRatio"),
    PrimeConnectionsURI("PrimeConnectionsURI"),
    PoolMaxThreads("PoolMaxThreads"),
    PoolMinThreads("PoolMinThreads"),
    PoolKeepAliveTime("PoolKeepAliveTime"),
    PoolKeepAliveTimeUnits("PoolKeepAliveTimeUnits"),
=======
    AppName("AppName"),
    Version("Version"),
    Port("Port"),
    SecurePort("SecurePort"),
    ForceClientPortConfiguration("ForceClientPortConfiguration"), // use client defined port regardless of server advert
    VipAddress("VipAddress"),
    DeploymentContextBasedVipAddresses("DeploymentContextBasedVipAddresses"),
    MaxAutoRetries("MaxAutoRetries"),
    MaxAutoRetriesNextServer("MaxAutoRetriesNextServer"),
    OkToRetryOnAllOperations("OkToRetryOnAllOperations"),
    RequestSpecificRetryOn("RequestSpecificRetryOn"),
    ReceiveBuffferSize("ReceiveBuffferSize"),
    EnablePrimeConnections("EnablePrimeConnections"),
    PrimeConnectionsClassName("PrimeConnectionsClassName"),
    MaxRetriesPerServerPrimeConnection("MaxRetriesPerServerPrimeConnection"),
    MaxTotalTimeToPrimeConnections("MaxTotalTimeToPrimeConnections"),
    MinPrimeConnectionsRatio("MinPrimeConnectionsRatio"),
    PrimeConnectionsURI("PrimeConnectionsURI"),
    PoolMaxThreads("PoolMaxThreads"),
    PoolMinThreads("PoolMinThreads"),
    PoolKeepAliveTime("PoolKeepAliveTime"),
    PoolKeepAliveTimeUnits("PoolKeepAliveTimeUnits"),
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-core/src/main/java/com/netflix/client/LoadBalancerContext.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-core/src/main/java/com/netflix/client/LoadBalancerContext.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-core/src/main/java/com/netflix/client/LoadBalancerContext.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-core/src/main/java/com/netflix/client/LoadBalancerContext.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-core/src/test/java/com/netflix/client/LoadBalancerContextTest.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClientFactory.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClientFactory.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClientFactory.java;null
/home/taes/taes/projects/ribbon/revisions/rev_1c246b4_f841ad0/rev_1c246b4-f841ad0/ribbon-httpclient/src/main/java/com/netflix/http4/NFHttpClientFactory.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/mapreduce/input/LzoInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/mapreduce/input/LzoInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/mapreduce/input/LzoInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/mapreduce/input/LzoInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/mapreduce/input/LzoRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/store/LzoPigStorage.java;<<<<<<< MINE
package com.twitter.elephantbird.pig.store;

import java.io.DataOutputStream;
import java.io.IOException;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.InputFormat;
import org.apache.hadoop.mapreduce.OutputFormat;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.pig.builtin.PigStorage;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.util.StorageUtil;

import com.twitter.elephantbird.mapreduce.input.LzoTextInputFormat;
import com.twitter.elephantbird.mapreduce.output.LzoOutputFormat;

/**
 * A wrapper for {@link PigStorage} to enable LZO compression.
 * LzoTextInputFormat is used for loading since PigStorage
 * can not split lzo files.
 * LzoTextOutputFormat is used for storage so that lzo index files
 * can be written at the same time.
 *
 * This is similar to:
 * <pre>
 *   set output.compression.enabled true;
 *   set output.compression.codec com.hadoop.compression.lzo.LzopCodec;
 *   store/load ... using PigStorage();
 * </pre>
 */
public class LzoPigStorage extends PigStorage {

  private String delimiter = null; // temporary for outpupt format

  public LzoPigStorage() {
    super();
  }

  public LzoPigStorage(String delimiter) {
    super(delimiter);
  }

  @Override
  public InputFormat<LongWritable, Text> getInputFormat() {
    // PigStorage can handle lzo files, but cannot split them.
    return new LzoTextInputFormat();
  }

  @Override
  public OutputFormat<NullWritable, Tuple> getOutputFormat() {
    // LzoOutputFormat can write lzo index file.
    // LzoTextInputFormat can't be used here.
    return new TupleOutputFormat(delimiter);
  }

  // This is a temporary work around for PigStorage since
  // it writes a Tuple to outputformat rather than Text.
  // This may change soon and we can use LzoTextOutputFormat directly.
  protected static class TupleOutputFormat extends LzoOutputFormat<NullWritable, Tuple> {

    private byte fieldDel;

    public TupleOutputFormat(String delimiter) {
      this.fieldDel = delimiter == null ? (byte)'\t' : StorageUtil.parseFieldDel(delimiter);
    }

    @Override
    public RecordWriter<NullWritable, Tuple> getRecordWriter(
        TaskAttemptContext job) throws IOException, InterruptedException {
      final DataOutputStream out = getOutputStream(job);

      return new RecordWriter<NullWritable, Tuple>() {
        public void close(TaskAttemptContext context) throws IOException,
                                                      InterruptedException {
          out.close();
        }

        public void write(NullWritable key, Tuple value) throws IOException,
                                                         InterruptedException {
          int sz = value.size();
          for (int i = 0; i < sz; i++) {
              StorageUtil.putField(out, value.get(i));
              if (i != sz - 1) {
                  out.writeByte(fieldDel);
              }
          }
          out.write('\n');
        }
      };
    }
  }
}||||||| BASE
=======
package com.twitter.elephantbird.pig.store;

import java.io.IOException;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.InputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.pig.builtin.PigStorage;
import com.hadoop.compression.lzo.LzopCodec;
import com.twitter.elephantbird.mapreduce.input.LzoTextInputFormat;

/**
 * A wrapper for {@link PigStorage} to enable LZO compression.
 * LzoTextInputFormat is used for loading since PigStorage
 * can not split lzo files.
 *
 * This is similar to:
 * <pre>
 *   set output.compression.enabled true;
 *   set output.compression.codec com.hadoop.compression.lzo.LzopCodec;
 *   storage alias using PigStorage();
 * </pre>
 */
public class LzoPigStorage extends PigStorage {

  public LzoPigStorage() {
    super();
  }

  public LzoPigStorage(String delimiter) {
    super(delimiter);
  }

  @Override
  public InputFormat<LongWritable, Text> getInputFormat() {
    // PigStorage can handle lzo files, but cannot split them.
    return new LzoTextInputFormat();
  }

  @Override
  public void setStoreLocation(String location, Job job) throws IOException {
    // looks like each context gets different job. modifying conf here
    // does not affect subsequent store statements in the script.
    job.getConfiguration().set("output.compression.enabled", "true");
    job.getConfiguration().set("output.compression.codec", LzopCodec.class.getName());
    super.setStoreLocation(location, job);
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/store/Bz2PigStorage.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/store/LzoTokenizedStorage.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/store/LzoTokenizedStorage.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/store/LzoTokenizedStorage.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/store/LzoTokenizedStorage.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/store/LzoTokenizedStorage.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/store/LzoTokenizedStorage.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/store/LzoTokenizedStorage.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/load/LzoTokenizedLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/load/LzoTokenizedLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/load/LzoTokenizedLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/load/LzoTokenizedLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/load/LzoTokenizedLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/load/LzoTokenizedLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/load/LzoTokenizedLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/load/LzoTextLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/load/LzoTextLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/load/LzoTextLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_ad562ae_130cfc6/rev_ad562ae-130cfc6/src/java/com/twitter/elephantbird/pig/load/LzoTextLoader.java;null
/home/taes/taes/projects/archaius/revisions/rev_61eb7f8_8b0dddc/rev_61eb7f8-8b0dddc/archaius-core/src/main/java/com/netflix/config/DeploymentContext.java;null
/home/taes/taes/projects/archaius/revisions/rev_61eb7f8_8b0dddc/rev_61eb7f8-8b0dddc/archaius-core/src/main/java/com/netflix/config/ConfigurationBasedDeploymentContext.java;null
/home/taes/taes/projects/atlas/revisions/rev_98ae4ea_9a79e22/rev_98ae4ea-9a79e22/atlas-core/src/main/java/android/taobao/atlas/startup/patch/KernalBundle.java;null
/home/taes/taes/projects/atlas/revisions/rev_98ae4ea_9a79e22/rev_98ae4ea-9a79e22/atlas-core/src/main/java/android/taobao/atlas/startup/patch/KernalBundle.java;null
/home/taes/taes/projects/ribbon/revisions/rev_7620032_42c3e4c/rev_7620032-42c3e4c/ribbon-core/src/main/java/com/netflix/client/LoadBalancerContext.java;<<<<<<< MINE
||||||| BASE
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.client;

import java.net.URI;
import java.net.URISyntaxException;
import java.net.URLEncoder;
import java.util.Collection;
import java.util.concurrent.TimeUnit;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.base.Strings;
import com.netflix.client.config.CommonClientConfigKey;
import com.netflix.client.config.DefaultClientConfigImpl;
import com.netflix.client.config.IClientConfig;
import com.netflix.loadbalancer.AbstractLoadBalancer;
import com.netflix.loadbalancer.ILoadBalancer;
import com.netflix.loadbalancer.LoadBalancerStats;
import com.netflix.loadbalancer.Server;
import com.netflix.loadbalancer.ServerStats;
import com.netflix.servo.monitor.Monitors;
import com.netflix.servo.monitor.Timer;
import com.netflix.util.Pair;

/**
 * A class contains APIs intended to be used be load balancing client which is subclass of this class.
 * 
 * @author awang
 *
 * @param <T> Type of the request
 * @param <S> Type of the response
 */
public abstract class LoadBalancerContext<T extends ClientRequest, S extends IResponse> implements IClientConfigAware {
    private static final Logger logger = LoggerFactory.getLogger(LoadBalancerContext.class);

    protected String clientName = "default";          

    protected String vipAddresses;
    
    protected int maxAutoRetriesNextServer = DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES_NEXT_SERVER;
    protected int maxAutoRetries = DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES;

    protected LoadBalancerErrorHandler<? super T, ? super S> errorHandler = new DefaultLoadBalancerErrorHandler<ClientRequest, IResponse>();


    boolean okToRetryOnAllOperations = DefaultClientConfigImpl.DEFAULT_OK_TO_RETRY_ON_ALL_OPERATIONS.booleanValue();
        
    private ILoadBalancer lb;
    
    private volatile Timer tracer;

    public LoadBalancerContext() {
    }

    /**
     * Delegate to {@link #initWithNiwsConfig(IClientConfig)}
     * @param clientConfig
     */
    public LoadBalancerContext(IClientConfig clientConfig) {
        initWithNiwsConfig(clientConfig);        
    }

    /**
     * Set necessary parameters from client configuration and register with Servo monitors.
     */
    @Override
    public void initWithNiwsConfig(IClientConfig clientConfig) {
        if (clientConfig == null) {
            return;    
        }
        clientName = clientConfig.getClientName();
        if (clientName == null) {
            clientName = "default";
        }
        vipAddresses = clientConfig.resolveDeploymentContextbasedVipAddresses();
        maxAutoRetries = clientConfig.getPropertyAsInteger(CommonClientConfigKey.MaxAutoRetries, DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES);
        maxAutoRetriesNextServer = clientConfig.getPropertyAsInteger(CommonClientConfigKey.MaxAutoRetriesNextServer,maxAutoRetriesNextServer);
        
       okToRetryOnAllOperations = clientConfig.getPropertyAsBoolean(CommonClientConfigKey.OkToRetryOnAllOperations, okToRetryOnAllOperations);
       tracer = getExecuteTracer();

       Monitors.registerObject("Client_" + clientName, this);
    }

    protected Timer getExecuteTracer() {
        if (tracer == null) {
            synchronized(this) {
                if (tracer == null) {
                    tracer = Monitors.newTimer(clientName + "_OperationTimer", TimeUnit.MILLISECONDS);                    
                }
            }
        } 
        return tracer;        
    }
    
    public String getClientName() {
        return clientName;
    }
        
    public ILoadBalancer getLoadBalancer() {
        return lb;    
    }
        
    public void setLoadBalancer(ILoadBalancer lb) {
        this.lb = lb;
    }

    public int getMaxAutoRetriesNextServer() {
        return maxAutoRetriesNextServer;
    }

    public void setMaxAutoRetriesNextServer(int maxAutoRetriesNextServer) {
        this.maxAutoRetriesNextServer = maxAutoRetriesNextServer;
    }

    public int getMaxAutoRetries() {
        return maxAutoRetries;
    }

    public void setMaxAutoRetries(int maxAutoRetries) {
        this.maxAutoRetries = maxAutoRetries;
    }

    protected Throwable getDeepestCause(Throwable e) {
        if(e != null) {
            int infiniteLoopPreventionCounter = 10;
            while (e.getCause() != null && infiniteLoopPreventionCounter > 0) {
                infiniteLoopPreventionCounter--;
                e = e.getCause();
            }
        }
        return e;
    }

    private boolean isPresentAsCause(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor) {
        return isPresentAsCauseHelper(throwableToSearchIn, throwableToSearchFor) != null;
    }

    static Throwable isPresentAsCauseHelper(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor) {
        int infiniteLoopPreventionCounter = 10;
        while (throwableToSearchIn != null && infiniteLoopPreventionCounter > 0) {
            infiniteLoopPreventionCounter--;
            if (throwableToSearchIn.getClass().isAssignableFrom(
                    throwableToSearchFor)) {
                return throwableToSearchIn;
            } else {
                throwableToSearchIn = throwableToSearchIn.getCause();
            }
        }
        return null;
    }
    
    /**
     * Test if certain exception classes exist as a cause in a Throwable 
     */
    public static boolean isPresentAsCause(Throwable throwableToSearchIn,
            Collection<Class<? extends Throwable>> throwableToSearchFor) {
        int infiniteLoopPreventionCounter = 10;
        while (throwableToSearchIn != null && infiniteLoopPreventionCounter > 0) {
            infiniteLoopPreventionCounter--;
            for (Class<? extends Throwable> c: throwableToSearchFor) {
                if (throwableToSearchIn.getClass().isAssignableFrom(c)) {
                    return true;
                }
            }
            throwableToSearchIn = throwableToSearchIn.getCause();
        }
        return false;
    }

    protected ClientException generateNIWSException(String uri, Throwable e){
        ClientException niwsClientException;
        if (isPresentAsCause(e, java.net.SocketTimeoutException.class)) {
            niwsClientException = generateTimeoutNIWSException(uri, e);
        }else if (e.getCause() instanceof java.net.UnknownHostException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.UNKNOWN_HOST_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e.getCause() instanceof java.net.ConnectException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.CONNECT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e.getCause() instanceof java.net.NoRouteToHostException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.NO_ROUTE_TO_HOST_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e instanceof ClientException){
            niwsClientException = (ClientException)e;
        }else {
            niwsClientException = new ClientException(
                ClientException.ErrorType.GENERAL,
                "Unable to execute RestClient request for URI:" + uri,
                e);
        }
        return niwsClientException;
    }

    private boolean isPresentAsCause(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor, String messageSubStringToSearchFor) {
        Throwable throwableFound = isPresentAsCauseHelper(throwableToSearchIn, throwableToSearchFor);
        if(throwableFound != null) {
            return throwableFound.getMessage().contains(messageSubStringToSearchFor);
        }
        return false;
    }
    private ClientException generateTimeoutNIWSException(String uri, Throwable e){
        ClientException niwsClientException;
        if (isPresentAsCause(e, java.net.SocketTimeoutException.class,
                "Read timed out")) {
            niwsClientException = new ClientException(
                    ClientException.ErrorType.READ_TIMEOUT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri + ":"
                            + getDeepestCause(e).getMessage(), e);
        } else {
            niwsClientException = new ClientException(
                    ClientException.ErrorType.SOCKET_TIMEOUT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri + ":"
                            + getDeepestCause(e).getMessage(), e);
        }
        return niwsClientException;
    }

    /**
     * This is called after a response is received or an exception is thrown from the client
     * to update related stats.  
     */
    protected void noteRequestCompletion(ServerStats stats, ClientRequest request, IResponse response, Throwable e, long responseTime) {        
        try {
            if (stats != null) {
                stats.decrementActiveRequestsCount();
                stats.incrementNumRequests();
                stats.noteResponseTime(responseTime);
                if (response != null) {
                    stats.clearSuccessiveConnectionFailureCount();                    
                }
            }            
        } catch (Throwable ex) {
            logger.error("Unexpected exception", ex);
        }            
    }
       
    /**
     * This is usually called just before client execute a request.
     */
    protected void noteOpenConnection(ServerStats serverStats, ClientRequest request) {
        if (serverStats == null) {
            return;
        }
        try {
            serverStats.incrementActiveRequestsCount();
        } catch (Throwable e) {
            logger.info("Unable to note Server Stats:", e);
        }
    }

      
    /**
     * Derive scheme and port from a partial URI. For example, for HTTP based client, the URI with 
     * only path "/" should return "http" and 80, whereas the URI constructed with scheme "https" and
     * path "/" should return
     * "https" and 443. This method is called by {@link #computeFinalUriWithLoadBalancer(ClientRequest)}
     * to get the complete executable URI.
     * 
     */
    protected Pair<String, Integer> deriveSchemeAndPortFromPartialUri(T request) {
        URI theUrl = request.getUri();
        boolean isSecure = false;
        String scheme = theUrl.getScheme();
        if (scheme != null) {
            isSecure =  scheme.equalsIgnoreCase("https");
        }
        int port = theUrl.getPort();
        if (port < 0 && !isSecure){
            port = 80;
        } else if (port < 0 && isSecure){
            port = 443;
        }
        if (scheme == null){
            if (isSecure) {
                scheme = "https";
            } else {
                scheme = "http";
            }
        }
        return new Pair<String, Integer>(scheme, port);
    }
    
    /**
     * Get the default port of the target server given the scheme of vip address if it is available. 
     * Subclass should override it to provider protocol specific default port number if any.
     * 
     * @param scheme from the vip address. null if not present.
     * @return 80 if scheme is http, 443 if scheme is https, -1 else.
     */
    protected int getDefaultPortFromScheme(String scheme) {
        if (scheme == null) {
            return -1;
        }
        if (scheme.equals("http")) {
            return 80;
        } else if (scheme.equals("https")) {
            return 443;
        } else {
            return -1;
        }
    }

        
    /**
     * Derive the host and port from virtual address if virtual address is indeed contains the actual host 
     * and port of the server. This is the final resort to compute the final URI in {@link #computeFinalUriWithLoadBalancer(ClientRequest)}
     * if there is no load balancer available and the request URI is incomplete. Sub classes can override this method
     * to be more accurate or throws ClientException if it does not want to support virtual address to be the
     * same as physical server address.
     * <p>
     *  The virtual address is used by certain load balancers to filter the servers of the same function 
     *  to form the server pool. 
     *  
     */
    protected  Pair<String, Integer> deriveHostAndPortFromVipAddress(String vipAddress) 
            throws URISyntaxException, ClientException {
        Pair<String, Integer> hostAndPort = new Pair<String, Integer>(null, -1);
        URI uri = new URI(vipAddress);
        String scheme = uri.getScheme();
        if (scheme == null) {
            uri = new URI("http://" + vipAddress);
        }
        String host = uri.getHost();
        if (host == null) {
            throw new ClientException("Unable to derive host/port from vip address " + vipAddress);
        }
        int port = uri.getPort();
        if (port < 0) {
            port = getDefaultPortFromScheme(scheme);
        }
        if (port < 0) {
            throw new ClientException("Unable to derive host/port from vip address " + vipAddress);
        }
        hostAndPort.setFirst(host);
        hostAndPort.setSecond(port);
        return hostAndPort;
    }
    
    private boolean isVipRecognized(String vipEmbeddedInUri) {
        if (vipEmbeddedInUri == null) {
            return false;
        }
        if (vipAddresses == null) {
            return false;
        }
        String[] addresses = vipAddresses.split(",");
        for (String address: addresses) {
            if (vipEmbeddedInUri.equalsIgnoreCase(address.trim())) {
                return true;
            }
        }
        return false;
    }
    
    /**
     * Compute the final URI from a partial URI in the request. The following steps are performed:
     * 
     * <li> if host is missing and there is a load balancer, get the host/port from server chosen from load balancer
     * <li> if host is missing and there is no load balancer, try to derive host/port from virtual address set with the client
     * <li> if host is present and the authority part of the URI is a virtual address set for the client, 
     * and there is a load balancer, get the host/port from server chosen from load balancer
     * <li> if host is present but none of the above applies, interpret the host as the actual physical address
     * <li> if host is missing but none of the above applies, throws ClientException
     * 
     * @param original Original URI passed from caller
     * @return new request with the final URI  
     */
    @SuppressWarnings("unchecked")
    protected T computeFinalUriWithLoadBalancer(T original) throws ClientException{
        URI newURI;
        URI theUrl = original.getUri();

        if (theUrl == null){
            throw new ClientException(ClientException.ErrorType.GENERAL, "NULL URL passed in");
        }

        String host = theUrl.getHost();
        Pair<String, Integer> schemeAndPort = deriveSchemeAndPortFromPartialUri(original);
        String scheme = schemeAndPort.first();
        int port = schemeAndPort.second();
        // Various Supported Cases
        // The loadbalancer to use and the instances it has is based on how it was registered
        // In each of these cases, the client might come in using Full Url or Partial URL
        ILoadBalancer lb = getLoadBalancer();
        Object loadBalancerKey = original.getLoadBalancerKey();
        if (host == null){
            // Partial URL Case
            // well we have to just get the right instances from lb - or we fall back
            if (lb != null){
                Server svc = lb.chooseServer(loadBalancerKey);
                if (svc == null){
                    throw new ClientException(ClientException.ErrorType.GENERAL,
                            "LoadBalancer returned null Server for :"
                            + clientName);
                }
                host = svc.getHost();
                port = svc.getPort();

                if (host == null){
                    throw new ClientException(ClientException.ErrorType.GENERAL,
                            "Invalid Server for :" + svc);
                }
                if (logger.isDebugEnabled()){
                    logger.debug(clientName + " using LB returned Server:" + svc + "for request:" + theUrl);
                }
            } else {
                // No Full URL - and we dont have a LoadBalancer registered to
                // obtain a server
                // if we have a vipAddress that came with the registration, we
                // can use that else we
                // bail out
                if (vipAddresses != null && vipAddresses.contains(",")) {
                    throw new ClientException(
                            ClientException.ErrorType.GENERAL,
                            this.clientName
                                    + "Partial URI of ("
                                    + theUrl
                                    + ") has been sent in to RestClient (with no LB) to be executed."
                                    + " Also, there are multiple vipAddresses and hence RestClient cant pick"
                                    + "one vipAddress to complete this partial uri");
                } else if (vipAddresses != null) {
                    try {
                        Pair<String,Integer> hostAndPort = deriveHostAndPortFromVipAddress(vipAddresses);
                        host = hostAndPort.first();
                        port = hostAndPort.second();
                    } catch (URISyntaxException e) {
                        throw new ClientException(
                                ClientException.ErrorType.GENERAL,
                                this.clientName
                                        + "Partial URI of ("
                                        + theUrl
                                        + ") has been sent in to RestClient (with no LB) to be executed."
                                        + " Also, the configured/registered vipAddress is unparseable (to determine host and port)");
                    }
                }else{
                    throw new ClientException(
                            ClientException.ErrorType.GENERAL,
                            this.clientName
                                    + " has no LoadBalancer registered and passed in a partial URL request (with no host:port)."
                                    + " Also has no vipAddress registered");
                }
            }
        } else {
            // Full URL Case
            // This could either be a vipAddress or a hostAndPort or a real DNS
            // if vipAddress or hostAndPort, we just have to consult the loadbalancer
            // but if it does not return a server, we should just proceed anyways
            // and assume its a DNS
            // For restClients registered using a vipAddress AND executing a request
            // by passing in the full URL (including host and port), we should only
            // consult lb IFF the URL passed is registered as vipAddress in Discovery
            boolean shouldInterpretAsVip = false;

            if (lb != null) {
                shouldInterpretAsVip = isVipRecognized(original.getUri().getAuthority());
            }
            if (shouldInterpretAsVip) {
                Server svc = lb.chooseServer(loadBalancerKey);
                if (svc != null){
                    host = svc.getHost();
                    port = svc.getPort();
                    if (host == null){
                        throw new ClientException(ClientException.ErrorType.GENERAL,
                                "Invalid Server for :" + svc);
                    }
                    if (logger.isDebugEnabled()){
                        logger.debug("using LB returned Server:" + svc + "for request:" + theUrl);
                    }
                }else{
                    // just fall back as real DNS
                    if (logger.isDebugEnabled()){
                        logger.debug(host + ":" + port + " assumed to be a valid VIP address or exists in the DNS");
                    }
                }
            } else {
             // consult LB to obtain vipAddress backed instance given full URL
                //Full URL execute request - where url!=vipAddress
               if (logger.isDebugEnabled()){
                   logger.debug("Using full URL passed in by caller (not using LB/Discovery):" + theUrl);
               }
            }
        }
        // end of creating final URL
        if (host == null){
            throw new ClientException(ClientException.ErrorType.GENERAL,"Request contains no HOST to talk to");
        }
        // just verify that at this point we have a full URL

        try {
            String urlPath = "";
            if (theUrl.getRawPath() != null && theUrl.getRawPath().startsWith("/")) {
                urlPath = theUrl.getRawPath();
            } else {
                urlPath = "/" + theUrl.getRawPath();
            }
            
            newURI = new URI(scheme, theUrl.getUserInfo(), host, port, urlPath, theUrl.getQuery(), theUrl.getFragment());
            if (isURIEncoded(theUrl)) {
                StringBuilder sb = new StringBuilder();
                sb.append(newURI.getScheme())
                  .append("://")
                  .append(newURI.getRawAuthority())
                  .append(theUrl.getRawPath());
                if (!Strings.isNullOrEmpty(theUrl.getRawQuery())) {
                    sb.append("?").append(theUrl.getRawQuery());
                }
                if (!Strings.isNullOrEmpty(theUrl.getRawFragment())) {
                    sb.append("#").append(theUrl.getRawFragment());
                }
                newURI = new URI(sb.toString());
            }
            return (T) original.replaceUri(newURI);            
        } catch (URISyntaxException e) {
            throw new ClientException(ClientException.ErrorType.GENERAL, e.getMessage());
        }
    }

    private boolean isURIEncoded(URI uri) {
        String original = uri.toString();
        try {
            return !URLEncoder.encode(original, "UTF-8").equals(original);
        } catch (Exception e) {
            return false;
        }
    }
    
    protected boolean isRetriable(T request) {
        if (request.isRetriable()) {
            return true;            
        } else {
            boolean retryOkayOnOperation = okToRetryOnAllOperations;
            IClientConfig overriddenClientConfig = request.getOverrideConfig();
            if (overriddenClientConfig != null) {
                retryOkayOnOperation = overriddenClientConfig.getPropertyAsBoolean(CommonClientConfigKey.RequestSpecificRetryOn, okToRetryOnAllOperations);
            }
            return retryOkayOnOperation;
        }
    }
    
    protected int getRetriesNextServer(IClientConfig overriddenClientConfig) {
        int numRetries = maxAutoRetriesNextServer;
        if (overriddenClientConfig != null) {
            numRetries = overriddenClientConfig.getPropertyAsInteger(CommonClientConfigKey.MaxAutoRetriesNextServer, maxAutoRetriesNextServer);
        }
        return numRetries;
    }
    
    public final ServerStats getServerStats(Server server) {
        ServerStats serverStats = null;
        ILoadBalancer lb = this.getLoadBalancer();
        if (lb instanceof AbstractLoadBalancer){
            LoadBalancerStats lbStats = ((AbstractLoadBalancer) lb).getLoadBalancerStats();
            serverStats = lbStats.getSingleServerStat(server);
        }
        return serverStats;

    }

    protected int getNumberRetriesOnSameServer(IClientConfig overriddenClientConfig) {
        int numRetries =  maxAutoRetries;
        if (overriddenClientConfig!=null){
            try {
                numRetries = overriddenClientConfig.getPropertyAsInteger(CommonClientConfigKey.MaxAutoRetries, maxAutoRetries);
            } catch (Exception e) {
                logger.warn("Invalid maxRetries requested for RestClient:" + this.clientName);
            }
        }
        return numRetries;
    }
    
    protected boolean handleSameServerRetry(URI uri, int currentRetryCount, int maxRetries, Throwable e) {
        if (currentRetryCount > maxRetries) {
            return false;
        }
        logger.debug("Exception while executing request which is deemed retry-able, retrying ..., SAME Server Retry Attempt#: {}, URI: {}",  
                currentRetryCount, uri);
        return true;
    }

    public final LoadBalancerErrorHandler<? super T, ? super S> getErrorHandler() {
        return errorHandler;
    }

    public final void setErrorHandler(
            LoadBalancerErrorHandler<? super T, ? super S> errorHandler) {
        this.errorHandler = errorHandler;
    }

    public final boolean isOkToRetryOnAllOperations() {
        return okToRetryOnAllOperations;
    }

    public final void setOkToRetryOnAllOperations(boolean okToRetryOnAllOperations) {
        this.okToRetryOnAllOperations = okToRetryOnAllOperations;
    }
}=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.client;

import java.net.URI;
import java.net.URISyntaxException;
import java.net.URLEncoder;
import java.util.Collection;
import java.util.concurrent.TimeUnit;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.base.Strings;
import com.netflix.client.config.CommonClientConfigKey;
import com.netflix.client.config.DefaultClientConfigImpl;
import com.netflix.client.config.IClientConfig;
import com.netflix.loadbalancer.AbstractLoadBalancer;
import com.netflix.loadbalancer.ILoadBalancer;
import com.netflix.loadbalancer.LoadBalancerStats;
import com.netflix.loadbalancer.Server;
import com.netflix.loadbalancer.ServerStats;
import com.netflix.servo.monitor.Monitors;
import com.netflix.servo.monitor.Timer;
import com.netflix.util.Pair;

/**
 * A class contains APIs intended to be used be load balancing client which is subclass of this class.
 * 
 * @author awang
 *
 * @param <T> Type of the request
 * @param <S> Type of the response
 */
public abstract class LoadBalancerContext<T extends ClientRequest, S extends IResponse> implements IClientConfigAware {
    private static final Logger logger = LoggerFactory.getLogger(LoadBalancerContext.class);

    protected String clientName = "default";          

    protected String vipAddresses;
    
    protected int maxAutoRetriesNextServer = DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES_NEXT_SERVER;
    protected int maxAutoRetries = DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES;

    protected LoadBalancerErrorHandler<? super T, ? super S> errorHandler = new DefaultLoadBalancerErrorHandler<ClientRequest, IResponse>();


    boolean okToRetryOnAllOperations = DefaultClientConfigImpl.DEFAULT_OK_TO_RETRY_ON_ALL_OPERATIONS.booleanValue();
        
    private ILoadBalancer lb;
    
    private volatile Timer tracer;

    public LoadBalancerContext() {
    }

    /**
     * Delegate to {@link #initWithNiwsConfig(IClientConfig)}
     * @param clientConfig
     */
    public LoadBalancerContext(IClientConfig clientConfig) {
        initWithNiwsConfig(clientConfig);        
    }

    /**
     * Set necessary parameters from client configuration and register with Servo monitors.
     */
    @Override
    public void initWithNiwsConfig(IClientConfig clientConfig) {
        if (clientConfig == null) {
            return;    
        }
        clientName = clientConfig.getClientName();
        if (clientName == null) {
            clientName = "default";
        }
        vipAddresses = clientConfig.resolveDeploymentContextbasedVipAddresses();
        maxAutoRetries = clientConfig.getPropertyAsInteger(CommonClientConfigKey.MaxAutoRetries, DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES);
        maxAutoRetriesNextServer = clientConfig.getPropertyAsInteger(CommonClientConfigKey.MaxAutoRetriesNextServer,maxAutoRetriesNextServer);
        
       okToRetryOnAllOperations = clientConfig.getPropertyAsBoolean(CommonClientConfigKey.OkToRetryOnAllOperations, okToRetryOnAllOperations);
       tracer = getExecuteTracer();

       Monitors.registerObject("Client_" + clientName, this);
    }

    protected Timer getExecuteTracer() {
        if (tracer == null) {
            synchronized(this) {
                if (tracer == null) {
                    tracer = Monitors.newTimer(clientName + "_OperationTimer", TimeUnit.MILLISECONDS);                    
                }
            }
        } 
        return tracer;        
    }
    
    public String getClientName() {
        return clientName;
    }
        
    public ILoadBalancer getLoadBalancer() {
        return lb;    
    }
        
    public void setLoadBalancer(ILoadBalancer lb) {
        this.lb = lb;
    }

    public int getMaxAutoRetriesNextServer() {
        return maxAutoRetriesNextServer;
    }

    public void setMaxAutoRetriesNextServer(int maxAutoRetriesNextServer) {
        this.maxAutoRetriesNextServer = maxAutoRetriesNextServer;
    }

    public int getMaxAutoRetries() {
        return maxAutoRetries;
    }

    public void setMaxAutoRetries(int maxAutoRetries) {
        this.maxAutoRetries = maxAutoRetries;
    }

    protected Throwable getDeepestCause(Throwable e) {
        if(e != null) {
            int infiniteLoopPreventionCounter = 10;
            while (e.getCause() != null && infiniteLoopPreventionCounter > 0) {
                infiniteLoopPreventionCounter--;
                e = e.getCause();
            }
        }
        return e;
    }

    private boolean isPresentAsCause(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor) {
        return isPresentAsCauseHelper(throwableToSearchIn, throwableToSearchFor) != null;
    }

    static Throwable isPresentAsCauseHelper(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor) {
        int infiniteLoopPreventionCounter = 10;
        while (throwableToSearchIn != null && infiniteLoopPreventionCounter > 0) {
            infiniteLoopPreventionCounter--;
            if (throwableToSearchIn.getClass().isAssignableFrom(
                    throwableToSearchFor)) {
                return throwableToSearchIn;
            } else {
                throwableToSearchIn = throwableToSearchIn.getCause();
            }
        }
        return null;
    }
    
    /**
     * Test if certain exception classes exist as a cause in a Throwable 
     */
    public static boolean isPresentAsCause(Throwable throwableToSearchIn,
            Collection<Class<? extends Throwable>> throwableToSearchFor) {
        int infiniteLoopPreventionCounter = 10;
        while (throwableToSearchIn != null && infiniteLoopPreventionCounter > 0) {
            infiniteLoopPreventionCounter--;
            for (Class<? extends Throwable> c: throwableToSearchFor) {
                if (throwableToSearchIn.getClass().isAssignableFrom(c)) {
                    return true;
                }
            }
            throwableToSearchIn = throwableToSearchIn.getCause();
        }
        return false;
    }

    protected ClientException generateNIWSException(String uri, Throwable e){
        ClientException niwsClientException;
        if (isPresentAsCause(e, java.net.SocketTimeoutException.class)) {
            niwsClientException = generateTimeoutNIWSException(uri, e);
        }else if (e.getCause() instanceof java.net.UnknownHostException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.UNKNOWN_HOST_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e.getCause() instanceof java.net.ConnectException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.CONNECT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e.getCause() instanceof java.net.NoRouteToHostException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.NO_ROUTE_TO_HOST_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e instanceof ClientException){
            niwsClientException = (ClientException)e;
        }else {
            niwsClientException = new ClientException(
                ClientException.ErrorType.GENERAL,
                "Unable to execute RestClient request for URI:" + uri,
                e);
        }
        return niwsClientException;
    }

    private boolean isPresentAsCause(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor, String messageSubStringToSearchFor) {
        Throwable throwableFound = isPresentAsCauseHelper(throwableToSearchIn, throwableToSearchFor);
        if(throwableFound != null) {
            return throwableFound.getMessage().contains(messageSubStringToSearchFor);
        }
        return false;
    }
    private ClientException generateTimeoutNIWSException(String uri, Throwable e){
        ClientException niwsClientException;
        if (isPresentAsCause(e, java.net.SocketTimeoutException.class,
                "Read timed out")) {
            niwsClientException = new ClientException(
                    ClientException.ErrorType.READ_TIMEOUT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri + ":"
                            + getDeepestCause(e).getMessage(), e);
        } else {
            niwsClientException = new ClientException(
                    ClientException.ErrorType.SOCKET_TIMEOUT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri + ":"
                            + getDeepestCause(e).getMessage(), e);
        }
        return niwsClientException;
    }

    /**
     * This is called after a response is received or an exception is thrown from the client
     * to update related stats.  
     */
    protected void noteRequestCompletion(ServerStats stats, ClientRequest request, IResponse response, Throwable e, long responseTime) {        
        try {
            if (stats != null) {
                stats.decrementActiveRequestsCount();
                stats.incrementNumRequests();
                stats.noteResponseTime(responseTime);
                if (response != null) {
                    stats.clearSuccessiveConnectionFailureCount();                    
                }
            }            
        } catch (Throwable ex) {
            logger.error("Unexpected exception", ex);
        }            
    }
       
    /**
     * This is usually called just before client execute a request.
     */
    protected void noteOpenConnection(ServerStats serverStats, ClientRequest request) {
        if (serverStats == null) {
            return;
        }
        try {
            serverStats.incrementActiveRequestsCount();
        } catch (Throwable e) {
            logger.info("Unable to note Server Stats:", e);
        }
    }

      
    /**
     * Derive scheme and port from a partial URI. For example, for HTTP based client, the URI with 
     * only path "/" should return "http" and 80, whereas the URI constructed with scheme "https" and
     * path "/" should return
     * "https" and 443. This method is called by {@link #computeFinalUriWithLoadBalancer(ClientRequest)}
     * to get the complete executable URI.
     * 
     */
    protected Pair<String, Integer> deriveSchemeAndPortFromPartialUri(T request) {
        URI theUrl = request.getUri();
        boolean isSecure = false;
        String scheme = theUrl.getScheme();
        if (scheme != null) {
            isSecure =  scheme.equalsIgnoreCase("https");
        }
        int port = theUrl.getPort();
        if (port < 0 && !isSecure){
            port = 80;
        } else if (port < 0 && isSecure){
            port = 443;
        }
        if (scheme == null){
            if (isSecure) {
                scheme = "https";
            } else {
                scheme = "http";
            }
        }
        return new Pair<String, Integer>(scheme, port);
    }
    
    /**
     * Get the default port of the target server given the scheme of vip address if it is available. 
     * Subclass should override it to provider protocol specific default port number if any.
     * 
     * @param scheme from the vip address. null if not present.
     * @return 80 if scheme is http, 443 if scheme is https, -1 else.
     */
    protected int getDefaultPortFromScheme(String scheme) {
        if (scheme == null) {
            return -1;
        }
        if (scheme.equals("http")) {
            return 80;
        } else if (scheme.equals("https")) {
            return 443;
        } else {
            return -1;
        }
    }

        
    /**
     * Derive the host and port from virtual address if virtual address is indeed contains the actual host 
     * and port of the server. This is the final resort to compute the final URI in {@link #computeFinalUriWithLoadBalancer(ClientRequest)}
     * if there is no load balancer available and the request URI is incomplete. Sub classes can override this method
     * to be more accurate or throws ClientException if it does not want to support virtual address to be the
     * same as physical server address.
     * <p>
     *  The virtual address is used by certain load balancers to filter the servers of the same function 
     *  to form the server pool. 
     *  
     */
    protected  Pair<String, Integer> deriveHostAndPortFromVipAddress(String vipAddress) 
            throws URISyntaxException, ClientException {
        Pair<String, Integer> hostAndPort = new Pair<String, Integer>(null, -1);
        URI uri = new URI(vipAddress);
        String scheme = uri.getScheme();
        if (scheme == null) {
            uri = new URI("http://" + vipAddress);
        }
        String host = uri.getHost();
        if (host == null) {
            throw new ClientException("Unable to derive host/port from vip address " + vipAddress);
        }
        int port = uri.getPort();
        if (port < 0) {
            port = getDefaultPortFromScheme(scheme);
        }
        if (port < 0) {
            throw new ClientException("Unable to derive host/port from vip address " + vipAddress);
        }
        hostAndPort.setFirst(host);
        hostAndPort.setSecond(port);
        return hostAndPort;
    }
    
    private boolean isVipRecognized(String vipEmbeddedInUri) {
        if (vipEmbeddedInUri == null) {
            return false;
        }
        if (vipAddresses == null) {
            return false;
        }
        String[] addresses = vipAddresses.split(",");
        for (String address: addresses) {
            if (vipEmbeddedInUri.equalsIgnoreCase(address.trim())) {
                return true;
            }
        }
        return false;
    }
    
    /**
     * Compute the final URI from a partial URI in the request. The following steps are performed:
     * 
     * <li> if host is missing and there is a load balancer, get the host/port from server chosen from load balancer
     * <li> if host is missing and there is no load balancer, try to derive host/port from virtual address set with the client
     * <li> if host is present and the authority part of the URI is a virtual address set for the client, 
     * and there is a load balancer, get the host/port from server chosen from load balancer
     * <li> if host is present but none of the above applies, interpret the host as the actual physical address
     * <li> if host is missing but none of the above applies, throws ClientException
     * 
     * @param original Original URI passed from caller
     * @return new request with the final URI  
     */
    @SuppressWarnings("unchecked")
    protected T computeFinalUriWithLoadBalancer(T original) throws ClientException{
        URI theUrl = original.getUri();

        if (theUrl == null){
            throw new ClientException(ClientException.ErrorType.GENERAL, "NULL URL passed in");
        }

        String host = theUrl.getHost();
        Pair<String, Integer> schemeAndPort = deriveSchemeAndPortFromPartialUri(original);
        String scheme = schemeAndPort.first();
        int port = schemeAndPort.second();
        // Various Supported Cases
        // The loadbalancer to use and the instances it has is based on how it was registered
        // In each of these cases, the client might come in using Full Url or Partial URL
        ILoadBalancer lb = getLoadBalancer();
        Object loadBalancerKey = original.getLoadBalancerKey();
        if (host == null){
            // Partial URL Case
            // well we have to just get the right instances from lb - or we fall back
            if (lb != null){
                Server svc = lb.chooseServer(loadBalancerKey);
                if (svc == null){
                    throw new ClientException(ClientException.ErrorType.GENERAL,
                            "LoadBalancer returned null Server for :"
                            + clientName);
                }
                host = svc.getHost();
                port = svc.getPort();

                if (host == null){
                    throw new ClientException(ClientException.ErrorType.GENERAL,
                            "Invalid Server for :" + svc);
                }
                if (logger.isDebugEnabled()){
                    logger.debug(clientName + " using LB returned Server:" + svc + "for request:" + theUrl);
                }
            } else {
                // No Full URL - and we dont have a LoadBalancer registered to
                // obtain a server
                // if we have a vipAddress that came with the registration, we
                // can use that else we
                // bail out
                if (vipAddresses != null && vipAddresses.contains(",")) {
                    throw new ClientException(
                            ClientException.ErrorType.GENERAL,
                            this.clientName
                                    + "Partial URI of ("
                                    + theUrl
                                    + ") has been sent in to RestClient (with no LB) to be executed."
                                    + " Also, there are multiple vipAddresses and hence RestClient cant pick"
                                    + "one vipAddress to complete this partial uri");
                } else if (vipAddresses != null) {
                    try {
                        Pair<String,Integer> hostAndPort = deriveHostAndPortFromVipAddress(vipAddresses);
                        host = hostAndPort.first();
                        port = hostAndPort.second();
                    } catch (URISyntaxException e) {
                        throw new ClientException(
                                ClientException.ErrorType.GENERAL,
                                this.clientName
                                        + "Partial URI of ("
                                        + theUrl
                                        + ") has been sent in to RestClient (with no LB) to be executed."
                                        + " Also, the configured/registered vipAddress is unparseable (to determine host and port)");
                    }
                }else{
                    throw new ClientException(
                            ClientException.ErrorType.GENERAL,
                            this.clientName
                                    + " has no LoadBalancer registered and passed in a partial URL request (with no host:port)."
                                    + " Also has no vipAddress registered");
                }
            }
        } else {
            // Full URL Case
            // This could either be a vipAddress or a hostAndPort or a real DNS
            // if vipAddress or hostAndPort, we just have to consult the loadbalancer
            // but if it does not return a server, we should just proceed anyways
            // and assume its a DNS
            // For restClients registered using a vipAddress AND executing a request
            // by passing in the full URL (including host and port), we should only
            // consult lb IFF the URL passed is registered as vipAddress in Discovery
            boolean shouldInterpretAsVip = false;

            if (lb != null) {
                shouldInterpretAsVip = isVipRecognized(original.getUri().getAuthority());
            }
            if (shouldInterpretAsVip) {
                Server svc = lb.chooseServer(loadBalancerKey);
                if (svc != null){
                    host = svc.getHost();
                    port = svc.getPort();
                    if (host == null){
                        throw new ClientException(ClientException.ErrorType.GENERAL,
                                "Invalid Server for :" + svc);
                    }
                    if (logger.isDebugEnabled()){
                        logger.debug("using LB returned Server:" + svc + "for request:" + theUrl);
                    }
                }else{
                    // just fall back as real DNS
                    if (logger.isDebugEnabled()){
                        logger.debug(host + ":" + port + " assumed to be a valid VIP address or exists in the DNS");
                    }
                }
            } else {
             // consult LB to obtain vipAddress backed instance given full URL
                //Full URL execute request - where url!=vipAddress
               if (logger.isDebugEnabled()){
                   logger.debug("Using full URL passed in by caller (not using LB/Discovery):" + theUrl);
               }
            }
        }
        // end of creating final URL
        if (host == null){
            throw new ClientException(ClientException.ErrorType.GENERAL,"Request contains no HOST to talk to");
        }
        // just verify that at this point we have a full URL

        try {
            StringBuilder sb = new StringBuilder();
            sb.append(scheme).append("://");
            if (!Strings.isNullOrEmpty(theUrl.getRawUserInfo())) {
                sb.append(theUrl.getRawUserInfo()).append("@");
            }
            sb.append(host);
            if (port >= 0) {
                sb.append(":").append(port);
            }
            sb.append(theUrl.getRawPath());
            if (!Strings.isNullOrEmpty(theUrl.getRawQuery())) {
                sb.append("?").append(theUrl.getRawQuery());
            }
            if (!Strings.isNullOrEmpty(theUrl.getRawFragment())) {
                sb.append("#").append(theUrl.getRawFragment());
            }
            URI newURI = new URI(sb.toString());
            return (T) original.replaceUri(newURI);            
        } catch (URISyntaxException e) {
            throw new ClientException(ClientException.ErrorType.GENERAL, e.getMessage());
        }
    }
    
    protected boolean isRetriable(T request) {
        if (request.isRetriable()) {
            return true;            
        } else {
            boolean retryOkayOnOperation = okToRetryOnAllOperations;
            IClientConfig overriddenClientConfig = request.getOverrideConfig();
            if (overriddenClientConfig != null) {
                retryOkayOnOperation = overriddenClientConfig.getPropertyAsBoolean(CommonClientConfigKey.RequestSpecificRetryOn, okToRetryOnAllOperations);
            }
            return retryOkayOnOperation;
        }
    }
    
    protected int getRetriesNextServer(IClientConfig overriddenClientConfig) {
        int numRetries = maxAutoRetriesNextServer;
        if (overriddenClientConfig != null) {
            numRetries = overriddenClientConfig.getPropertyAsInteger(CommonClientConfigKey.MaxAutoRetriesNextServer, maxAutoRetriesNextServer);
        }
        return numRetries;
    }
    
    public final ServerStats getServerStats(Server server) {
        ServerStats serverStats = null;
        ILoadBalancer lb = this.getLoadBalancer();
        if (lb instanceof AbstractLoadBalancer){
            LoadBalancerStats lbStats = ((AbstractLoadBalancer) lb).getLoadBalancerStats();
            serverStats = lbStats.getSingleServerStat(server);
        }
        return serverStats;

    }

    protected int getNumberRetriesOnSameServer(IClientConfig overriddenClientConfig) {
        int numRetries =  maxAutoRetries;
        if (overriddenClientConfig!=null){
            try {
                numRetries = overriddenClientConfig.getPropertyAsInteger(CommonClientConfigKey.MaxAutoRetries, maxAutoRetries);
            } catch (Exception e) {
                logger.warn("Invalid maxRetries requested for RestClient:" + this.clientName);
            }
        }
        return numRetries;
    }
    
    protected boolean handleSameServerRetry(URI uri, int currentRetryCount, int maxRetries, Throwable e) {
        if (currentRetryCount > maxRetries) {
            return false;
        }
        logger.debug("Exception while executing request which is deemed retry-able, retrying ..., SAME Server Retry Attempt#: {}, URI: {}",  
                currentRetryCount, uri);
        return true;
    }

    public final LoadBalancerErrorHandler<? super T, ? super S> getErrorHandler() {
        return errorHandler;
    }

    public final void setErrorHandler(
            LoadBalancerErrorHandler<? super T, ? super S> errorHandler) {
        this.errorHandler = errorHandler;
    }

    public final boolean isOkToRetryOnAllOperations() {
        return okToRetryOnAllOperations;
    }

    public final void setOkToRetryOnAllOperations(boolean okToRetryOnAllOperations) {
        this.okToRetryOnAllOperations = okToRetryOnAllOperations;
    }
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_7620032_42c3e4c/rev_7620032-42c3e4c/ribbon-core/src/main/java/com/netflix/loadbalancer/BaseLoadBalancer.java;<<<<<<< MINE
||||||| BASE
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.loadbalancer;

import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.List;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReadWriteLock;
import java.util.concurrent.locks.ReentrantReadWriteLock;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.netflix.client.ClientFactory;
import com.netflix.client.IClientConfigAware;
import com.netflix.client.PrimeConnections;
import com.netflix.client.config.CommonClientConfigKey;
import com.netflix.client.config.IClientConfig;
import com.netflix.servo.annotations.DataSourceType;
import com.netflix.servo.annotations.Monitor;
import com.netflix.servo.monitor.Counter;
import com.netflix.servo.monitor.Monitors;
import com.netflix.util.concurrent.ShutdownEnabledTimer;

/**
 * A basic implementation of the load balancer where an arbitrary list of
 * servers can be set as the server pool. A ping can be set to determine the
 * liveness of a server. Internally, this class maintains an "all" server list
 * and an "up" server list and use them depending on what the caller asks for.
 * 
 * @author stonse
 * 
 */
public class BaseLoadBalancer extends AbstractLoadBalancer implements
        PrimeConnections.PrimeConnectionListener, IClientConfigAware {

    private static Logger logger = LoggerFactory
            .getLogger(BaseLoadBalancer.class);
    private final static IRule DEFAULT_RULE = new RoundRobinRule();
    private static final String DEFAULT_NAME = "default";
    private static final String PREFIX = "LoadBalancer_";

    protected IRule rule = DEFAULT_RULE;

    protected IPing ping = null;

    @Monitor(name = PREFIX + "AllServerList", type = DataSourceType.INFORMATIONAL)
    protected volatile List<Server> allServerList = Collections
            .synchronizedList(new ArrayList<Server>());
    @Monitor(name = PREFIX + "UpServerList", type = DataSourceType.INFORMATIONAL)
    protected volatile List<Server> upServerList = Collections
            .synchronizedList(new ArrayList<Server>());

    protected ReadWriteLock allServerLock = new ReentrantReadWriteLock();
    protected ReadWriteLock upServerLock = new ReentrantReadWriteLock();

    protected String name = DEFAULT_NAME;

    protected Timer lbTimer = null;
    protected int pingIntervalSeconds = 10;
    protected int maxTotalPingTimeSeconds = 5;
    protected Comparator<Server> serverComparator = new ServerComparator();

    protected AtomicBoolean pingInProgress = new AtomicBoolean(false);

    protected LoadBalancerStats lbStats;

    private volatile Counter counter;

    private PrimeConnections primeConnections;

    private volatile boolean enablePrimingConnections = false;
    
    private IClientConfig config;

    /**
     * Default constructor which sets name as "default", sets null ping, and
     * {@link RoundRobinRule} as the rule.
     * <p>
     * This constructor is mainly used by {@link ClientFactory}. Calling this
     * constructor must be followed by calling {@link #init()} or
     * {@link #initWithNiwsConfig(IClientConfig)} to complete initialization.
     * This constructor is provided for reflection. When constructing
     * programatically, it is recommended to use other constructors.
     */
    public BaseLoadBalancer() {
        this.name = DEFAULT_NAME;
        this.ping = null;
        setRule(DEFAULT_RULE);
        setupPingTask();
        lbStats = new LoadBalancerStats(DEFAULT_NAME);
        counter = createCounter();
    }

    public BaseLoadBalancer(String lbName, IRule rule, LoadBalancerStats lbStats) {
        this(lbName, rule, lbStats, null);
    }

    public BaseLoadBalancer(IPing ping, IRule rule) {
        this(DEFAULT_NAME, rule, new LoadBalancerStats(DEFAULT_NAME), ping);
    }

    public BaseLoadBalancer(String name, IRule rule, LoadBalancerStats stats,
            IPing ping) {
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  initialized");
        }
        this.name = name;
        this.ping = ping;
        setRule(rule);
        setupPingTask();
        lbStats = stats;
        counter = createCounter();
        init();
    }

    public BaseLoadBalancer(IClientConfig config) {
        initWithNiwsConfig(config);
    }

    @Override
    public void initWithNiwsConfig(IClientConfig clientConfig) {
    	this.config = clientConfig;
        String clientName = clientConfig.getClientName();
        String ruleClassName = (String) clientConfig
                .getProperty(CommonClientConfigKey.NFLoadBalancerRuleClassName);
        String pingClassName = (String) clientConfig
                .getProperty(CommonClientConfigKey.NFLoadBalancerPingClassName);

        IRule rule;
        IPing ping;
        try {
            rule = (IRule) ClientFactory.instantiateInstanceWithClientConfig(
                    ruleClassName, clientConfig);
            ping = (IPing) ClientFactory.instantiateInstanceWithClientConfig(
                    pingClassName, clientConfig);
        } catch (Exception e) {
            throw new RuntimeException("Error initializing load balancer", e);
        }

        this.name = clientName;
        int pingIntervalTime = Integer.parseInt(""
                + clientConfig.getProperty(
                        CommonClientConfigKey.NFLoadBalancerPingInterval,
                        Integer.parseInt("30")));
        int maxTotalPingTime = Integer.parseInt(""
                + clientConfig.getProperty(
                        CommonClientConfigKey.NFLoadBalancerMaxTotalPingTime,
                        Integer.parseInt("2")));

        setPingInterval(pingIntervalTime);
        setMaxTotalPingTime(maxTotalPingTime);

        // cross associate with each other
        // i.e. Rule,Ping meet your container LB
        // LB, these are your Ping and Rule guys ...
        setRule(rule);
        setPing(ping);
        setLoadBalancerStats(new LoadBalancerStats(clientName));
        rule.setLoadBalancer(this);
        if (ping instanceof AbstractLoadBalancerPing) {
            ((AbstractLoadBalancerPing) ping).setLoadBalancer(this);
        }
        logger.info("Client:" + name + " instantiated a LoadBalancer:"
                + toString());
        boolean enablePrimeConnections = false;

        if (clientConfig
                .getProperty(CommonClientConfigKey.EnablePrimeConnections) != null) {
            Boolean bEnablePrimeConnections = Boolean.valueOf(""
                    + clientConfig.getProperty(
                            CommonClientConfigKey.EnablePrimeConnections,
                            "false"));
            enablePrimeConnections = bEnablePrimeConnections.booleanValue();
        }

        if (enablePrimeConnections) {
            this.setEnablePrimingConnections(true);
            PrimeConnections primeConnections = new PrimeConnections(
                    this.getName(), clientConfig);
            this.setPrimeConnections(primeConnections);
        }
        init();
    }

    public IClientConfig getClientConfig() {
    	return config;
    }
    
    private boolean canSkipPing() {
        if (ping == null
                || ping.getClass().getName().equals(DummyPing.class.getName())) {
            // default ping, no need to set up timer
            return true;
        } else {
            return false;
        }
    }

    private void setupPingTask() {
        if (canSkipPing()) {
            return;
        }
        if (lbTimer != null) {
            lbTimer.cancel();
        }
        lbTimer = new ShutdownEnabledTimer("NFLoadBalancer-PingTimer-" + name,
                true);
        lbTimer.schedule(new PingTask(), 0, pingIntervalSeconds * 1000);
        forceQuickPing();
    }

    /**
     * Set the name for the load balancer. This should not be called since name
     * should be immutable after initialization. Calling this method does not
     * guarantee that all other data structures that depend on this name will be
     * changed accordingly.
     */
    void setName(String name) {
        // and register
        this.name = name;
        if (lbStats == null) {
            lbStats = new LoadBalancerStats(name);
        } else {
            lbStats.setName(name);
        }
    }

    public String getName() {
        return name;
    }

    @Override
    public LoadBalancerStats getLoadBalancerStats() {
        return lbStats;
    }

    public void setLoadBalancerStats(LoadBalancerStats lbStats) {
        this.lbStats = lbStats;
    }

    public Lock lockAllServerList(boolean write) {
        Lock aproposLock = write ? allServerLock.writeLock() : allServerLock
                .readLock();
        aproposLock.lock();
        return aproposLock;
    }

    public Lock lockUpServerList(boolean write) {
        Lock aproposLock = write ? upServerLock.writeLock() : upServerLock
                .readLock();
        aproposLock.lock();
        return aproposLock;
    }

    public void setPingInterval(int pingIntervalSeconds) {
        if (pingIntervalSeconds < 1) {
            return;
        }

        this.pingIntervalSeconds = pingIntervalSeconds;
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  pingIntervalSeconds set to "
                    + this.pingIntervalSeconds);
        }
        setupPingTask(); // since ping data changed
    }

    public int getPingInterval() {
        return pingIntervalSeconds;
    }

    /*
     * Maximum time allowed for the ping cycle
     */
    public void setMaxTotalPingTime(int maxTotalPingTimeSeconds) {
        if (maxTotalPingTimeSeconds < 1) {
            return;
        }
        this.maxTotalPingTimeSeconds = maxTotalPingTimeSeconds;
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer: maxTotalPingTime set to "
                    + this.maxTotalPingTimeSeconds);
        }

    }

    public int getMaxTotalPingTime() {
        return maxTotalPingTimeSeconds;
    }

    public IPing getPing() {
        return ping;
    }

    public IRule getRule() {
        return rule;
    }

    public boolean isPingInProgress() {
        return pingInProgress.get();
    }

    /* Specify the object which is used to send pings. */

    public void setPing(IPing ping) {
        if (ping != null) {
            if (!ping.equals(this.ping)) {
                this.ping = ping;
                setupPingTask(); // since ping data changed
            }
        } else {
            this.ping = null;
            // cancel the timer task
            lbTimer.cancel();
        }
    }

    /* Ignore null rules */

    public void setRule(IRule rule) {
        if (rule != null) {
            this.rule = rule;
        } else {
            /* default rule */
            this.rule = new RoundRobinRule();
        }
        if (this.rule.getLoadBalancer() != this) {
            this.rule.setLoadBalancer(this);
        }
    }

    /**
     * get the count of servers.
     * 
     * @param onlyAvailable
     *            if true, return only up servers.
     */
    public int getServerCount(boolean onlyAvailable) {
        if (onlyAvailable) {
            return upServerList.size();
        } else {
            return allServerList.size();
        }
    }

    /**
     * Add a server to the 'allServer' list; does not verify uniqueness, so you
     * could give a server a greater share by adding it more than once.
     */
    public void addServer(Server newServer) {
        if (newServer != null) {
            try {
                ArrayList<Server> newList = new ArrayList<Server>();

                newList.addAll(allServerList);
                newList.add(newServer);
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding a newServer", e);
            }
        }
    }

    /**
     * Add a list of servers to the 'allServer' list; does not verify
     * uniqueness, so you could give a server a greater share by adding it more
     * than once
     */
    @Override
    public void addServers(List<Server> newServers) {
        if (newServers != null && newServers.size() > 0) {
            try {
                ArrayList<Server> newList = new ArrayList<Server>();
                newList.addAll(allServerList);
                newList.addAll(newServers);
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding Servers", e);
            }
        }
    }

    /*
     * Add a list of servers to the 'allServer' list; does not verify
     * uniqueness, so you could give a server a greater share by adding it more
     * than once USED by Test Cases only for legacy reason. DO NOT USE!!
     */
    void addServers(Object[] newServers) {
        if ((newServers != null) && (newServers.length > 0)) {

            try {
                ArrayList<Server> newList = new ArrayList<Server>();
                newList.addAll(allServerList);

                for (Object server : newServers) {
                    if (server != null) {
                        if (server instanceof String) {
                            server = new Server((String) server);
                        }
                        if (server instanceof Server) {
                            newList.add((Server) server);
                        }
                    }
                }
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding Servers", e);
            }
        }
    }

    /**
     * Set the list of servers used as the server pool. This overrides existing
     * server list.
     */
    public void setServersList(List lsrv) {
        Lock writeLock = allServerLock.writeLock();
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  clearing server list (SET op)");
        }
        ArrayList<Server> newServers = new ArrayList<Server>();
        writeLock.lock();
        try {
            ArrayList<Server> allServers = new ArrayList<Server>();
            for (Object server : lsrv) {
                if (server == null) {
                    continue;
                }

                if (server instanceof String) {
                    server = new Server((String) server);
                }

                if (server instanceof Server) {
                    if (logger.isDebugEnabled()) {
                        logger.debug("LoadBalancer:  addServer ["
                                + ((Server) server).getId() + "]");
                    }
                    allServers.add((Server) server);
                } else {
                    throw new IllegalArgumentException(
                            "Type String or Server expected, instead found:"
                                    + server.getClass());
                }

            }
            boolean listChanged = false;
            if (!allServerList.equals(allServers)) {
                listChanged = true;
            }
            if (isEnablePrimingConnections()) {
                for (Server server : allServers) {
                    if (!allServerList.contains(server)) {
                        server.setReadyToServe(false);
                        newServers.add((Server) server);
                    }
                }
                if (primeConnections != null) {
                    primeConnections.primeConnectionsAsync(newServers, this);
                }
            }
            // This will reset readyToServe flag to true on all servers
            // regardless whether
            // previous priming connections are success or not
            allServerList = allServers;
            if (canSkipPing()) {
                for (Server s : allServerList) {
                    s.setAlive(true);
                }
                upServerList = allServerList;
            } else if (listChanged) {
                forceQuickPing();
            }
        } finally {
            writeLock.unlock();
        }
    }

    /* List in string form. SETS, does not add. */
    void setServers(String srvString) {
        if (srvString != null) {

            try {
                String[] serverArr = srvString.split(",");
                ArrayList<Server> newList = new ArrayList<Server>();

                for (String serverString : serverArr) {
                    if (serverString != null) {
                        serverString = serverString.trim();
                        if (serverString.length() > 0) {
                            Server svr = new Server(serverString);
                            newList.add(svr);
                        }
                    }
                }
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding Servers", e);
            }
        }
    }

    /**
     * return the server
     * 
     * @param index
     * @param availableOnly
     */
    public Server getServerByIndex(int index, boolean availableOnly) {
        try {
            return (availableOnly ? upServerList.get(index) : allServerList
                    .get(index));
        } catch (Exception e) {
            return null;
        }
    }

    @Override
    public List<Server> getServerList(boolean availableOnly) {
        return (availableOnly ? Collections.unmodifiableList(upServerList) : 
        	Collections.unmodifiableList(allServerList));
    }

    @Override
    public List<Server> getServerList(ServerGroup serverGroup) {
        switch (serverGroup) {
        case ALL:
            return allServerList;
        case STATUS_UP:
            return upServerList;
        case STATUS_NOT_UP:
            ArrayList<Server> notAvailableServers = new ArrayList<Server>(
                    allServerList);
            ArrayList<Server> upServers = new ArrayList<Server>(upServerList);
            notAvailableServers.removeAll(upServers);
            return notAvailableServers;
        }
        return new ArrayList<Server>();
    }

    public void cancelPingTask() {
        if (lbTimer != null) {
            lbTimer.cancel();
        }
    }

    /**
     * TimerTask that keeps runs every X seconds to check the status of each
     * server/node in the Server List
     * 
     * @author stonse
     * 
     */
    class PingTask extends TimerTask {
        public void run() {
            Pinger ping = new Pinger();
            try {
                ping.runPinger();
            } catch (Throwable t) {
                logger.error("Throwable caught while running extends for "
                        + name, t);
            }
        }
    }

    /**
     * Class that contains the mechanism to "ping" all the instances
     * 
     * @author stonse
     *
     */
    class Pinger {

        public void runPinger() {

            if (pingInProgress.get()) {
                return; // Ping in progress - nothing to do
            } else {
                pingInProgress.set(true);
            }

            // we are "in" - we get to Ping

            Object[] allServers = null;
            boolean[] results = null;

            Lock allLock = null;
            Lock upLock = null;

            try {
                /*
                 * The readLock should be free unless an addServer operation is
                 * going on...
                 */
                allLock = allServerLock.readLock();
                allLock.lock();
                allServers = allServerList.toArray();
                allLock.unlock();

                int numCandidates = allServers.length;
                results = new boolean[numCandidates];

                if (logger.isDebugEnabled()) {
                    logger.debug("LoadBalancer:  PingTask executing ["
                            + numCandidates + "] servers configured");
                }

                for (int i = 0; i < numCandidates; i++) {
                    results[i] = false; /* Default answer is DEAD. */
                    try {
                        // NOTE: IFF we were doing a real ping
                        // assuming we had a large set of servers (say 15)
                        // the logic below will run them serially
                        // hence taking 15 times the amount of time it takes
                        // to ping each server
                        // A better method would be to put this in an executor
                        // pool
                        // But, at the time of this writing, we dont REALLY
                        // use a Real Ping (its mostly in memory eureka call)
                        // hence we can afford to simplify this design and run
                        // this
                        // serially
                        if (ping != null) {
                            results[i] = ping.isAlive((Server) allServers[i]);
                        }
                    } catch (Throwable t) {
                        logger.error("Exception while pinging Server:"
                                + allServers[i], t);
                    }
                }

                ArrayList<Server> newUpList = new ArrayList<Server>();

                for (int i = 0; i < numCandidates; i++) {
                    boolean isAlive = results[i];
                    Server svr = (Server) allServers[i];
                    boolean oldIsAlive = svr.isAlive();

                    svr.setAlive(isAlive);

                    if (oldIsAlive != isAlive && logger.isDebugEnabled()) {
                        logger.debug("LoadBalancer:  Server [" + svr.getId()
                                + "] status changed to "
                                + (isAlive ? "ALIVE" : "DEAD"));
                    }

                    if (isAlive) {
                        newUpList.add(svr);
                    }
                }
                // System.out.println(count + " servers alive");
                upLock = upServerLock.writeLock();
                upLock.lock();
                upServerList = newUpList;
                upLock.unlock();
            } catch (Throwable t) {
                logger.error("Throwable caught while running the Pinger-"
                        + name, t);
            } finally {
                pingInProgress.set(false);
            }
        }
    }

    private final Counter createCounter() {
        return Monitors.newCounter("LoadBalancer_ChooseServer");
    }

    /*
     * Get the alive server dedicated to key
     * 
     * @return the dedicated server
     */
    public Server chooseServer(Object key) {
        if (counter == null) {
            counter = createCounter();
        }
        counter.increment();
        if (rule == null) {
            return null;
        } else {
            try {
                return rule.choose(key);
            } catch (Throwable t) {
                return null;
            }
        }
    }

    /* Returns either null, or "server:port/servlet" */
    public String choose(Object key) {
        if (rule == null) {
            return null;
        } else {
            try {
                Server svr = rule.choose(key);
                return ((svr == null) ? null : svr.getId());
            } catch (Throwable t) {
                return null;
            }
        }
    }

    public void markServerDown(Server server) {
        if (server == null) {
            return;
        }

        if (!server.isAlive()) {
            return;
        }

        logger.error("LoadBalancer:  markServerDown called on ["
                + server.getId() + "]");
        server.setAlive(false);
        // forceQuickPing();
    }

    public void markServerDown(String id) {
        boolean triggered = false;

        id = Server.normalizeId(id);

        if (id == null) {
            return;
        }

        Lock writeLock = upServerLock.writeLock();

        try {

            for (Server svr : upServerList) {
                if (svr.isAlive() && (svr.getId().equals(id))) {
                    triggered = true;
                    svr.setAlive(false);
                }
            }

            if (triggered) {
                logger.error("LoadBalancer:  markServerDown called on [" + id
                        + "]");
            }

        } finally {
            try {
                writeLock.unlock();
            } catch (Exception e) { // NOPMD
            }
        }
    }

    /*
     * Force an immediate ping, if we're not currently pinging and don't have a
     * quick-ping already scheduled.
     */
    public void forceQuickPing() {
        if (canSkipPing()) {
            return;
        }
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  forceQuickPing invoked");
        }
        Pinger ping = new Pinger();
        try {
            ping.runPinger();
        } catch (Throwable t) {
            logger.error("Throwable caught while running forceQuickPing() for "
                    + name, t);
        }
    }

    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("{NFLoadBalancer:name=").append(this.getName())
                .append(",current list of Servers=").append(this.allServerList)
                .append(",Load balancer stats=")
                .append(this.lbStats.toString()).append("}");
        return sb.toString();
    }

    /**
     * Register with monitors and start priming connections if it is set.
     */
    protected void init() {
        Monitors.registerObject("LoadBalancer_" + name, this);
        // register the rule as it contains metric for available servers count
        Monitors.registerObject("Rule_" + name, this.getRule());
        if (enablePrimingConnections && primeConnections != null) {
            primeConnections.primeConnections(getServerList(true));
        }
    }

    public final PrimeConnections getPrimeConnections() {
        return primeConnections;
    }

    public final void setPrimeConnections(PrimeConnections primeConnections) {
        this.primeConnections = primeConnections;
    }

    @Override
    public void primeCompleted(Server s, Throwable lastException) {
        s.setReadyToServe(true);
    }

    public boolean isEnablePrimingConnections() {
        return enablePrimingConnections;
    }

    public final void setEnablePrimingConnections(
            boolean enablePrimingConnections) {
        this.enablePrimingConnections = enablePrimingConnections;
    }
}=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.loadbalancer;

import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.List;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.CopyOnWriteArrayList;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReadWriteLock;
import java.util.concurrent.locks.ReentrantReadWriteLock;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.collect.ImmutableList;
import com.netflix.client.ClientFactory;
import com.netflix.client.IClientConfigAware;
import com.netflix.client.PrimeConnections;
import com.netflix.client.config.CommonClientConfigKey;
import com.netflix.client.config.IClientConfig;
import com.netflix.servo.annotations.DataSourceType;
import com.netflix.servo.annotations.Monitor;
import com.netflix.servo.monitor.Counter;
import com.netflix.servo.monitor.Monitors;
import com.netflix.util.concurrent.ShutdownEnabledTimer;

/**
 * A basic implementation of the load balancer where an arbitrary list of
 * servers can be set as the server pool. A ping can be set to determine the
 * liveness of a server. Internally, this class maintains an "all" server list
 * and an "up" server list and use them depending on what the caller asks for.
 * 
 * @author stonse
 * 
 */
public class BaseLoadBalancer extends AbstractLoadBalancer implements
        PrimeConnections.PrimeConnectionListener, IClientConfigAware {

    private static Logger logger = LoggerFactory
            .getLogger(BaseLoadBalancer.class);
    private final static IRule DEFAULT_RULE = new RoundRobinRule();
    private static final String DEFAULT_NAME = "default";
    private static final String PREFIX = "LoadBalancer_";

    protected IRule rule = DEFAULT_RULE;

    protected IPing ping = null;

    @Monitor(name = PREFIX + "AllServerList", type = DataSourceType.INFORMATIONAL)
    protected volatile List<Server> allServerList = Collections
            .synchronizedList(new ArrayList<Server>());
    @Monitor(name = PREFIX + "UpServerList", type = DataSourceType.INFORMATIONAL)
    protected volatile List<Server> upServerList = Collections
            .synchronizedList(new ArrayList<Server>());

    protected ReadWriteLock allServerLock = new ReentrantReadWriteLock();
    protected ReadWriteLock upServerLock = new ReentrantReadWriteLock();

    protected String name = DEFAULT_NAME;

    protected Timer lbTimer = null;
    protected int pingIntervalSeconds = 10;
    protected int maxTotalPingTimeSeconds = 5;
    protected Comparator<Server> serverComparator = new ServerComparator();

    protected AtomicBoolean pingInProgress = new AtomicBoolean(false);

    protected LoadBalancerStats lbStats;

    private volatile Counter counter;

    private PrimeConnections primeConnections;

    private volatile boolean enablePrimingConnections = false;
    
    private IClientConfig config;
    
    private List<ServerListChangeListener> changeListeners = new CopyOnWriteArrayList<ServerListChangeListener>();

    /**
     * Default constructor which sets name as "default", sets null ping, and
     * {@link RoundRobinRule} as the rule.
     * <p>
     * This constructor is mainly used by {@link ClientFactory}. Calling this
     * constructor must be followed by calling {@link #init()} or
     * {@link #initWithNiwsConfig(IClientConfig)} to complete initialization.
     * This constructor is provided for reflection. When constructing
     * programatically, it is recommended to use other constructors.
     */
    public BaseLoadBalancer() {
        this.name = DEFAULT_NAME;
        this.ping = null;
        setRule(DEFAULT_RULE);
        setupPingTask();
        lbStats = new LoadBalancerStats(DEFAULT_NAME);
        counter = createCounter();
    }

    public BaseLoadBalancer(String lbName, IRule rule, LoadBalancerStats lbStats) {
        this(lbName, rule, lbStats, null);
    }

    public BaseLoadBalancer(IPing ping, IRule rule) {
        this(DEFAULT_NAME, rule, new LoadBalancerStats(DEFAULT_NAME), ping);
    }

    public BaseLoadBalancer(String name, IRule rule, LoadBalancerStats stats,
            IPing ping) {
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  initialized");
        }
        this.name = name;
        this.ping = ping;
        setRule(rule);
        setupPingTask();
        lbStats = stats;
        counter = createCounter();
        init();
    }

    public BaseLoadBalancer(IClientConfig config) {
        initWithNiwsConfig(config);
    }

    @Override
    public void initWithNiwsConfig(IClientConfig clientConfig) {
    	this.config = clientConfig;
        String clientName = clientConfig.getClientName();
        String ruleClassName = (String) clientConfig
                .getProperty(CommonClientConfigKey.NFLoadBalancerRuleClassName);
        String pingClassName = (String) clientConfig
                .getProperty(CommonClientConfigKey.NFLoadBalancerPingClassName);

        IRule rule;
        IPing ping;
        try {
            rule = (IRule) ClientFactory.instantiateInstanceWithClientConfig(
                    ruleClassName, clientConfig);
            ping = (IPing) ClientFactory.instantiateInstanceWithClientConfig(
                    pingClassName, clientConfig);
        } catch (Exception e) {
            throw new RuntimeException("Error initializing load balancer", e);
        }

        this.name = clientName;
        int pingIntervalTime = Integer.parseInt(""
                + clientConfig.getProperty(
                        CommonClientConfigKey.NFLoadBalancerPingInterval,
                        Integer.parseInt("30")));
        int maxTotalPingTime = Integer.parseInt(""
                + clientConfig.getProperty(
                        CommonClientConfigKey.NFLoadBalancerMaxTotalPingTime,
                        Integer.parseInt("2")));

        setPingInterval(pingIntervalTime);
        setMaxTotalPingTime(maxTotalPingTime);

        // cross associate with each other
        // i.e. Rule,Ping meet your container LB
        // LB, these are your Ping and Rule guys ...
        setRule(rule);
        setPing(ping);
        setLoadBalancerStats(new LoadBalancerStats(clientName));
        rule.setLoadBalancer(this);
        if (ping instanceof AbstractLoadBalancerPing) {
            ((AbstractLoadBalancerPing) ping).setLoadBalancer(this);
        }
        logger.info("Client:" + name + " instantiated a LoadBalancer:"
                + toString());
        boolean enablePrimeConnections = false;

        if (clientConfig
                .getProperty(CommonClientConfigKey.EnablePrimeConnections) != null) {
            Boolean bEnablePrimeConnections = Boolean.valueOf(""
                    + clientConfig.getProperty(
                            CommonClientConfigKey.EnablePrimeConnections,
                            "false"));
            enablePrimeConnections = bEnablePrimeConnections.booleanValue();
        }

        if (enablePrimeConnections) {
            this.setEnablePrimingConnections(true);
            PrimeConnections primeConnections = new PrimeConnections(
                    this.getName(), clientConfig);
            this.setPrimeConnections(primeConnections);
        }
        init();
    }

    public void addServerListChangeListener(ServerListChangeListener listener) {
        changeListeners.add(listener);
    }
    
    public void removeServerListChangeListener(ServerListChangeListener listener) {
        changeListeners.remove(listener);
    }

    public IClientConfig getClientConfig() {
    	return config;
    }
    
    private boolean canSkipPing() {
        if (ping == null
                || ping.getClass().getName().equals(DummyPing.class.getName())) {
            // default ping, no need to set up timer
            return true;
        } else {
            return false;
        }
    }

    private void setupPingTask() {
        if (canSkipPing()) {
            return;
        }
        if (lbTimer != null) {
            lbTimer.cancel();
        }
        lbTimer = new ShutdownEnabledTimer("NFLoadBalancer-PingTimer-" + name,
                true);
        lbTimer.schedule(new PingTask(), 0, pingIntervalSeconds * 1000);
        forceQuickPing();
    }

    /**
     * Set the name for the load balancer. This should not be called since name
     * should be immutable after initialization. Calling this method does not
     * guarantee that all other data structures that depend on this name will be
     * changed accordingly.
     */
    void setName(String name) {
        // and register
        this.name = name;
        if (lbStats == null) {
            lbStats = new LoadBalancerStats(name);
        } else {
            lbStats.setName(name);
        }
    }

    public String getName() {
        return name;
    }

    @Override
    public LoadBalancerStats getLoadBalancerStats() {
        return lbStats;
    }

    public void setLoadBalancerStats(LoadBalancerStats lbStats) {
        this.lbStats = lbStats;
    }

    public Lock lockAllServerList(boolean write) {
        Lock aproposLock = write ? allServerLock.writeLock() : allServerLock
                .readLock();
        aproposLock.lock();
        return aproposLock;
    }

    public Lock lockUpServerList(boolean write) {
        Lock aproposLock = write ? upServerLock.writeLock() : upServerLock
                .readLock();
        aproposLock.lock();
        return aproposLock;
    }

    public void setPingInterval(int pingIntervalSeconds) {
        if (pingIntervalSeconds < 1) {
            return;
        }

        this.pingIntervalSeconds = pingIntervalSeconds;
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  pingIntervalSeconds set to "
                    + this.pingIntervalSeconds);
        }
        setupPingTask(); // since ping data changed
    }

    public int getPingInterval() {
        return pingIntervalSeconds;
    }

    /*
     * Maximum time allowed for the ping cycle
     */
    public void setMaxTotalPingTime(int maxTotalPingTimeSeconds) {
        if (maxTotalPingTimeSeconds < 1) {
            return;
        }
        this.maxTotalPingTimeSeconds = maxTotalPingTimeSeconds;
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer: maxTotalPingTime set to "
                    + this.maxTotalPingTimeSeconds);
        }

    }

    public int getMaxTotalPingTime() {
        return maxTotalPingTimeSeconds;
    }

    public IPing getPing() {
        return ping;
    }

    public IRule getRule() {
        return rule;
    }

    public boolean isPingInProgress() {
        return pingInProgress.get();
    }

    /* Specify the object which is used to send pings. */

    public void setPing(IPing ping) {
        if (ping != null) {
            if (!ping.equals(this.ping)) {
                this.ping = ping;
                setupPingTask(); // since ping data changed
            }
        } else {
            this.ping = null;
            // cancel the timer task
            lbTimer.cancel();
        }
    }

    /* Ignore null rules */

    public void setRule(IRule rule) {
        if (rule != null) {
            this.rule = rule;
        } else {
            /* default rule */
            this.rule = new RoundRobinRule();
        }
        if (this.rule.getLoadBalancer() != this) {
            this.rule.setLoadBalancer(this);
        }
    }

    /**
     * get the count of servers.
     * 
     * @param onlyAvailable
     *            if true, return only up servers.
     */
    public int getServerCount(boolean onlyAvailable) {
        if (onlyAvailable) {
            return upServerList.size();
        } else {
            return allServerList.size();
        }
    }

    /**
     * Add a server to the 'allServer' list; does not verify uniqueness, so you
     * could give a server a greater share by adding it more than once.
     */
    public void addServer(Server newServer) {
        if (newServer != null) {
            try {
                ArrayList<Server> newList = new ArrayList<Server>();

                newList.addAll(allServerList);
                newList.add(newServer);
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding a newServer", e);
            }
        }
    }

    /**
     * Add a list of servers to the 'allServer' list; does not verify
     * uniqueness, so you could give a server a greater share by adding it more
     * than once
     */
    @Override
    public void addServers(List<Server> newServers) {
        if (newServers != null && newServers.size() > 0) {
            try {
                ArrayList<Server> newList = new ArrayList<Server>();
                newList.addAll(allServerList);
                newList.addAll(newServers);
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding Servers", e);
            }
        }
    }

    /*
     * Add a list of servers to the 'allServer' list; does not verify
     * uniqueness, so you could give a server a greater share by adding it more
     * than once USED by Test Cases only for legacy reason. DO NOT USE!!
     */
    void addServers(Object[] newServers) {
        if ((newServers != null) && (newServers.length > 0)) {

            try {
                ArrayList<Server> newList = new ArrayList<Server>();
                newList.addAll(allServerList);

                for (Object server : newServers) {
                    if (server != null) {
                        if (server instanceof String) {
                            server = new Server((String) server);
                        }
                        if (server instanceof Server) {
                            newList.add((Server) server);
                        }
                    }
                }
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding Servers", e);
            }
        }
    }

    /**
     * Set the list of servers used as the server pool. This overrides existing
     * server list.
     */
    public void setServersList(List lsrv) {
        Lock writeLock = allServerLock.writeLock();
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  clearing server list (SET op)");
        }
        ArrayList<Server> newServers = new ArrayList<Server>();
        writeLock.lock();
        try {
            ArrayList<Server> allServers = new ArrayList<Server>();
            for (Object server : lsrv) {
                if (server == null) {
                    continue;
                }

                if (server instanceof String) {
                    server = new Server((String) server);
                }

                if (server instanceof Server) {
                    if (logger.isDebugEnabled()) {
                        logger.debug("LoadBalancer:  addServer ["
                                + ((Server) server).getId() + "]");
                    }
                    allServers.add((Server) server);
                } else {
                    throw new IllegalArgumentException(
                            "Type String or Server expected, instead found:"
                                    + server.getClass());
                }

            }
            boolean listChanged = false;
            if (!allServerList.equals(allServers)) {
                listChanged = true;
                if (changeListeners != null && changeListeners.size() > 0) {
                   List<Server> oldList = ImmutableList.copyOf(allServerList);
                   List<Server> newList = ImmutableList.copyOf(allServers);                   
                   for (ServerListChangeListener l: changeListeners) {
                       try {
                           l.serverListChanged(oldList, newList);
                       } catch (Throwable e) {
                           logger.error("Error invoking server list change listener", e);
                       }
                   }
                }
            }
            if (isEnablePrimingConnections()) {
                for (Server server : allServers) {
                    if (!allServerList.contains(server)) {
                        server.setReadyToServe(false);
                        newServers.add((Server) server);
                    }
                }
                if (primeConnections != null) {
                    primeConnections.primeConnectionsAsync(newServers, this);
                }
            }
            // This will reset readyToServe flag to true on all servers
            // regardless whether
            // previous priming connections are success or not
            allServerList = allServers;
            if (canSkipPing()) {
                for (Server s : allServerList) {
                    s.setAlive(true);
                }
                upServerList = allServerList;
            } else if (listChanged) {
                forceQuickPing();
            }
        } finally {
            writeLock.unlock();
        }
    }

    /* List in string form. SETS, does not add. */
    void setServers(String srvString) {
        if (srvString != null) {

            try {
                String[] serverArr = srvString.split(",");
                ArrayList<Server> newList = new ArrayList<Server>();

                for (String serverString : serverArr) {
                    if (serverString != null) {
                        serverString = serverString.trim();
                        if (serverString.length() > 0) {
                            Server svr = new Server(serverString);
                            newList.add(svr);
                        }
                    }
                }
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding Servers", e);
            }
        }
    }

    /**
     * return the server
     * 
     * @param index
     * @param availableOnly
     */
    public Server getServerByIndex(int index, boolean availableOnly) {
        try {
            return (availableOnly ? upServerList.get(index) : allServerList
                    .get(index));
        } catch (Exception e) {
            return null;
        }
    }

    @Override
    public List<Server> getServerList(boolean availableOnly) {
        return (availableOnly ? Collections.unmodifiableList(upServerList) : 
        	Collections.unmodifiableList(allServerList));
    }

    @Override
    public List<Server> getServerList(ServerGroup serverGroup) {
        switch (serverGroup) {
        case ALL:
            return allServerList;
        case STATUS_UP:
            return upServerList;
        case STATUS_NOT_UP:
            ArrayList<Server> notAvailableServers = new ArrayList<Server>(
                    allServerList);
            ArrayList<Server> upServers = new ArrayList<Server>(upServerList);
            notAvailableServers.removeAll(upServers);
            return notAvailableServers;
        }
        return new ArrayList<Server>();
    }

    public void cancelPingTask() {
        if (lbTimer != null) {
            lbTimer.cancel();
        }
    }

    /**
     * TimerTask that keeps runs every X seconds to check the status of each
     * server/node in the Server List
     * 
     * @author stonse
     * 
     */
    class PingTask extends TimerTask {
        public void run() {
            Pinger ping = new Pinger();
            try {
                ping.runPinger();
            } catch (Throwable t) {
                logger.error("Throwable caught while running extends for "
                        + name, t);
            }
        }
    }

    /**
     * Class that contains the mechanism to "ping" all the instances
     * 
     * @author stonse
     *
     */
    class Pinger {

        public void runPinger() {

            if (pingInProgress.get()) {
                return; // Ping in progress - nothing to do
            } else {
                pingInProgress.set(true);
            }

            // we are "in" - we get to Ping

            Object[] allServers = null;
            boolean[] results = null;

            Lock allLock = null;
            Lock upLock = null;

            try {
                /*
                 * The readLock should be free unless an addServer operation is
                 * going on...
                 */
                allLock = allServerLock.readLock();
                allLock.lock();
                allServers = allServerList.toArray();
                allLock.unlock();

                int numCandidates = allServers.length;
                results = new boolean[numCandidates];

                if (logger.isDebugEnabled()) {
                    logger.debug("LoadBalancer:  PingTask executing ["
                            + numCandidates + "] servers configured");
                }

                for (int i = 0; i < numCandidates; i++) {
                    results[i] = false; /* Default answer is DEAD. */
                    try {
                        // NOTE: IFF we were doing a real ping
                        // assuming we had a large set of servers (say 15)
                        // the logic below will run them serially
                        // hence taking 15 times the amount of time it takes
                        // to ping each server
                        // A better method would be to put this in an executor
                        // pool
                        // But, at the time of this writing, we dont REALLY
                        // use a Real Ping (its mostly in memory eureka call)
                        // hence we can afford to simplify this design and run
                        // this
                        // serially
                        if (ping != null) {
                            results[i] = ping.isAlive((Server) allServers[i]);
                        }
                    } catch (Throwable t) {
                        logger.error("Exception while pinging Server:"
                                + allServers[i], t);
                    }
                }

                ArrayList<Server> newUpList = new ArrayList<Server>();

                for (int i = 0; i < numCandidates; i++) {
                    boolean isAlive = results[i];
                    Server svr = (Server) allServers[i];
                    boolean oldIsAlive = svr.isAlive();

                    svr.setAlive(isAlive);

                    if (oldIsAlive != isAlive && logger.isDebugEnabled()) {
                        logger.debug("LoadBalancer:  Server [" + svr.getId()
                                + "] status changed to "
                                + (isAlive ? "ALIVE" : "DEAD"));
                    }

                    if (isAlive) {
                        newUpList.add(svr);
                    }
                }
                // System.out.println(count + " servers alive");
                upLock = upServerLock.writeLock();
                upLock.lock();
                upServerList = newUpList;
                upLock.unlock();
            } catch (Throwable t) {
                logger.error("Throwable caught while running the Pinger-"
                        + name, t);
            } finally {
                pingInProgress.set(false);
            }
        }
    }

    private final Counter createCounter() {
        return Monitors.newCounter("LoadBalancer_ChooseServer");
    }

    /*
     * Get the alive server dedicated to key
     * 
     * @return the dedicated server
     */
    public Server chooseServer(Object key) {
        if (counter == null) {
            counter = createCounter();
        }
        counter.increment();
        if (rule == null) {
            return null;
        } else {
            try {
                return rule.choose(key);
            } catch (Throwable t) {
                return null;
            }
        }
    }

    /* Returns either null, or "server:port/servlet" */
    public String choose(Object key) {
        if (rule == null) {
            return null;
        } else {
            try {
                Server svr = rule.choose(key);
                return ((svr == null) ? null : svr.getId());
            } catch (Throwable t) {
                return null;
            }
        }
    }

    public void markServerDown(Server server) {
        if (server == null) {
            return;
        }

        if (!server.isAlive()) {
            return;
        }

        logger.error("LoadBalancer:  markServerDown called on ["
                + server.getId() + "]");
        server.setAlive(false);
        // forceQuickPing();
    }

    public void markServerDown(String id) {
        boolean triggered = false;

        id = Server.normalizeId(id);

        if (id == null) {
            return;
        }

        Lock writeLock = upServerLock.writeLock();

        try {

            for (Server svr : upServerList) {
                if (svr.isAlive() && (svr.getId().equals(id))) {
                    triggered = true;
                    svr.setAlive(false);
                }
            }

            if (triggered) {
                logger.error("LoadBalancer:  markServerDown called on [" + id
                        + "]");
            }

        } finally {
            try {
                writeLock.unlock();
            } catch (Exception e) { // NOPMD
            }
        }
    }

    /*
     * Force an immediate ping, if we're not currently pinging and don't have a
     * quick-ping already scheduled.
     */
    public void forceQuickPing() {
        if (canSkipPing()) {
            return;
        }
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  forceQuickPing invoked");
        }
        Pinger ping = new Pinger();
        try {
            ping.runPinger();
        } catch (Throwable t) {
            logger.error("Throwable caught while running forceQuickPing() for "
                    + name, t);
        }
    }

    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("{NFLoadBalancer:name=").append(this.getName())
                .append(",current list of Servers=").append(this.allServerList)
                .append(",Load balancer stats=")
                .append(this.lbStats.toString()).append("}");
        return sb.toString();
    }

    /**
     * Register with monitors and start priming connections if it is set.
     */
    protected void init() {
        Monitors.registerObject("LoadBalancer_" + name, this);
        // register the rule as it contains metric for available servers count
        Monitors.registerObject("Rule_" + name, this.getRule());
        if (enablePrimingConnections && primeConnections != null) {
            primeConnections.primeConnections(getServerList(true));
        }
    }

    public final PrimeConnections getPrimeConnections() {
        return primeConnections;
    }

    public final void setPrimeConnections(PrimeConnections primeConnections) {
        this.primeConnections = primeConnections;
    }

    @Override
    public void primeCompleted(Server s, Throwable lastException) {
        s.setReadyToServe(true);
    }

    public boolean isEnablePrimingConnections() {
        return enablePrimingConnections;
    }

    public final void setEnablePrimingConnections(
            boolean enablePrimingConnections) {
        this.enablePrimingConnections = enablePrimingConnections;
    }
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_7620032_42c3e4c/rev_7620032-42c3e4c/ribbon-core/src/main/java/com/netflix/loadbalancer/DynamicServerListLoadBalancer.java;<<<<<<< MINE
||||||| BASE
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.loadbalancer;

import java.util.Date;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ScheduledThreadPoolExecutor;
import java.util.concurrent.ThreadFactory;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicLong;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.netflix.client.ClientFactory;
import com.netflix.client.config.CommonClientConfigKey;
import com.netflix.client.config.DefaultClientConfigImpl;
import com.netflix.client.config.IClientConfig;
import com.netflix.servo.annotations.DataSourceType;
import com.netflix.servo.annotations.Monitor;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.util.concurrent.ThreadFactoryBuilder;

/**
 * A LoadBalancer that has the capabilities to obtain the candidate list of
 * servers using a dynamic source. i.e. The list of servers can potentially be
 * changed at Runtime. It also contains facilities wherein the list of servers
 * can be passed through a Filter criteria to filter out servers that do not
 * meet the desired criteria.
 * 
 * @author stonse
 * 
 */
public class DynamicServerListLoadBalancer<T extends Server> extends
        BaseLoadBalancer {
    private static final Logger LOGGER = LoggerFactory
            .getLogger(DynamicServerListLoadBalancer.class);

    boolean isSecure = false;
    boolean useTunnel = false;
    private Thread _shutdownThread;

    // to keep track of modification of server lists
    protected AtomicBoolean serverListUpdateInProgress = new AtomicBoolean(
            false);

    private static long LISTOFSERVERS_CACHE_UPDATE_DELAY = 1000; // msecs;
    private static long LISTOFSERVERS_CACHE_REPEAT_INTERVAL = 30 * 1000; // msecs;
                                                                         // //
                                                                         // every
                                                                         // 30
                                                                         // secs

    private ScheduledThreadPoolExecutor _serverListRefreshExecutor = null;

    private long refeshIntervalMills = LISTOFSERVERS_CACHE_REPEAT_INTERVAL;

    volatile ServerList<T> serverListImpl;

    volatile ServerListFilter<T> filter;
    
    private AtomicLong lastUpdated = new AtomicLong(System.currentTimeMillis());
    
    protected volatile boolean serverRefreshEnabled = false;

    public DynamicServerListLoadBalancer() {
        super();
    }

    public DynamicServerListLoadBalancer(IClientConfig niwsClientConfig) {
        initWithNiwsConfig(niwsClientConfig);
    }

    @Override
    public void initWithNiwsConfig(IClientConfig clientConfig) {
        try {
            super.initWithNiwsConfig(clientConfig);
            String niwsServerListClassName = clientConfig.getProperty(
                    CommonClientConfigKey.NIWSServerListClassName,
                    DefaultClientConfigImpl.DEFAULT_SEVER_LIST_CLASS)
                    .toString();

            ServerList<T> niwsServerListImpl = (ServerList<T>) ClientFactory
                    .instantiateInstanceWithClientConfig(
                            niwsServerListClassName, clientConfig);
            this.serverListImpl = niwsServerListImpl;

            if (niwsServerListImpl instanceof AbstractServerList) {
                AbstractServerListFilter<T> niwsFilter = ((AbstractServerList) niwsServerListImpl)
                        .getFilterImpl(clientConfig);
                niwsFilter.setLoadBalancerStats(getLoadBalancerStats());
                this.filter = niwsFilter;
            }

            refeshIntervalMills = Integer.valueOf(clientConfig.getProperty(
                    CommonClientConfigKey.ServerListRefreshInterval,
                    LISTOFSERVERS_CACHE_REPEAT_INTERVAL).toString());

            boolean primeConnection = this.isEnablePrimingConnections();
            // turn this off to avoid duplicated asynchronous priming done in BaseLoadBalancer.setServerList()
            this.setEnablePrimingConnections(false);
            enableAndInitLearnNewServersFeature();

            updateListOfServers();
            if (primeConnection && this.getPrimeConnections() != null) {
                this.getPrimeConnections()
                        .primeConnections(getServerList(true));
            }
            this.setEnablePrimingConnections(primeConnection);

        } catch (Exception e) {
            throw new RuntimeException(
                    "Exception while initializing NIWSDiscoveryLoadBalancer:"
                            + clientConfig.getClientName()
                            + ", niwsClientConfig:" + clientConfig, e);
        }
    }

    @Override
    public void setServersList(List lsrv) {
        super.setServersList(lsrv);
        List<T> serverList = (List<T>) lsrv;
        Map<String, List<Server>> serversInZones = new HashMap<String, List<Server>>();
        for (Server server : serverList) {
            // make sure ServerStats is created to avoid creating them on hot
            // path
            getLoadBalancerStats().getSingleServerStat(server);
            String zone = server.getZone();
            if (zone != null) {
                zone = zone.toLowerCase();
                List<Server> servers = serversInZones.get(zone);
                if (servers == null) {
                    servers = new ArrayList<Server>();
                    serversInZones.put(zone, servers);
                }
                servers.add(server);
            }
        }
        setServerListForZones(serversInZones);
    }

    protected void setServerListForZones(
            Map<String, List<Server>> zoneServersMap) {
        LOGGER.debug("Setting server list for zones: {}", zoneServersMap);
        getLoadBalancerStats().updateZoneServerMapping(zoneServersMap);
    }

    public ServerList<T> getServerListImpl() {
        return serverListImpl;
    }

    public void setServerListImpl(ServerList<T> niwsServerList) {
        this.serverListImpl = niwsServerList;
    }

    @Override
    public void setPing(IPing ping) {
        this.ping = ping;
    }

    public ServerListFilter<T> getFilter() {
        return filter;
    }

    public void setFilter(ServerListFilter<T> filter) {
        this.filter = filter;
    }

    @Override
    /**
     * Makes no sense to ping an inmemory disc client
     * 
     */
    public void forceQuickPing() {
        // no-op
    }

    /**
     * Feature that lets us add new instances (from AMIs) to the list of
     * existing servers that the LB will use Call this method if you want this
     * feature enabled
     */
    public void enableAndInitLearnNewServersFeature() {
        String threadName = "DynamicServerListLoadBalancer-" + getIdentifier();
        ThreadFactory factory = (new ThreadFactoryBuilder()).setDaemon(true)
                .setNameFormat(threadName).build();
        _serverListRefreshExecutor = new ScheduledThreadPoolExecutor(1, factory);
        keepServerListUpdated();
        serverRefreshEnabled = true;
        // Add it to the shutdown hook

        if (_shutdownThread == null) {

            _shutdownThread = new Thread(new Runnable() {
                public void run() {
                    LOGGER.info("Shutting down the Executor Pool for "
                            + getIdentifier());
                    shutdownExecutorPool();
                }
            });

            Runtime.getRuntime().addShutdownHook(_shutdownThread);
        }
    }

    private String getIdentifier() {
        return this.getClientConfig().getClientName();
    }

    private void keepServerListUpdated() {
        _serverListRefreshExecutor.scheduleAtFixedRate(
                new ServerListRefreshExecutorThread(),
                LISTOFSERVERS_CACHE_UPDATE_DELAY, refeshIntervalMills,
                TimeUnit.MILLISECONDS);
    }

    public void shutdownExecutorPool() {
        if (_serverListRefreshExecutor != null) {
            _serverListRefreshExecutor.shutdown();

            if (_shutdownThread != null) {
                try {
                    Runtime.getRuntime().removeShutdownHook(_shutdownThread);
                } catch (IllegalStateException ise) { // NOPMD
                    // this can happen if we're in the middle of a real
                    // shutdown,
                    // and that's 'ok'
                }
            }

        }
    }

    /**
     * Class that updates the list of Servers This is based on the method used
     * by the client * Appropriate Filters are applied before coming up with the
     * right set of servers
     * 
     * @author stonse
     * 
     */
    class ServerListRefreshExecutorThread implements Runnable {

        public void run() {
            try {
                updateListOfServers();

            } catch (Throwable e) {
                LOGGER.error(
                        "Exception while updating List of Servers obtained from Discovery client",
                        e);
                // e.printStackTrace();
            }
        }

    }

    @VisibleForTesting
    public void updateListOfServers() {
        List<T> servers = new ArrayList<T>();
        if (serverListImpl != null) {
            servers = serverListImpl.getUpdatedListOfServers();
            LOGGER.debug("List of Servers for {} obtained from Discovery client: {}",
                    getIdentifier(), servers);

            if (filter != null) {
                servers = filter.getFilteredListOfServers(servers);
                LOGGER.debug("Filtered List of Servers for {} obtained from Discovery client: {}",
                        getIdentifier(), servers);
            }
        }
        lastUpdated.set(System.currentTimeMillis());
        updateAllServerList(servers);
    }

    /**
     * Update the AllServer list in the LoadBalancer if necessary and enabled
     * 
     * @param ls
     */
    protected void updateAllServerList(List<T> ls) {
        // other threads might be doing this - in which case, we pass
        if (!serverListUpdateInProgress.get()) {
            serverListUpdateInProgress.set(true);
            for (T s : ls) {
                s.setAlive(true); // set so that clients can start using these
                                  // servers right away instead
                // of having to wait out the ping cycle.
            }
            setServersList(ls);
            super.forceQuickPing();
            serverListUpdateInProgress.set(false);
        }
    }

    @Monitor(name="NumUpdateCyclesMissed", type=DataSourceType.GAUGE)
    public int getNumberMissedCycles() {
        if (!serverRefreshEnabled) {
            return 0;
        }
        return (int) ((int) (System.currentTimeMillis() - lastUpdated.get()) / refeshIntervalMills);
    }
    
    @Monitor(name="LastUpdated", type=DataSourceType.INFORMATIONAL)
    public String getLastUpdate() {
        return new Date(lastUpdated.get()).toString();
    }
    
    public String toString() {
        StringBuilder sb = new StringBuilder("DynamicServerListLoadBalancer:");
        sb.append(super.toString());
        sb.append("ServerList:" + String.valueOf(serverListImpl));
        return sb.toString();
    }
}=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.loadbalancer;

import java.util.Date;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ScheduledFuture;
import java.util.concurrent.ScheduledThreadPoolExecutor;
import java.util.concurrent.ThreadFactory;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicLong;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.netflix.client.ClientFactory;
import com.netflix.client.config.CommonClientConfigKey;
import com.netflix.client.config.DefaultClientConfigImpl;
import com.netflix.client.config.IClientConfig;
import com.netflix.config.DynamicIntProperty;
import com.netflix.config.DynamicProperty;
import com.netflix.servo.annotations.DataSourceType;
import com.netflix.servo.annotations.Monitor;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.util.concurrent.ThreadFactoryBuilder;

/**
 * A LoadBalancer that has the capabilities to obtain the candidate list of
 * servers using a dynamic source. i.e. The list of servers can potentially be
 * changed at Runtime. It also contains facilities wherein the list of servers
 * can be passed through a Filter criteria to filter out servers that do not
 * meet the desired criteria.
 * 
 * @author stonse
 * 
 */
public class DynamicServerListLoadBalancer<T extends Server> extends
        BaseLoadBalancer {
    private static final Logger LOGGER = LoggerFactory
            .getLogger(DynamicServerListLoadBalancer.class);

    boolean isSecure = false;
    boolean useTunnel = false;
    private static Thread _shutdownThread;

    // to keep track of modification of server lists
    protected AtomicBoolean serverListUpdateInProgress = new AtomicBoolean(
            false);

    private static long LISTOFSERVERS_CACHE_UPDATE_DELAY = 1000; // msecs;
    private static long LISTOFSERVERS_CACHE_REPEAT_INTERVAL = 30 * 1000; // msecs;
                                                                         // //
                                                                         // every
                                                                         // 30
                                                                         // secs

    private static ScheduledThreadPoolExecutor _serverListRefreshExecutor = null;

    private long refeshIntervalMills = LISTOFSERVERS_CACHE_REPEAT_INTERVAL;

    volatile ServerList<T> serverListImpl;

    volatile ServerListFilter<T> filter;
    
    private AtomicLong lastUpdated = new AtomicLong(System.currentTimeMillis());
    
    protected volatile boolean serverRefreshEnabled = false;
    private final static String CORE_THREAD = "DynamicServerListLoadBalancer.ThreadPoolSize";
    private final static DynamicIntProperty poolSizeProp = new DynamicIntProperty(CORE_THREAD, 2);
    
    private volatile ScheduledFuture<?> scheduledFuture;

    static {
        int coreSize = poolSizeProp.get();
        ThreadFactory factory = (new ThreadFactoryBuilder()).setDaemon(true).build();
        _serverListRefreshExecutor = new ScheduledThreadPoolExecutor(coreSize, factory);
        poolSizeProp.addCallback(new Runnable() {
            @Override
            public void run() {
                _serverListRefreshExecutor.setCorePoolSize(poolSizeProp.get());
            }
        
        });
        _shutdownThread = new Thread(new Runnable() {
            public void run() {
                LOGGER.info("Shutting down the Executor Pool for DynamicServerListLoadBalancer");
                shutdownExecutorPool();
            }
        });
        Runtime.getRuntime().addShutdownHook(_shutdownThread);
    }
    
    public DynamicServerListLoadBalancer() {
        super();
    }

    public DynamicServerListLoadBalancer(IClientConfig niwsClientConfig) {
        initWithNiwsConfig(niwsClientConfig);
    }

    @Override
    public void initWithNiwsConfig(IClientConfig clientConfig) {
        try {
            super.initWithNiwsConfig(clientConfig);
            String niwsServerListClassName = clientConfig.getProperty(
                    CommonClientConfigKey.NIWSServerListClassName,
                    DefaultClientConfigImpl.DEFAULT_SEVER_LIST_CLASS)
                    .toString();

            ServerList<T> niwsServerListImpl = (ServerList<T>) ClientFactory
                    .instantiateInstanceWithClientConfig(
                            niwsServerListClassName, clientConfig);
            this.serverListImpl = niwsServerListImpl;

            if (niwsServerListImpl instanceof AbstractServerList) {
                AbstractServerListFilter<T> niwsFilter = ((AbstractServerList) niwsServerListImpl)
                        .getFilterImpl(clientConfig);
                niwsFilter.setLoadBalancerStats(getLoadBalancerStats());
                this.filter = niwsFilter;
            }

            refeshIntervalMills = Integer.valueOf(clientConfig.getProperty(
                    CommonClientConfigKey.ServerListRefreshInterval,
                    LISTOFSERVERS_CACHE_REPEAT_INTERVAL).toString());

            boolean primeConnection = this.isEnablePrimingConnections();
            // turn this off to avoid duplicated asynchronous priming done in BaseLoadBalancer.setServerList()
            this.setEnablePrimingConnections(false);
            enableAndInitLearnNewServersFeature();

            updateListOfServers();
            if (primeConnection && this.getPrimeConnections() != null) {
                this.getPrimeConnections()
                        .primeConnections(getServerList(true));
            }
            this.setEnablePrimingConnections(primeConnection);

        } catch (Exception e) {
            throw new RuntimeException(
                    "Exception while initializing NIWSDiscoveryLoadBalancer:"
                            + clientConfig.getClientName()
                            + ", niwsClientConfig:" + clientConfig, e);
        }
    }

    @Override
    public void setServersList(List lsrv) {
        super.setServersList(lsrv);
        List<T> serverList = (List<T>) lsrv;
        Map<String, List<Server>> serversInZones = new HashMap<String, List<Server>>();
        for (Server server : serverList) {
            // make sure ServerStats is created to avoid creating them on hot
            // path
            getLoadBalancerStats().getSingleServerStat(server);
            String zone = server.getZone();
            if (zone != null) {
                zone = zone.toLowerCase();
                List<Server> servers = serversInZones.get(zone);
                if (servers == null) {
                    servers = new ArrayList<Server>();
                    serversInZones.put(zone, servers);
                }
                servers.add(server);
            }
        }
        setServerListForZones(serversInZones);
    }

    protected void setServerListForZones(
            Map<String, List<Server>> zoneServersMap) {
        LOGGER.debug("Setting server list for zones: {}", zoneServersMap);
        getLoadBalancerStats().updateZoneServerMapping(zoneServersMap);
    }

    public ServerList<T> getServerListImpl() {
        return serverListImpl;
    }

    public void setServerListImpl(ServerList<T> niwsServerList) {
        this.serverListImpl = niwsServerList;
    }

    @Override
    public void setPing(IPing ping) {
        this.ping = ping;
    }

    public ServerListFilter<T> getFilter() {
        return filter;
    }

    public void setFilter(ServerListFilter<T> filter) {
        this.filter = filter;
    }

    @Override
    /**
     * Makes no sense to ping an inmemory disc client
     * 
     */
    public void forceQuickPing() {
        // no-op
    }

    /**
     * Feature that lets us add new instances (from AMIs) to the list of
     * existing servers that the LB will use Call this method if you want this
     * feature enabled
     */
    public void enableAndInitLearnNewServersFeature() {
        keepServerListUpdated();
        serverRefreshEnabled = true;
    }

    private String getIdentifier() {
        return this.getClientConfig().getClientName();
    }

    private void keepServerListUpdated() {
        scheduledFuture = _serverListRefreshExecutor.scheduleAtFixedRate(
                new ServerListRefreshExecutorThread(),
                LISTOFSERVERS_CACHE_UPDATE_DELAY, refeshIntervalMills,
                TimeUnit.MILLISECONDS);
    }

    private static void shutdownExecutorPool() {
        if (_serverListRefreshExecutor != null) {
            _serverListRefreshExecutor.shutdown();

            if (_shutdownThread != null) {
                try {
                    Runtime.getRuntime().removeShutdownHook(_shutdownThread);
                } catch (IllegalStateException ise) { // NOPMD
                    // this can happen if we're in the middle of a real
                    // shutdown,
                    // and that's 'ok'
                }
            }

        }
    }

    public void stopServerListRefreshing() {
        serverRefreshEnabled = false;
        if (scheduledFuture != null) {
            scheduledFuture.cancel(true);
        }
    }
    
    /**
     * Class that updates the list of Servers This is based on the method used
     * by the client * Appropriate Filters are applied before coming up with the
     * right set of servers
     * 
     * @author stonse
     * 
     */
    class ServerListRefreshExecutorThread implements Runnable {

        public void run() {
            if (!serverRefreshEnabled) {
                return;
            }
            try {
                updateListOfServers();

            } catch (Throwable e) {
                LOGGER.error(
                        "Exception while updating List of Servers obtained from Discovery client",
                        e);
                // e.printStackTrace();
            }
        }

    }

    @VisibleForTesting
    public void updateListOfServers() {
        List<T> servers = new ArrayList<T>();
        if (serverListImpl != null) {
            servers = serverListImpl.getUpdatedListOfServers();
            LOGGER.debug("List of Servers for {} obtained from Discovery client: {}",
                    getIdentifier(), servers);

            if (filter != null) {
                servers = filter.getFilteredListOfServers(servers);
                LOGGER.debug("Filtered List of Servers for {} obtained from Discovery client: {}",
                        getIdentifier(), servers);
            }
        }
        lastUpdated.set(System.currentTimeMillis());
        updateAllServerList(servers);
    }

    /**
     * Update the AllServer list in the LoadBalancer if necessary and enabled
     * 
     * @param ls
     */
    protected void updateAllServerList(List<T> ls) {
        // other threads might be doing this - in which case, we pass
        if (!serverListUpdateInProgress.get()) {
            serverListUpdateInProgress.set(true);
            for (T s : ls) {
                s.setAlive(true); // set so that clients can start using these
                                  // servers right away instead
                // of having to wait out the ping cycle.
            }
            setServersList(ls);
            super.forceQuickPing();
            serverListUpdateInProgress.set(false);
        }
    }

    @Monitor(name="NumUpdateCyclesMissed", type=DataSourceType.GAUGE)
    public int getNumberMissedCycles() {
        if (!serverRefreshEnabled) {
            return 0;
        }
        return (int) ((int) (System.currentTimeMillis() - lastUpdated.get()) / refeshIntervalMills);
    }
    
    @Monitor(name="LastUpdated", type=DataSourceType.INFORMATIONAL)
    public String getLastUpdate() {
        return new Date(lastUpdated.get()).toString();
    }
    
    @Monitor(name="NumThreads", type=DataSourceType.GAUGE) 
    public int getCoreThreads() {
        if (_serverListRefreshExecutor != null) {
            return _serverListRefreshExecutor.getCorePoolSize();
        } else {
            return 0;
        }
    }
    
    public String toString() {
        StringBuilder sb = new StringBuilder("DynamicServerListLoadBalancer:");
        sb.append(super.toString());
        sb.append("ServerList:" + String.valueOf(serverListImpl));
        return sb.toString();
    }
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_7620032_42c3e4c/rev_7620032-42c3e4c/ribbon-core/src/test/java/com/netflix/client/LoadBalancerContextTest.java;<<<<<<< MINE
||||||| BASE
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.client;

import static org.junit.Assert.*;

import java.net.URLEncoder;

import org.junit.Test;

import com.netflix.client.http.HttpRequest;
import com.netflix.client.http.HttpResponse;
import com.netflix.loadbalancer.BaseLoadBalancer;
import com.netflix.loadbalancer.Server;

public class LoadBalancerContextTest {

    static BaseLoadBalancer lb = new BaseLoadBalancer() {

        @Override
        public Server chooseServer(Object key) {
            return new Server("www.example.com:8080");
        }
    };
    
    
    private MyLoadBalancerContext context;
    
    public LoadBalancerContextTest() {
        context = new MyLoadBalancerContext();
        context.setLoadBalancer(lb);
    }
    
    @Test
    public void testComputeFinalUriWithLoadBalancer() throws ClientException {
        HttpRequest request = HttpRequest.newBuilder().uri("/test?abc=xyz").build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals("http://www.example.com:8080/test?abc=xyz", newRequest.getUri().toString());
    }
    
    @Test
    public void testEncodedPath() throws ClientException {
        String uri = "http://localhost:8080/resources/abc%2Fxyz";
        HttpRequest request = HttpRequest.newBuilder().uri(uri).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals(uri, newRequest.getUri().toString());
    }
    
    @Test
    public void testEncodedPathAndHostChange() throws ClientException {
        String uri = "/abc%2Fxyz";
        HttpRequest request = HttpRequest.newBuilder().uri(uri).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals("http://www.example.com:8080" + uri, newRequest.getUri().toString());
    }

    
    @Test
    public void testEncodedQuery() throws Exception {
        String uri = "http://localhost:8080/resources/abc?";
        String queryString = "name=" + URLEncoder.encode("&=*%!@#$%^&*()", "UTF-8");   
        HttpRequest request = HttpRequest.newBuilder().uri(uri + queryString).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals(uri + queryString, newRequest.getUri().toString());        
    }
}

class MyLoadBalancerContext extends LoadBalancerContext<HttpRequest, HttpResponse> {
}=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.client;

import static org.junit.Assert.*;

import java.net.URLEncoder;

import org.junit.Test;

import com.netflix.client.http.HttpRequest;
import com.netflix.client.http.HttpResponse;
import com.netflix.loadbalancer.BaseLoadBalancer;
import com.netflix.loadbalancer.Server;

public class LoadBalancerContextTest {

    static BaseLoadBalancer lb = new BaseLoadBalancer() {

        @Override
        public Server chooseServer(Object key) {
            return new Server("www.example.com:8080");
        }
    };
    
    
    private MyLoadBalancerContext context;
    
    public LoadBalancerContextTest() {
        context = new MyLoadBalancerContext();
        context.setLoadBalancer(lb);
    }
    
    @Test
    public void testComputeFinalUriWithLoadBalancer() throws ClientException {
        HttpRequest request = HttpRequest.newBuilder().uri("/test?abc=xyz").build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals("http://www.example.com:8080/test?abc=xyz", newRequest.getUri().toString());
    }
    
    @Test
    public void testEncodedPath() throws ClientException {
        String uri = "http://localhost:8080/resources/abc%2Fxyz";
        HttpRequest request = HttpRequest.newBuilder().uri(uri).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals(uri, newRequest.getUri().toString());
    }
    
    @Test
    public void testPreservesUserInfo() throws ClientException {
        // %3A == ":" -- ensure user info is not decoded
        String uri = "http://us%3Aer:pass@localhost:8080?foo=bar";
        HttpRequest request = HttpRequest.newBuilder().uri(uri).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals(uri, newRequest.getUri().toString());
    }
    
    @Test
    public void testQueryWithoutPath() throws ClientException {
        String uri = "?foo=bar";
        HttpRequest request = HttpRequest.newBuilder().uri(uri).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals("http://www.example.com:8080?foo=bar", newRequest.getUri().toString());
    }
    
    @Test
    public void testEncodedPathAndHostChange() throws ClientException {
        String uri = "/abc%2Fxyz";
        HttpRequest request = HttpRequest.newBuilder().uri(uri).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals("http://www.example.com:8080" + uri, newRequest.getUri().toString());
    }

    
    @Test
    public void testEncodedQuery() throws Exception {
        String uri = "http://localhost:8080/resources/abc?";
        String queryString = "name=" + URLEncoder.encode("&=*%!@#$%^&*()", "UTF-8");   
        HttpRequest request = HttpRequest.newBuilder().uri(uri + queryString).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals(uri + queryString, newRequest.getUri().toString());        
    }
}

class MyLoadBalancerContext extends LoadBalancerContext<HttpRequest, HttpResponse> {
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_7620032_42c3e4c/rev_7620032-42c3e4c/ribbon-httpclient/src/test/java/com/netflix/niws/client/http/RestClientTest.java;null
/home/taes/taes/projects/ribbon/revisions/rev_7620032_42c3e4c/rev_7620032-42c3e4c/ribbon-httpclient/src/test/java/com/netflix/niws/client/http/RestClientTest.java;null
/home/taes/taes/projects/ribbon/revisions/rev_7620032_42c3e4c/rev_7620032-42c3e4c/ribbon-httpclient/src/test/java/com/netflix/niws/client/http/RestClientTest.java;null
/home/taes/taes/projects/ribbon/revisions/rev_7620032_42c3e4c/rev_7620032-42c3e4c/ribbon-httpclient/src/test/java/com/netflix/niws/client/http/RestClientTest.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/extension/MultiDexConfig.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/extension/MultiDexConfig.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffResAPBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffResAPBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffResAPBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffResAPBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffResAPBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffResAPBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffResAPBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffResAPBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffResAPBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffResAPBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffResAPBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffResAPBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffResAPBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffResAPBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tasks/tpatch/TPatchDiffResAPBuildTask.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/xml/XmlHelper.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/xml/XmlHelper.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/xml/XmlHelper.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/xml/XmlHelper.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/diff/DiffResExtractor.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/diff/DiffResExtractor.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/diff/DiffResExtractor.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/diff/DiffResExtractor.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/diff/DiffResExtractor.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/diff/DiffResExtractor.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/diff/DiffResExtractor.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/diff/DiffResExtractor.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/diff/DiffResExtractor.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/diff/DiffResExtractor.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/diff/DiffResExtractor.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/diff/DiffResExtractor.java;<<<<<<< MINE
        //values.xml
        File valuesXml = new File(resDir, "values/values.xml");
        try {
            removeStringValue(valuesXml, "ttid");
        } catch (Exception e) {
            throw new RuntimeException(e);
||||||| BASE
=======
        //values.xml
        File valuesXml = new File(resDir, "values/values.xml");
        AtlasBuildContext.sBuilderAdapter.apkInjectInfoCreator.injectTpatchValuesRes(appVariantContext, valuesXml);
        try {
            removeStringValue(valuesXml, "config_channel");
            removeStringValue(valuesXml, "ttid");
            removeStringValue(valuesXml, "config_channel");
        } catch (Exception e) {
            throw new RuntimeException(e);
>>>>>>> YOURS
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/diff/DiffResExtractor.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/diff/DiffResExtractor.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/sign/AndroidSigner.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/sign/AndroidSigner.java;null
/home/taes/taes/projects/atlas/revisions/rev_4e151bd_ffb0907/rev_4e151bd-ffb0907/atlas-gradle-plugin/atlas-plugin/src/main/java/com/taobao/android/builder/tools/sign/AndroidSigner.java;null
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/serialization/SerializationFactory.java;<<<<<<< MINE
package com.netflix.serialization;

import com.google.common.base.Optional;


public interface SerializationFactory<K extends Object> {
    public Optional<Deserializer> getDeserializer(K key);  
    public Optional<Serializer> getSerializer(K key);
}||||||| BASE
=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.serialization;

import com.google.common.base.Optional;


public interface SerializationFactory<K> {
    public Optional<Deserializer> getDeserializer(K key);  
    public Optional<Serializer> getSerializer(K key);
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/serialization/ContentTypeBasedSerializerKey.java;<<<<<<< MINE
package com.netflix.serialization;

import com.google.common.reflect.TypeToken;

public class ContentTypeBasedSerializerKey {
    private final String contentType;
    private final TypeToken<?> typeToken;
    private final Class<?> classType;
    
    public ContentTypeBasedSerializerKey(String contentType, Class<?> classType) {
        super();
        this.contentType = contentType;
        this.typeToken = TypeToken.of(classType);
        this.classType = classType;
    }
    
    public ContentTypeBasedSerializerKey(String contentType, TypeToken<?> typeToken) {
        super();
        this.contentType = contentType;
        this.typeToken = typeToken;
        this.classType = typeToken.getClass();
    }


    public final String getContentType() {
        return contentType;
    }

    public final Class<?> getClassType() {
        return classType;
    }
    
    public final TypeToken<?> getTypeToken() {
        return typeToken;
    }
    
    @Override
    public int hashCode() {
        final int prime = 31;
        int result = 1;
        result = prime * result
                + ((classType == null) ? 0 : classType.hashCode());
        result = prime * result
                + ((contentType == null) ? 0 : contentType.hashCode());
        return result;
    }

    @Override
    public boolean equals(Object obj) {
        if (this == obj)
            return true;
        if (obj == null)
            return false;
        if (getClass() != obj.getClass())
            return false;
        ContentTypeBasedSerializerKey other = (ContentTypeBasedSerializerKey) obj;
        if (classType == null) {
            if (other.classType != null)
                return false;
        } else if (!classType.equals(other.classType))
            return false;
        if (contentType == null) {
            if (other.contentType != null)
                return false;
        } else if (!contentType.equals(other.contentType))
            return false;
        return true;
    }

    @Override
    public String toString() {
        return "DefaultSerializerKey [contentType=" + contentType
                + ", classType=" + classType + "]";
    }
    
    
    
}||||||| BASE
=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.serialization;

import com.google.common.reflect.TypeToken;

public class ContentTypeBasedSerializerKey {
    private final String contentType;
    private final TypeToken<?> typeToken;
    private final Class<?> classType;
    
    public ContentTypeBasedSerializerKey(String contentType, Class<?> classType) {
        super();
        this.contentType = contentType;
        this.typeToken = TypeToken.of(classType);
        this.classType = classType;
    }
    
    public ContentTypeBasedSerializerKey(String contentType, TypeToken<?> typeToken) {
        super();
        this.contentType = contentType;
        this.typeToken = typeToken;
        this.classType = typeToken.getClass();
    }


    public final String getContentType() {
        return contentType;
    }

    public final Class<?> getClassType() {
        return classType;
    }
    
    public final TypeToken<?> getTypeToken() {
        return typeToken;
    }
    
    @Override
    public int hashCode() {
        final int prime = 31;
        int result = 1;
        result = prime * result
                + ((classType == null) ? 0 : classType.hashCode());
        result = prime * result
                + ((contentType == null) ? 0 : contentType.hashCode());
        return result;
    }

    @Override
    public boolean equals(Object obj) {
        if (this == obj)
            return true;
        if (obj == null)
            return false;
        if (getClass() != obj.getClass())
            return false;
        ContentTypeBasedSerializerKey other = (ContentTypeBasedSerializerKey) obj;
        if (classType == null) {
            if (other.classType != null)
                return false;
        } else if (!classType.equals(other.classType))
            return false;
        if (contentType == null) {
            if (other.contentType != null)
                return false;
        } else if (!contentType.equals(other.contentType))
            return false;
        return true;
    }

    @Override
    public String toString() {
        return "DefaultSerializerKey [contentType=" + contentType
                + ", classType=" + classType + "]";
    }
    
    
    
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/serialization/JacksonSerializationFactory.java;<<<<<<< MINE
package com.netflix.serialization;

import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.lang.reflect.Type;

import org.codehaus.jackson.map.ObjectMapper;
import org.codehaus.jackson.type.TypeReference;

import com.google.common.base.Optional;
import com.google.common.reflect.TypeToken;

public class JacksonSerializationFactory implements SerializationFactory<ContentTypeBasedSerializerKey>{

    private static final JsonCodec instance = new JsonCodec();
    @Override
    public Optional<Deserializer> getDeserializer(ContentTypeBasedSerializerKey key) {
        if (key.getContentType().equalsIgnoreCase("application/json")) {
            return Optional.<Deserializer>of(instance);
        }
        return Optional.absent();
    }

    @Override
    public Optional<Serializer> getSerializer(ContentTypeBasedSerializerKey key) {
        if (key.getContentType().equalsIgnoreCase("application/json")) {
            return Optional.<Serializer>of(instance);
        }
        return Optional.absent();
    }

}

class JsonCodec implements Serializer, Deserializer {
    private ObjectMapper mapper = new ObjectMapper();
    
    @Override
    public <T> T deserialize(byte[] content, Class<T> type) throws IOException {
        return mapper.readValue(content, type);
    }

    @Override
    public byte[] serialize(Object object) throws IOException {
        return mapper.writeValueAsBytes(object);
    }

    @Override
    public <T> T deserialize(InputStream in, Class<T> type) throws IOException {
        return mapper.readValue(in, type);
    }

    @Override
    public void serialize(OutputStream out, Object object) throws IOException {
        mapper.writeValue(out, object);
    }

    @Override
    public <T> T deserialize(byte[] content, TypeToken<T> type)
            throws IOException {
        return mapper.readValue(content, new TypeTokenBasedReference<T>(type));
    }

    @Override
    public <T> T deserialize(InputStream in, TypeToken<T> type)
            throws IOException {
        return mapper.readValue(in, new TypeTokenBasedReference<T>(type));
    }
}

class TypeTokenBasedReference<T> extends TypeReference<T> {
    
    final Type type;
    public TypeTokenBasedReference(TypeToken<T> typeToken) {
        type = typeToken.getType();    
        
    }

    @Override
    public Type getType() {
        return type;
    }
}||||||| BASE
=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.serialization;

import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.lang.reflect.Type;

import org.codehaus.jackson.map.ObjectMapper;
import org.codehaus.jackson.type.TypeReference;

import com.google.common.base.Optional;
import com.google.common.reflect.TypeToken;

public class JacksonSerializationFactory implements SerializationFactory<ContentTypeBasedSerializerKey>{

    private static final JsonCodec instance = new JsonCodec();
    @Override
    public Optional<Deserializer> getDeserializer(ContentTypeBasedSerializerKey key) {
        if (key.getContentType().equalsIgnoreCase("application/json")) {
            return Optional.<Deserializer>of(instance);
        }
        return Optional.absent();
    }

    @Override
    public Optional<Serializer> getSerializer(ContentTypeBasedSerializerKey key) {
        if (key.getContentType().equalsIgnoreCase("application/json")) {
            return Optional.<Serializer>of(instance);
        }
        return Optional.absent();
    }

}

class JsonCodec implements Serializer, Deserializer {
    private final ObjectMapper mapper = new ObjectMapper();

    @Override
    public <T> T deserialize(InputStream in, TypeToken<T> type)
            throws IOException {
        return mapper.readValue(in, new TypeTokenBasedReference<T>(type));
    }
    
    @Override
    public void serialize(OutputStream out, Object object) throws IOException {
        mapper.writeValue(out, object);
    }
}

class TypeTokenBasedReference<T> extends TypeReference<T> {
    
    final Type type;
    public TypeTokenBasedReference(TypeToken<T> typeToken) {
        type = typeToken.getType();    
        
    }

    @Override
    public Type getType() {
        return type;
    }
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/serialization/Deserializer.java;<<<<<<< MINE
package com.netflix.serialization;

import java.io.IOException;
import java.io.InputStream;

import com.google.common.reflect.TypeToken;

public interface Deserializer {
    public <T> T deserialize(byte[] content, Class<T> type) throws IOException;
    
    public <T> T deserialize(InputStream in, Class<T> type) throws IOException;
    
    public <T> T deserialize(byte[] content, TypeToken<T> typeToken) throws IOException;
    
    public <T> T deserialize(InputStream in, TypeToken<T> type) throws IOException;    


}||||||| BASE
=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.serialization;

import java.io.IOException;
import java.io.InputStream;

import com.google.common.reflect.TypeToken;

public interface Deserializer {
    public <T> T deserialize(InputStream in, TypeToken<T> type) throws IOException;    
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/serialization/Serializer.java;<<<<<<< MINE
package com.netflix.serialization;

import java.io.IOException;
import java.io.OutputStream;

public interface Serializer {
    public byte[] serialize(Object object) throws IOException;
    
    public void serialize(OutputStream out, Object object) throws IOException;    

}||||||| BASE
=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.serialization;

import java.io.IOException;
import java.io.OutputStream;

public interface Serializer {
    public void serialize(OutputStream out, Object object) throws IOException;    
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/client/AbstractLoadBalancerAwareClient.java;<<<<<<< MINE

    protected String clientName = "default";          
    
    protected String vipAddresses;
    
    protected int maxAutoRetriesNextServer = DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES_NEXT_SERVER;
    protected int maxAutoRetries = DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES;


    boolean okToRetryOnAllOperations = DefaultClientConfigImpl.DEFAULT_OK_TO_RETRY_ON_ALL_OPERATIONS.booleanValue();
        
    private ILoadBalancer lb;
    protected volatile Timer tracer;

    
    public AbstractLoadBalancerAwareClient() {  
    }
||||||| BASE

    private String clientName;          
    
    private String vipAddresses;
    
    private int maxAutoRetriesNextServer = DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES_NEXT_SERVER;
    private int maxAutoRetries = DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES;


    boolean okToRetryOnAllOperations = DefaultClientConfigImpl.DEFAULT_OK_TO_RETRY_ON_ALL_OPERATIONS.booleanValue();
        
    private ILoadBalancer lb;
    private Timer tracer;

    
    public AbstractLoadBalancerAwareClient() {  
    }
=======
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/client/AbstractLoadBalancerAwareClient.java;<<<<<<< MINE
    public final String getClientName() {
        return clientName;
    }
        
    public ILoadBalancer getLoadBalancer() {
        return lb;    
    }
        
    public void setLoadBalancer(ILoadBalancer lb) {
        this.lb = lb;
    }

    public final int getMaxAutoRetriesNextServer() {
        return maxAutoRetriesNextServer;
    }

    public final void setMaxAutoRetriesNextServer(int maxAutoRetriesNextServer) {
        this.maxAutoRetriesNextServer = maxAutoRetriesNextServer;
    }

    public final int getMaxAutoRetries() {
        return maxAutoRetries;
    }

    public final void setMaxAutoRetries(int maxAutoRetries) {
        this.maxAutoRetries = maxAutoRetries;
    }

    protected Throwable getDeepestCause(Throwable e) {
        if(e != null) {
            int infiniteLoopPreventionCounter = 10;
            while (e.getCause() != null && infiniteLoopPreventionCounter > 0) {
                infiniteLoopPreventionCounter--;
                e = e.getCause();
            }
        }
        return e;
    }

||||||| BASE
    public final String getClientName() {
        return clientName;
    }
        
    public ILoadBalancer getLoadBalancer() {
        return lb;    
    }
        
    public void setLoadBalancer(ILoadBalancer lb) {
        this.lb = lb;
    }

    private Throwable getDeepestCause(Throwable e) {
        if(e != null) {
            int infiniteLoopPreventionCounter = 10;
            while (e.getCause() != null && infiniteLoopPreventionCounter > 0) {
                infiniteLoopPreventionCounter--;
                e = e.getCause();
            }
        }
        return e;
    }

=======
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/client/AbstractLoadBalancerAwareClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/client/AbstractLoadBalancerAwareClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/client/AbstractLoadBalancerAwareClient.java;<<<<<<< MINE
    private boolean isPresentAsCause(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor) {
        return isPresentAsCauseHelper(throwableToSearchIn, throwableToSearchFor) != null;
    }

    private Throwable isPresentAsCauseHelper(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor) {
        int infiniteLoopPreventionCounter = 10;
        while (throwableToSearchIn != null && infiniteLoopPreventionCounter > 0) {
            infiniteLoopPreventionCounter--;
            if (throwableToSearchIn.getClass().isAssignableFrom(
                    throwableToSearchFor)) {
                return throwableToSearchIn;
            } else {
                throwableToSearchIn = throwableToSearchIn.getCause();
            }
        }
        return null;
    }

    protected ClientException generateNIWSException(String uri, Throwable e){
        ClientException niwsClientException;
        if (isPresentAsCause(e, java.net.SocketTimeoutException.class)) {
            niwsClientException = generateTimeoutNIWSException(uri, e);
        }else if (e.getCause() instanceof java.net.UnknownHostException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.UNKNOWN_HOST_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e.getCause() instanceof java.net.ConnectException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.CONNECT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e.getCause() instanceof java.net.NoRouteToHostException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.NO_ROUTE_TO_HOST_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e instanceof ClientException){
            niwsClientException = (ClientException)e;
        }else {
            niwsClientException = new ClientException(
                ClientException.ErrorType.GENERAL,
                "Unable to execute RestClient request for URI:" + uri,
                e);
        }
        return niwsClientException;
    }

    private boolean isPresentAsCause(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor, String messageSubStringToSearchFor) {
        Throwable throwableFound = isPresentAsCauseHelper(throwableToSearchIn, throwableToSearchFor);
        if(throwableFound != null) {
            return throwableFound.getMessage().contains(messageSubStringToSearchFor);
        }
        return false;
    }
    private ClientException generateTimeoutNIWSException(String uri, Throwable e){
        ClientException niwsClientException;
        if (isPresentAsCause(e, java.net.SocketTimeoutException.class,
                "Read timed out")) {
            niwsClientException = new ClientException(
                    ClientException.ErrorType.READ_TIMEOUT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri + ":"
                            + getDeepestCause(e).getMessage(), e);
        } else {
            niwsClientException = new ClientException(
                    ClientException.ErrorType.SOCKET_TIMEOUT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri + ":"
                            + getDeepestCause(e).getMessage(), e);
        }
        return niwsClientException;
    }

    protected int handleRetry(String uri, int retries, int numRetries,
            Exception e) throws ClientException {
        retries++;

        if (retries > numRetries) {
            throw new ClientException(ClientException.ErrorType.NUMBEROF_RETRIES_EXEEDED,
                    "NUMBEROFRETRIESEXEEDED :" + numRetries + " retries, while making a RestClient call for:" + uri,
                    e !=null? e: new RuntimeException());
        }
        logger.error("Exception while executing request which is deemed retry-able, retrying ..., SAME Server Retry Attempt#:" +
                retries +
                ", URI:" +
                uri);
        try {
            Thread.sleep((int) Math.pow(2.0, retries) * 100); 
        } catch (InterruptedException ex) {
        }
        return retries;
    }

    /**
     * This is called after a response is received or an exception is thrown from the {@link #execute(ClientRequest)}
     * to update related stats.  
     */
    protected void noteRequestCompletion(ServerStats stats, S task, IResponse response, Throwable e, long responseTime) {        
        try {
            if (stats != null) {
                stats.decrementActiveRequestsCount();
                stats.incrementNumRequests();
                stats.noteResponseTime(responseTime);
                if (response != null) {
                    stats.clearSuccessiveConnectionFailureCount();                    
                }
            }            
        } catch (Throwable ex) {
            logger.error("Unexpected exception", ex);
        }            
    }
       
    /**
     * Called just before {@link #execute(ClientRequest)} call.
     */
    protected void noteOpenConnection(ServerStats serverStats, S task) {
        if (serverStats == null) {
            return;
        }
        try {
            serverStats.incrementActiveRequestsCount();
        } catch (Throwable e) {
            logger.info("Unable to note Server Stats:", e);
        }
    }

||||||| BASE
    private boolean isPresentAsCause(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor) {
        return isPresentAsCauseHelper(throwableToSearchIn, throwableToSearchFor) != null;
    }

    private Throwable isPresentAsCauseHelper(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor) {
        int infiniteLoopPreventionCounter = 10;
        while (throwableToSearchIn != null && infiniteLoopPreventionCounter > 0) {
            infiniteLoopPreventionCounter--;
            if (throwableToSearchIn.getClass().isAssignableFrom(
                    throwableToSearchFor)) {
                return throwableToSearchIn;
            } else {
                throwableToSearchIn = throwableToSearchIn.getCause();
            }
        }
        return null;
    }

    private ClientException generateNIWSException(String uri, Exception e){
        ClientException niwsClientException;
        if (isPresentAsCause(e, java.net.SocketTimeoutException.class)) {
            niwsClientException = generateTimeoutNIWSException(uri, e);
        }else if (e.getCause() instanceof java.net.UnknownHostException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.UNKNOWN_HOST_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e.getCause() instanceof java.net.ConnectException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.CONNECT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e.getCause() instanceof java.net.NoRouteToHostException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.NO_ROUTE_TO_HOST_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e instanceof ClientException){
            niwsClientException = (ClientException)e;
        }else {
            niwsClientException = new ClientException(
                ClientException.ErrorType.GENERAL,
                "Unable to execute RestClient request for URI:" + uri,
                e);
        }
        return niwsClientException;
    }

    private boolean isPresentAsCause(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor, String messageSubStringToSearchFor) {
        Throwable throwableFound = isPresentAsCauseHelper(throwableToSearchIn, throwableToSearchFor);
        if(throwableFound != null) {
            return throwableFound.getMessage().contains(messageSubStringToSearchFor);
        }
        return false;
    }
    private ClientException generateTimeoutNIWSException(String uri, Exception e){
        ClientException niwsClientException;
        if (isPresentAsCause(e, java.net.SocketTimeoutException.class,
                "Read timed out")) {
            niwsClientException = new ClientException(
                    ClientException.ErrorType.READ_TIMEOUT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri + ":"
                            + getDeepestCause(e).getMessage(), e);
        } else {
            niwsClientException = new ClientException(
                    ClientException.ErrorType.SOCKET_TIMEOUT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri + ":"
                            + getDeepestCause(e).getMessage(), e);
        }
        return niwsClientException;
    }

    private int handleRetry(String uri, int retries, int numRetries,
            Exception e) throws ClientException {
        retries++;

        if (retries > numRetries) {
            throw new ClientException(ClientException.ErrorType.NUMBEROF_RETRIES_EXEEDED,
                    "NUMBEROFRETRIESEXEEDED :" + numRetries + " retries, while making a RestClient call for:" + uri,
                    e !=null? e: new RuntimeException());
        }
        logger.error("Exception while executing request which is deemed retry-able, retrying ..., SAME Server Retry Attempt#:" +
                retries +
                ", URI:" +
                uri);
        try {
            Thread.sleep((int) Math.pow(2.0, retries) * 100); 
        } catch (InterruptedException ex) {
        }
        return retries;
    }

    /**
     * This is called after a response is received or an exception is thrown from the {@link #execute(ClientRequest)}
     * to update related stats.  
     */
    protected void noteRequestCompletion(ServerStats stats, S task, IResponse response, Exception e, long responseTime) {        
        try {
            if (stats != null) {
                stats.decrementActiveRequestsCount();
                stats.incrementNumRequests();
                stats.noteResponseTime(responseTime);
                if (response != null) {
                    stats.clearSuccessiveConnectionFailureCount();                    
                }
            }            
        } catch (Throwable ex) {
            logger.error("Unexpected exception", ex);
        }            
    }
        
    /**
     * This method is called after a response (either success or not) is received to update certain stats.
     */
    protected void noteResponseReceived(ServerStats stats, T task, IResponse response) {
        if (stats == null) {
            return;
        }
        try {
            stats.clearSuccessiveConnectionFailureCount();
        } catch (Throwable  e) {
            logger.info("Unable to note Server Stats:", e);
        }        
    }
       
    /**
     * Called just before {@link #execute(ClientRequest)} call.
     */
    protected void noteOpenConnection(ServerStats serverStats, S task) {
        if (serverStats == null) {
            return;
        }
        try {
            serverStats.incrementActiveRequestsCount();
        } catch (Throwable e) {
            logger.info("Unable to note Server Stats:", e);
        }
    }

=======
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/client/config/CommonClientConfigKey.java;<<<<<<< MINE
    RulePredicateClasses("RulePredicateClasses"),
    
    // serialization
    SerializationFactoryClassName("SerializationClassName");

||||||| BASE
    RulePredicateClasses("RulePredicateClasses");
=======
    RulePredicateClasses("RulePredicateClasses"),
    
    // serialization
    DefaultSerializationFactoryClassName("DefaultSerializationClassName");

>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/client/config/IClientConfig.java;null
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/client/config/DefaultClientConfigImpl.java;null
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/client/config/DefaultClientConfigImpl.java;null
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/client/config/DefaultClientConfigImpl.java;null
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/client/AsyncClient.java;<<<<<<< MINE
package com.netflix.client;

public interface AsyncClient<Request extends ClientRequest, Response extends ResponseWithTypedEntity> {
    public void execute(Request request, ResponseCallback<Response> callback) throws ClientException;

}||||||| BASE
=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.client;

import java.nio.ByteBuffer;
import java.util.concurrent.Future;

import com.netflix.serialization.ContentTypeBasedSerializerKey;
import com.netflix.serialization.Deserializer;
import com.netflix.serialization.Serializer;

/**
 * Interface for asynchronous communication client with streaming capability.
 * 
 * @author awang
 *
 * @param <T> Request type
 * @param <S> Response type
 * @param <U> Type of storage used for delivering partial content, for example, {@link ByteBuffer}
 * @param <V> Type of key to find {@link Serializer} and {@link Deserializer} for the content. For example, for HTTP communication,
 *            the key type is {@link ContentTypeBasedSerializerKey}
 */
public interface AsyncClient<T extends ClientRequest, S extends IResponse, U, V> extends ResponseBufferingAsyncClient<T, S, V> {
    /**
     * Asynchronously execute a request.
     * 
     * @param request Request to execute
     * @param decooder Decoder to decode objects from the native stream 
     * @param callback Callback to be invoked when execution completes or fails
     * @return Future of the response
     * @param <E> Type of object to be decoded from the stream
     * 
     * @throws ClientException if exception happens before the actual asynchronous execution happens, for example, an error to serialize 
     *         the entity
     */
    public <E> Future<S> execute(T request, StreamDecoder<E, U> decooder, ResponseCallback<S, E> callback) throws ClientException;
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/client/LoadBalancerContext.java;<<<<<<< MINE
package com.netflix.client;

import java.net.URI;
import java.net.URISyntaxException;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.netflix.client.config.CommonClientConfigKey;
import com.netflix.client.config.DefaultClientConfigImpl;
import com.netflix.client.config.IClientConfig;
import com.netflix.loadbalancer.AbstractLoadBalancer;
import com.netflix.loadbalancer.AvailabilityFilteringRule;
import com.netflix.loadbalancer.ILoadBalancer;
import com.netflix.loadbalancer.LoadBalancerStats;
import com.netflix.loadbalancer.Server;
import com.netflix.loadbalancer.ServerStats;
import com.netflix.util.Pair;

public abstract class LoadBalancerContext implements IClientConfigAware {
    private static final Logger logger = LoggerFactory.getLogger(LoadBalancerContext.class);

    protected String clientName = "default";          

    protected String vipAddresses;
    
    protected int maxAutoRetriesNextServer = DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES_NEXT_SERVER;
    protected int maxAutoRetries = DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES;


    boolean okToRetryOnAllOperations = DefaultClientConfigImpl.DEFAULT_OK_TO_RETRY_ON_ALL_OPERATIONS.booleanValue();
        
    private ILoadBalancer lb;

    
    /**
     * Set necessary parameters from client configuration and register with Servo monitors.
     */
    @Override
    public void initWithNiwsConfig(IClientConfig clientConfig) {
        if (clientConfig == null) {
            return;    
        }
        clientName = clientConfig.getClientName();
        if (clientName == null) {
            clientName = "default";
        }
        vipAddresses = clientConfig.resolveDeploymentContextbasedVipAddresses();
        maxAutoRetries = clientConfig.getPropertyAsInteger(CommonClientConfigKey.MaxAutoRetries, DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES);
        maxAutoRetriesNextServer = clientConfig.getPropertyAsInteger(CommonClientConfigKey.MaxAutoRetriesNextServer,maxAutoRetriesNextServer);
        
       okToRetryOnAllOperations = clientConfig.getPropertyAsBoolean(CommonClientConfigKey.OkToRetryOnAllOperations, okToRetryOnAllOperations);
    }
    
    /**
     * Delegate to {@link #initWithNiwsConfig(IClientConfig)}
     * @param clientConfig
     */
    public LoadBalancerContext(IClientConfig clientConfig) {
        initWithNiwsConfig(clientConfig);        
    }
    
    public LoadBalancerContext() {
        // TODO Auto-generated constructor stub
    }

    public final String getClientName() {
        return clientName;
    }
        
    public ILoadBalancer getLoadBalancer() {
        return lb;    
    }
        
    public void setLoadBalancer(ILoadBalancer lb) {
        this.lb = lb;
    }

    public final int getMaxAutoRetriesNextServer() {
        return maxAutoRetriesNextServer;
    }

    public final void setMaxAutoRetriesNextServer(int maxAutoRetriesNextServer) {
        this.maxAutoRetriesNextServer = maxAutoRetriesNextServer;
    }

    public final int getMaxAutoRetries() {
        return maxAutoRetries;
    }

    public final void setMaxAutoRetries(int maxAutoRetries) {
        this.maxAutoRetries = maxAutoRetries;
    }

    protected Throwable getDeepestCause(Throwable e) {
        if(e != null) {
            int infiniteLoopPreventionCounter = 10;
            while (e.getCause() != null && infiniteLoopPreventionCounter > 0) {
                infiniteLoopPreventionCounter--;
                e = e.getCause();
            }
        }
        return e;
    }

    /**
     * Determine if an exception should contribute to circuit breaker trip. If such exceptions happen consecutively
     * on a server, it will be deemed as circuit breaker tripped and enter into a time out when it will be
     * skipped by the {@link AvailabilityFilteringRule}, which is the default rule for load balancers.
     */
    protected abstract boolean isCircuitBreakerException(Throwable e);
        
    /**
     * Determine if operation can be retried if an exception is thrown. For example, connect 
     * timeout related exceptions
     * are typically retriable.
     * 
     */
    protected abstract boolean isRetriableException(Throwable e);
        
    private boolean isPresentAsCause(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor) {
        return isPresentAsCauseHelper(throwableToSearchIn, throwableToSearchFor) != null;
    }

    private Throwable isPresentAsCauseHelper(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor) {
        int infiniteLoopPreventionCounter = 10;
        while (throwableToSearchIn != null && infiniteLoopPreventionCounter > 0) {
            infiniteLoopPreventionCounter--;
            if (throwableToSearchIn.getClass().isAssignableFrom(
                    throwableToSearchFor)) {
                return throwableToSearchIn;
            } else {
                throwableToSearchIn = throwableToSearchIn.getCause();
            }
        }
        return null;
    }

    protected ClientException generateNIWSException(String uri, Throwable e){
        ClientException niwsClientException;
        if (isPresentAsCause(e, java.net.SocketTimeoutException.class)) {
            niwsClientException = generateTimeoutNIWSException(uri, e);
        }else if (e.getCause() instanceof java.net.UnknownHostException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.UNKNOWN_HOST_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e.getCause() instanceof java.net.ConnectException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.CONNECT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e.getCause() instanceof java.net.NoRouteToHostException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.NO_ROUTE_TO_HOST_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e instanceof ClientException){
            niwsClientException = (ClientException)e;
        }else {
            niwsClientException = new ClientException(
                ClientException.ErrorType.GENERAL,
                "Unable to execute RestClient request for URI:" + uri,
                e);
        }
        return niwsClientException;
    }

    private boolean isPresentAsCause(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor, String messageSubStringToSearchFor) {
        Throwable throwableFound = isPresentAsCauseHelper(throwableToSearchIn, throwableToSearchFor);
        if(throwableFound != null) {
            return throwableFound.getMessage().contains(messageSubStringToSearchFor);
        }
        return false;
    }
    private ClientException generateTimeoutNIWSException(String uri, Throwable e){
        ClientException niwsClientException;
        if (isPresentAsCause(e, java.net.SocketTimeoutException.class,
                "Read timed out")) {
            niwsClientException = new ClientException(
                    ClientException.ErrorType.READ_TIMEOUT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri + ":"
                            + getDeepestCause(e).getMessage(), e);
        } else {
            niwsClientException = new ClientException(
                    ClientException.ErrorType.SOCKET_TIMEOUT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri + ":"
                            + getDeepestCause(e).getMessage(), e);
        }
        return niwsClientException;
    }

    protected int handleRetry(String uri, int retries, int numRetries,
            Exception e) throws ClientException {
        retries++;

        if (retries > numRetries) {
            throw new ClientException(ClientException.ErrorType.NUMBEROF_RETRIES_EXEEDED,
                    "NUMBEROFRETRIESEXEEDED :" + numRetries + " retries, while making a RestClient call for:" + uri,
                    e !=null? e: new RuntimeException());
        }
        logger.error("Exception while executing request which is deemed retry-able, retrying ..., SAME Server Retry Attempt#:" +
                retries +
                ", URI:" +
                uri);
        try {
            Thread.sleep((int) Math.pow(2.0, retries) * 100); 
        } catch (InterruptedException ex) {
        }
        return retries;
    }

    /**
     * This is called after a response is received or an exception is thrown from the {@link #execute(ClientRequest)}
     * to update related stats.  
     */
    protected void noteRequestCompletion(ServerStats stats, ClientRequest request, IResponse response, Throwable e, long responseTime) {        
        try {
            if (stats != null) {
                stats.decrementActiveRequestsCount();
                stats.incrementNumRequests();
                stats.noteResponseTime(responseTime);
                if (response != null) {
                    stats.clearSuccessiveConnectionFailureCount();                    
                }
            }            
        } catch (Throwable ex) {
            logger.error("Unexpected exception", ex);
        }            
    }
       
    /**
     * Called just before {@link #execute(ClientRequest)} call.
     */
    protected void noteOpenConnection(ServerStats serverStats, ClientRequest request) {
        if (serverStats == null) {
            return;
        }
        try {
            serverStats.incrementActiveRequestsCount();
        } catch (Throwable e) {
            logger.info("Unable to note Server Stats:", e);
        }
    }

      
    /**
     * Derive scheme and port from a partial URI. For example, for HTTP based client, the URI with 
     * only path "/" should return "http" and 80, whereas the URI constructed with scheme "https" and
     * path "/" should return
     * "https" and 443. This method is called by {@link #computeFinalUriWithLoadBalancer(ClientRequest)}
     * to get the complete executable URI.
     * 
     */
    protected <T extends ClientRequest> Pair<String, Integer> deriveSchemeAndPortFromPartialUri(T request) {
        URI theUrl = request.getUri();
        boolean isSecure = false;
        String scheme = theUrl.getScheme();
        if (scheme != null) {
            isSecure =  scheme.equalsIgnoreCase("https");
        }
        int port = theUrl.getPort();
        if (port < 0 && !isSecure){
            port = 80;
        } else if (port < 0 && isSecure){
            port = 443;
        }
        if (scheme == null){
            if (isSecure) {
                scheme = "https";
            } else {
                scheme = "http";
            }
        }
        return new Pair<String, Integer>(scheme, port);
    }
    
    /**
     * Get the default port of the target server given the scheme of vip address if it is available. 
     * Subclass should override it to provider protocol specific default port number if any.
     * 
     * @param scheme from the vip address. null if not present.
     * @return 80 if scheme is http, 443 if scheme is https, -1 else.
     */
    protected int getDefaultPortFromScheme(String scheme) {
        if (scheme == null) {
            return -1;
        }
        if (scheme.equals("http")) {
            return 80;
        } else if (scheme.equals("https")) {
            return 443;
        } else {
            return -1;
        }
    }

        
    /**
     * Derive the host and port from virtual address if virtual address is indeed contains the actual host 
     * and port of the server. This is the final resort to compute the final URI in {@link #computeFinalUriWithLoadBalancer(ClientRequest)}
     * if there is no load balancer available and the request URI is incomplete. Sub classes can override this method
     * to be more accurate or throws ClientException if it does not want to support virtual address to be the
     * same as physical server address.
     * <p>
     *  The virtual address is used by certain load balancers to filter the servers of the same function 
     *  to form the server pool. 
     *  
     */
    protected  Pair<String, Integer> deriveHostAndPortFromVipAddress(String vipAddress) 
            throws URISyntaxException, ClientException {
        Pair<String, Integer> hostAndPort = new Pair<String, Integer>(null, -1);
        URI uri = new URI(vipAddress);
        String scheme = uri.getScheme();
        if (scheme == null) {
            uri = new URI("http://" + vipAddress);
        }
        String host = uri.getHost();
        if (host == null) {
            throw new ClientException("Unable to derive host/port from vip address " + vipAddress);
        }
        int port = uri.getPort();
        if (port < 0) {
            port = getDefaultPortFromScheme(scheme);
        }
        if (port < 0) {
            throw new ClientException("Unable to derive host/port from vip address " + vipAddress);
        }
        hostAndPort.setFirst(host);
        hostAndPort.setSecond(port);
        return hostAndPort;
    }
    
    private boolean isVipRecognized(String vipEmbeddedInUri) {
        if (vipEmbeddedInUri == null) {
            return false;
        }
        if (vipAddresses == null) {
            return false;
        }
        String[] addresses = vipAddresses.split(",");
        for (String address: addresses) {
            if (vipEmbeddedInUri.equalsIgnoreCase(address.trim())) {
                return true;
            }
        }
        return false;
    }
    
    /**
     * Compute the final URI from a partial URI in the request. The following steps are performed:
     * 
     * <li> if host is missing and there is a load balancer, get the host/port from server chosen from load balancer
     * <li> if host is missing and there is no load balancer, try to derive host/port from virtual address set with the client
     * <li> if host is present and the authority part of the URI is a virtual address set for the client, 
     * and there is a load balancer, get the host/port from server chosen from load balancer
     * <li> if host is present but none of the above applies, interpret the host as the actual physical address
     * <li> if host is missing but none of the above applies, throws ClientException
     * 
     * @param original Original URI passed from caller
     * @return new request with the final URI  
     */
    @SuppressWarnings("unchecked")
    protected <T extends ClientRequest> T computeFinalUriWithLoadBalancer(T original) throws ClientException{
        URI newURI;
        URI theUrl = original.getUri();

        if (theUrl == null){
            throw new ClientException(ClientException.ErrorType.GENERAL, "NULL URL passed in");
        }

        String host = theUrl.getHost();
        Pair<String, Integer> schemeAndPort = deriveSchemeAndPortFromPartialUri(original);
        String scheme = schemeAndPort.first();
        int port = schemeAndPort.second();
        // Various Supported Cases
        // The loadbalancer to use and the instances it has is based on how it was registered
        // In each of these cases, the client might come in using Full Url or Partial URL
        ILoadBalancer lb = getLoadBalancer();
        Object loadBalancerKey = original.getLoadBalancerKey();
        if (host == null){
            // Partial URL Case
            // well we have to just get the right instances from lb - or we fall back
            if (lb != null){
                Server svc = lb.chooseServer(loadBalancerKey);
                if (svc == null){
                    throw new ClientException(ClientException.ErrorType.GENERAL,
                            "LoadBalancer returned null Server for :"
                            + clientName);
                }
                host = svc.getHost();
                port = svc.getPort();
                if (host == null){
                    throw new ClientException(ClientException.ErrorType.GENERAL,
                            "Invalid Server for :" + svc);
                }
                if (logger.isDebugEnabled()){
                    logger.debug(clientName + " using LB returned Server:" + svc + "for request:" + theUrl);
                }
            } else {
                // No Full URL - and we dont have a LoadBalancer registered to
                // obtain a server
                // if we have a vipAddress that came with the registration, we
                // can use that else we
                // bail out
                if (vipAddresses != null && vipAddresses.contains(",")) {
                    throw new ClientException(
                            ClientException.ErrorType.GENERAL,
                            this.clientName
                                    + "Partial URI of ("
                                    + theUrl
                                    + ") has been sent in to RestClient (with no LB) to be executed."
                                    + " Also, there are multiple vipAddresses and hence RestClient cant pick"
                                    + "one vipAddress to complete this partial uri");
                } else if (vipAddresses != null) {
                    try {
                        Pair<String,Integer> hostAndPort = deriveHostAndPortFromVipAddress(vipAddresses);
                        host = hostAndPort.first();
                        port = hostAndPort.second();
                    } catch (URISyntaxException e) {
                        throw new ClientException(
                                ClientException.ErrorType.GENERAL,
                                this.clientName
                                        + "Partial URI of ("
                                        + theUrl
                                        + ") has been sent in to RestClient (with no LB) to be executed."
                                        + " Also, the configured/registered vipAddress is unparseable (to determine host and port)");
                    }
                }else{
                    throw new ClientException(
                            ClientException.ErrorType.GENERAL,
                            this.clientName
                                    + " has no LoadBalancer registered and passed in a partial URL request (with no host:port)."
                                    + " Also has no vipAddress registered");
                }
            }
        } else {
            // Full URL Case
            // This could either be a vipAddress or a hostAndPort or a real DNS
            // if vipAddress or hostAndPort, we just have to consult the loadbalancer
            // but if it does not return a server, we should just proceed anyways
            // and assume its a DNS
            // For restClients registered using a vipAddress AND executing a request
            // by passing in the full URL (including host and port), we should only
            // consult lb IFF the URL passed is registered as vipAddress in Discovery
            boolean shouldInterpretAsVip = false;

            if (lb != null) {
                shouldInterpretAsVip = isVipRecognized(original.getUri().getAuthority());
            }
            if (shouldInterpretAsVip) {
                Server svc = lb.chooseServer(loadBalancerKey);
                if (svc != null){
                    host = svc.getHost();
                    port = svc.getPort();
                    if (host == null){
                        throw new ClientException(ClientException.ErrorType.GENERAL,
                                "Invalid Server for :" + svc);
                    }
                    if (logger.isDebugEnabled()){
                        logger.debug("using LB returned Server:" + svc + "for request:" + theUrl);
                    }
                }else{
                    // just fall back as real DNS
                    if (logger.isDebugEnabled()){
                        logger.debug(host + ":" + port + " assumed to be a valid VIP address or exists in the DNS");
                    }
                }
            } else {
             // consult LB to obtain vipAddress backed instance given full URL
                //Full URL execute request - where url!=vipAddress
               if (logger.isDebugEnabled()){
                   logger.debug("Using full URL passed in by caller (not using LB/Discovery):" + theUrl);
               }
            }
        }
        // end of creating final URL
        if (host == null){
            throw new ClientException(ClientException.ErrorType.GENERAL,"Request contains no HOST to talk to");
        }
        // just verify that at this point we have a full URL

        try {
            String urlPath = "";
            if (theUrl.getRawPath() != null && theUrl.getRawPath().startsWith("/")) {
                urlPath = theUrl.getRawPath();
            } else {
                urlPath = "/" + theUrl.getRawPath();
            }
            
            newURI = new URI(scheme, theUrl.getUserInfo(), host, port, urlPath, theUrl.getQuery(), theUrl.getFragment());
            return (T) original.replaceUri(newURI);            
        } catch (URISyntaxException e) {
            throw new ClientException(ClientException.ErrorType.GENERAL, e.getMessage());
        }
    }

    protected boolean isRetriable(ClientRequest request) {
        if (request.isRetriable()) {
            return true;            
        } else {
            boolean retryOkayOnOperation = okToRetryOnAllOperations;
            IClientConfig overriddenClientConfig = request.getOverrideConfig();
            if (overriddenClientConfig != null) {
                retryOkayOnOperation = overriddenClientConfig.getPropertyAsBoolean(CommonClientConfigKey.RequestSpecificRetryOn, okToRetryOnAllOperations);
            }
            return retryOkayOnOperation;
        }
    }
    
    protected int getRetriesNextServer(IClientConfig overriddenClientConfig) {
        int numRetries = maxAutoRetriesNextServer;
        if (overriddenClientConfig != null) {
            numRetries = overriddenClientConfig.getPropertyAsInteger(CommonClientConfigKey.MaxAutoRetriesNextServer, maxAutoRetriesNextServer);
        }
        return numRetries;
    }
    
    public final ServerStats getServerStats(Server server) {
        ServerStats serverStats = null;
        ILoadBalancer lb = this.getLoadBalancer();
        if (lb instanceof AbstractLoadBalancer){
            LoadBalancerStats lbStats = ((AbstractLoadBalancer) lb).getLoadBalancerStats();
            serverStats = lbStats.getSingleServerStat(server);
        }
        return serverStats;

    }

    public final int getNumberRetriesOnSameServer(IClientConfig overriddenClientConfig) {
        int numRetries =  maxAutoRetries;
        if (overriddenClientConfig!=null){
            try {
                numRetries = Integer.parseInt(""+overriddenClientConfig.getProperty(CommonClientConfigKey.MaxAutoRetries,maxAutoRetries));
            } catch (Exception e) {
                logger.warn("Invalid maxRetries requested for RestClient:" + this.clientName);
            }
        }
        return numRetries;
    }

}||||||| BASE
=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.client;

import java.net.URI;
import java.net.URISyntaxException;
import java.net.URLEncoder;
import java.util.Collection;
import java.util.concurrent.TimeUnit;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.base.Strings;
import com.netflix.client.config.CommonClientConfigKey;
import com.netflix.client.config.DefaultClientConfigImpl;
import com.netflix.client.config.IClientConfig;
import com.netflix.loadbalancer.AbstractLoadBalancer;
import com.netflix.loadbalancer.ILoadBalancer;
import com.netflix.loadbalancer.LoadBalancerStats;
import com.netflix.loadbalancer.Server;
import com.netflix.loadbalancer.ServerStats;
import com.netflix.servo.monitor.Monitors;
import com.netflix.servo.monitor.Timer;
import com.netflix.util.Pair;

/**
 * A class contains APIs intended to be used be load balancing client which is subclass of this class.
 * 
 * @author awang
 *
 * @param <T> Type of the request
 * @param <S> Type of the response
 */
public abstract class LoadBalancerContext<T extends ClientRequest, S extends IResponse> implements IClientConfigAware {
    private static final Logger logger = LoggerFactory.getLogger(LoadBalancerContext.class);

    protected String clientName = "default";          

    protected String vipAddresses;
    
    protected int maxAutoRetriesNextServer = DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES_NEXT_SERVER;
    protected int maxAutoRetries = DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES;

    protected LoadBalancerErrorHandler<? super T, ? super S> errorHandler = new DefaultLoadBalancerErrorHandler<ClientRequest, IResponse>();


    boolean okToRetryOnAllOperations = DefaultClientConfigImpl.DEFAULT_OK_TO_RETRY_ON_ALL_OPERATIONS.booleanValue();
        
    private ILoadBalancer lb;
    
    private volatile Timer tracer;

    public LoadBalancerContext() {
    }

    /**
     * Delegate to {@link #initWithNiwsConfig(IClientConfig)}
     * @param clientConfig
     */
    public LoadBalancerContext(IClientConfig clientConfig) {
        initWithNiwsConfig(clientConfig);        
    }

    /**
     * Set necessary parameters from client configuration and register with Servo monitors.
     */
    @Override
    public void initWithNiwsConfig(IClientConfig clientConfig) {
        if (clientConfig == null) {
            return;    
        }
        clientName = clientConfig.getClientName();
        if (clientName == null) {
            clientName = "default";
        }
        vipAddresses = clientConfig.resolveDeploymentContextbasedVipAddresses();
        maxAutoRetries = clientConfig.getPropertyAsInteger(CommonClientConfigKey.MaxAutoRetries, DefaultClientConfigImpl.DEFAULT_MAX_AUTO_RETRIES);
        maxAutoRetriesNextServer = clientConfig.getPropertyAsInteger(CommonClientConfigKey.MaxAutoRetriesNextServer,maxAutoRetriesNextServer);
        
       okToRetryOnAllOperations = clientConfig.getPropertyAsBoolean(CommonClientConfigKey.OkToRetryOnAllOperations, okToRetryOnAllOperations);
       tracer = getExecuteTracer();

       Monitors.registerObject("Client_" + clientName, this);
    }

    protected Timer getExecuteTracer() {
        if (tracer == null) {
            synchronized(this) {
                if (tracer == null) {
                    tracer = Monitors.newTimer(clientName + "_OperationTimer", TimeUnit.MILLISECONDS);                    
                }
            }
        } 
        return tracer;        
    }
    
    public String getClientName() {
        return clientName;
    }
        
    public ILoadBalancer getLoadBalancer() {
        return lb;    
    }
        
    public void setLoadBalancer(ILoadBalancer lb) {
        this.lb = lb;
    }

    public int getMaxAutoRetriesNextServer() {
        return maxAutoRetriesNextServer;
    }

    public void setMaxAutoRetriesNextServer(int maxAutoRetriesNextServer) {
        this.maxAutoRetriesNextServer = maxAutoRetriesNextServer;
    }

    public int getMaxAutoRetries() {
        return maxAutoRetries;
    }

    public void setMaxAutoRetries(int maxAutoRetries) {
        this.maxAutoRetries = maxAutoRetries;
    }

    protected Throwable getDeepestCause(Throwable e) {
        if(e != null) {
            int infiniteLoopPreventionCounter = 10;
            while (e.getCause() != null && infiniteLoopPreventionCounter > 0) {
                infiniteLoopPreventionCounter--;
                e = e.getCause();
            }
        }
        return e;
    }

    private boolean isPresentAsCause(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor) {
        return isPresentAsCauseHelper(throwableToSearchIn, throwableToSearchFor) != null;
    }

    static Throwable isPresentAsCauseHelper(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor) {
        int infiniteLoopPreventionCounter = 10;
        while (throwableToSearchIn != null && infiniteLoopPreventionCounter > 0) {
            infiniteLoopPreventionCounter--;
            if (throwableToSearchIn.getClass().isAssignableFrom(
                    throwableToSearchFor)) {
                return throwableToSearchIn;
            } else {
                throwableToSearchIn = throwableToSearchIn.getCause();
            }
        }
        return null;
    }
    
    /**
     * Test if certain exception classes exist as a cause in a Throwable 
     */
    public static boolean isPresentAsCause(Throwable throwableToSearchIn,
            Collection<Class<? extends Throwable>> throwableToSearchFor) {
        int infiniteLoopPreventionCounter = 10;
        while (throwableToSearchIn != null && infiniteLoopPreventionCounter > 0) {
            infiniteLoopPreventionCounter--;
            for (Class<? extends Throwable> c: throwableToSearchFor) {
                if (throwableToSearchIn.getClass().isAssignableFrom(c)) {
                    return true;
                }
            }
            throwableToSearchIn = throwableToSearchIn.getCause();
        }
        return false;
    }

    protected ClientException generateNIWSException(String uri, Throwable e){
        ClientException niwsClientException;
        if (isPresentAsCause(e, java.net.SocketTimeoutException.class)) {
            niwsClientException = generateTimeoutNIWSException(uri, e);
        }else if (e.getCause() instanceof java.net.UnknownHostException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.UNKNOWN_HOST_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e.getCause() instanceof java.net.ConnectException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.CONNECT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e.getCause() instanceof java.net.NoRouteToHostException){
            niwsClientException = new ClientException(
                    ClientException.ErrorType.NO_ROUTE_TO_HOST_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri,
                    e);
        }else if (e instanceof ClientException){
            niwsClientException = (ClientException)e;
        }else {
            niwsClientException = new ClientException(
                ClientException.ErrorType.GENERAL,
                "Unable to execute RestClient request for URI:" + uri,
                e);
        }
        return niwsClientException;
    }

    private boolean isPresentAsCause(Throwable throwableToSearchIn,
            Class<? extends Throwable> throwableToSearchFor, String messageSubStringToSearchFor) {
        Throwable throwableFound = isPresentAsCauseHelper(throwableToSearchIn, throwableToSearchFor);
        if(throwableFound != null) {
            return throwableFound.getMessage().contains(messageSubStringToSearchFor);
        }
        return false;
    }
    private ClientException generateTimeoutNIWSException(String uri, Throwable e){
        ClientException niwsClientException;
        if (isPresentAsCause(e, java.net.SocketTimeoutException.class,
                "Read timed out")) {
            niwsClientException = new ClientException(
                    ClientException.ErrorType.READ_TIMEOUT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri + ":"
                            + getDeepestCause(e).getMessage(), e);
        } else {
            niwsClientException = new ClientException(
                    ClientException.ErrorType.SOCKET_TIMEOUT_EXCEPTION,
                    "Unable to execute RestClient request for URI:" + uri + ":"
                            + getDeepestCause(e).getMessage(), e);
        }
        return niwsClientException;
    }

    /**
     * This is called after a response is received or an exception is thrown from the client
     * to update related stats.  
     */
    protected void noteRequestCompletion(ServerStats stats, ClientRequest request, IResponse response, Throwable e, long responseTime) {        
        try {
            if (stats != null) {
                stats.decrementActiveRequestsCount();
                stats.incrementNumRequests();
                stats.noteResponseTime(responseTime);
                if (response != null) {
                    stats.clearSuccessiveConnectionFailureCount();                    
                }
            }            
        } catch (Throwable ex) {
            logger.error("Unexpected exception", ex);
        }            
    }
       
    /**
     * This is usually called just before client execute a request.
     */
    protected void noteOpenConnection(ServerStats serverStats, ClientRequest request) {
        if (serverStats == null) {
            return;
        }
        try {
            serverStats.incrementActiveRequestsCount();
        } catch (Throwable e) {
            logger.info("Unable to note Server Stats:", e);
        }
    }

      
    /**
     * Derive scheme and port from a partial URI. For example, for HTTP based client, the URI with 
     * only path "/" should return "http" and 80, whereas the URI constructed with scheme "https" and
     * path "/" should return
     * "https" and 443. This method is called by {@link #computeFinalUriWithLoadBalancer(ClientRequest)}
     * to get the complete executable URI.
     * 
     */
    protected Pair<String, Integer> deriveSchemeAndPortFromPartialUri(T request) {
        URI theUrl = request.getUri();
        boolean isSecure = false;
        String scheme = theUrl.getScheme();
        if (scheme != null) {
            isSecure =  scheme.equalsIgnoreCase("https");
        }
        int port = theUrl.getPort();
        if (port < 0 && !isSecure){
            port = 80;
        } else if (port < 0 && isSecure){
            port = 443;
        }
        if (scheme == null){
            if (isSecure) {
                scheme = "https";
            } else {
                scheme = "http";
            }
        }
        return new Pair<String, Integer>(scheme, port);
    }
    
    /**
     * Get the default port of the target server given the scheme of vip address if it is available. 
     * Subclass should override it to provider protocol specific default port number if any.
     * 
     * @param scheme from the vip address. null if not present.
     * @return 80 if scheme is http, 443 if scheme is https, -1 else.
     */
    protected int getDefaultPortFromScheme(String scheme) {
        if (scheme == null) {
            return -1;
        }
        if (scheme.equals("http")) {
            return 80;
        } else if (scheme.equals("https")) {
            return 443;
        } else {
            return -1;
        }
    }

        
    /**
     * Derive the host and port from virtual address if virtual address is indeed contains the actual host 
     * and port of the server. This is the final resort to compute the final URI in {@link #computeFinalUriWithLoadBalancer(ClientRequest)}
     * if there is no load balancer available and the request URI is incomplete. Sub classes can override this method
     * to be more accurate or throws ClientException if it does not want to support virtual address to be the
     * same as physical server address.
     * <p>
     *  The virtual address is used by certain load balancers to filter the servers of the same function 
     *  to form the server pool. 
     *  
     */
    protected  Pair<String, Integer> deriveHostAndPortFromVipAddress(String vipAddress) 
            throws URISyntaxException, ClientException {
        Pair<String, Integer> hostAndPort = new Pair<String, Integer>(null, -1);
        URI uri = new URI(vipAddress);
        String scheme = uri.getScheme();
        if (scheme == null) {
            uri = new URI("http://" + vipAddress);
        }
        String host = uri.getHost();
        if (host == null) {
            throw new ClientException("Unable to derive host/port from vip address " + vipAddress);
        }
        int port = uri.getPort();
        if (port < 0) {
            port = getDefaultPortFromScheme(scheme);
        }
        if (port < 0) {
            throw new ClientException("Unable to derive host/port from vip address " + vipAddress);
        }
        hostAndPort.setFirst(host);
        hostAndPort.setSecond(port);
        return hostAndPort;
    }
    
    private boolean isVipRecognized(String vipEmbeddedInUri) {
        if (vipEmbeddedInUri == null) {
            return false;
        }
        if (vipAddresses == null) {
            return false;
        }
        String[] addresses = vipAddresses.split(",");
        for (String address: addresses) {
            if (vipEmbeddedInUri.equalsIgnoreCase(address.trim())) {
                return true;
            }
        }
        return false;
    }
    
    /**
     * Compute the final URI from a partial URI in the request. The following steps are performed:
     * 
     * <li> if host is missing and there is a load balancer, get the host/port from server chosen from load balancer
     * <li> if host is missing and there is no load balancer, try to derive host/port from virtual address set with the client
     * <li> if host is present and the authority part of the URI is a virtual address set for the client, 
     * and there is a load balancer, get the host/port from server chosen from load balancer
     * <li> if host is present but none of the above applies, interpret the host as the actual physical address
     * <li> if host is missing but none of the above applies, throws ClientException
     * 
     * @param original Original URI passed from caller
     * @return new request with the final URI  
     */
    @SuppressWarnings("unchecked")
    protected T computeFinalUriWithLoadBalancer(T original) throws ClientException{
        URI theUrl = original.getUri();

        if (theUrl == null){
            throw new ClientException(ClientException.ErrorType.GENERAL, "NULL URL passed in");
        }

        String host = theUrl.getHost();
        Pair<String, Integer> schemeAndPort = deriveSchemeAndPortFromPartialUri(original);
        String scheme = schemeAndPort.first();
        int port = schemeAndPort.second();
        // Various Supported Cases
        // The loadbalancer to use and the instances it has is based on how it was registered
        // In each of these cases, the client might come in using Full Url or Partial URL
        ILoadBalancer lb = getLoadBalancer();
        Object loadBalancerKey = original.getLoadBalancerKey();
        if (host == null){
            // Partial URL Case
            // well we have to just get the right instances from lb - or we fall back
            if (lb != null){
                Server svc = lb.chooseServer(loadBalancerKey);
                if (svc == null){
                    throw new ClientException(ClientException.ErrorType.GENERAL,
                            "LoadBalancer returned null Server for :"
                            + clientName);
                }
                host = svc.getHost();
                port = svc.getPort();

                if (host == null){
                    throw new ClientException(ClientException.ErrorType.GENERAL,
                            "Invalid Server for :" + svc);
                }
                if (logger.isDebugEnabled()){
                    logger.debug(clientName + " using LB returned Server:" + svc + "for request:" + theUrl);
                }
            } else {
                // No Full URL - and we dont have a LoadBalancer registered to
                // obtain a server
                // if we have a vipAddress that came with the registration, we
                // can use that else we
                // bail out
                if (vipAddresses != null && vipAddresses.contains(",")) {
                    throw new ClientException(
                            ClientException.ErrorType.GENERAL,
                            this.clientName
                                    + "Partial URI of ("
                                    + theUrl
                                    + ") has been sent in to RestClient (with no LB) to be executed."
                                    + " Also, there are multiple vipAddresses and hence RestClient cant pick"
                                    + "one vipAddress to complete this partial uri");
                } else if (vipAddresses != null) {
                    try {
                        Pair<String,Integer> hostAndPort = deriveHostAndPortFromVipAddress(vipAddresses);
                        host = hostAndPort.first();
                        port = hostAndPort.second();
                    } catch (URISyntaxException e) {
                        throw new ClientException(
                                ClientException.ErrorType.GENERAL,
                                this.clientName
                                        + "Partial URI of ("
                                        + theUrl
                                        + ") has been sent in to RestClient (with no LB) to be executed."
                                        + " Also, the configured/registered vipAddress is unparseable (to determine host and port)");
                    }
                }else{
                    throw new ClientException(
                            ClientException.ErrorType.GENERAL,
                            this.clientName
                                    + " has no LoadBalancer registered and passed in a partial URL request (with no host:port)."
                                    + " Also has no vipAddress registered");
                }
            }
        } else {
            // Full URL Case
            // This could either be a vipAddress or a hostAndPort or a real DNS
            // if vipAddress or hostAndPort, we just have to consult the loadbalancer
            // but if it does not return a server, we should just proceed anyways
            // and assume its a DNS
            // For restClients registered using a vipAddress AND executing a request
            // by passing in the full URL (including host and port), we should only
            // consult lb IFF the URL passed is registered as vipAddress in Discovery
            boolean shouldInterpretAsVip = false;

            if (lb != null) {
                shouldInterpretAsVip = isVipRecognized(original.getUri().getAuthority());
            }
            if (shouldInterpretAsVip) {
                Server svc = lb.chooseServer(loadBalancerKey);
                if (svc != null){
                    host = svc.getHost();
                    port = svc.getPort();
                    if (host == null){
                        throw new ClientException(ClientException.ErrorType.GENERAL,
                                "Invalid Server for :" + svc);
                    }
                    if (logger.isDebugEnabled()){
                        logger.debug("using LB returned Server:" + svc + "for request:" + theUrl);
                    }
                }else{
                    // just fall back as real DNS
                    if (logger.isDebugEnabled()){
                        logger.debug(host + ":" + port + " assumed to be a valid VIP address or exists in the DNS");
                    }
                }
            } else {
             // consult LB to obtain vipAddress backed instance given full URL
                //Full URL execute request - where url!=vipAddress
               if (logger.isDebugEnabled()){
                   logger.debug("Using full URL passed in by caller (not using LB/Discovery):" + theUrl);
               }
            }
        }
        // end of creating final URL
        if (host == null){
            throw new ClientException(ClientException.ErrorType.GENERAL,"Request contains no HOST to talk to");
        }
        // just verify that at this point we have a full URL

        try {
            StringBuilder sb = new StringBuilder();
            sb.append(scheme).append("://");
            if (!Strings.isNullOrEmpty(theUrl.getRawUserInfo())) {
                sb.append(theUrl.getRawUserInfo()).append("@");
            }
            sb.append(host);
            if (port >= 0) {
                sb.append(":").append(port);
            }
            sb.append(theUrl.getRawPath());
            if (!Strings.isNullOrEmpty(theUrl.getRawQuery())) {
                sb.append("?").append(theUrl.getRawQuery());
            }
            if (!Strings.isNullOrEmpty(theUrl.getRawFragment())) {
                sb.append("#").append(theUrl.getRawFragment());
            }
            URI newURI = new URI(sb.toString());
            return (T) original.replaceUri(newURI);            
        } catch (URISyntaxException e) {
            throw new ClientException(ClientException.ErrorType.GENERAL, e.getMessage());
        }
    }
    
    protected boolean isRetriable(T request) {
        if (request.isRetriable()) {
            return true;            
        } else {
            boolean retryOkayOnOperation = okToRetryOnAllOperations;
            IClientConfig overriddenClientConfig = request.getOverrideConfig();
            if (overriddenClientConfig != null) {
                retryOkayOnOperation = overriddenClientConfig.getPropertyAsBoolean(CommonClientConfigKey.RequestSpecificRetryOn, okToRetryOnAllOperations);
            }
            return retryOkayOnOperation;
        }
    }
    
    protected int getRetriesNextServer(IClientConfig overriddenClientConfig) {
        int numRetries = maxAutoRetriesNextServer;
        if (overriddenClientConfig != null) {
            numRetries = overriddenClientConfig.getPropertyAsInteger(CommonClientConfigKey.MaxAutoRetriesNextServer, maxAutoRetriesNextServer);
        }
        return numRetries;
    }
    
    public final ServerStats getServerStats(Server server) {
        ServerStats serverStats = null;
        ILoadBalancer lb = this.getLoadBalancer();
        if (lb instanceof AbstractLoadBalancer){
            LoadBalancerStats lbStats = ((AbstractLoadBalancer) lb).getLoadBalancerStats();
            serverStats = lbStats.getSingleServerStat(server);
        }
        return serverStats;

    }

    protected int getNumberRetriesOnSameServer(IClientConfig overriddenClientConfig) {
        int numRetries =  maxAutoRetries;
        if (overriddenClientConfig!=null){
            try {
                numRetries = overriddenClientConfig.getPropertyAsInteger(CommonClientConfigKey.MaxAutoRetries, maxAutoRetries);
            } catch (Exception e) {
                logger.warn("Invalid maxRetries requested for RestClient:" + this.clientName);
            }
        }
        return numRetries;
    }
    
    protected boolean handleSameServerRetry(URI uri, int currentRetryCount, int maxRetries, Throwable e) {
        if (currentRetryCount > maxRetries) {
            return false;
        }
        logger.debug("Exception while executing request which is deemed retry-able, retrying ..., SAME Server Retry Attempt#: {}, URI: {}",  
                currentRetryCount, uri);
        return true;
    }

    public final LoadBalancerErrorHandler<? super T, ? super S> getErrorHandler() {
        return errorHandler;
    }

    public final void setErrorHandler(
            LoadBalancerErrorHandler<? super T, ? super S> errorHandler) {
        this.errorHandler = errorHandler;
    }

    public final boolean isOkToRetryOnAllOperations() {
        return okToRetryOnAllOperations;
    }

    public final void setOkToRetryOnAllOperations(boolean okToRetryOnAllOperations) {
        this.okToRetryOnAllOperations = okToRetryOnAllOperations;
    }
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/client/ResponseWithTypedEntity.java;<<<<<<< MINE
package com.netflix.client;

import com.google.common.reflect.TypeToken;

public interface ResponseWithTypedEntity extends IResponse {
    public <T> T get(Class<T> type) throws ClientException;
    
    public <T> T get(TypeToken<T> type) throws ClientException;
}||||||| BASE
=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.client;

import java.io.InputStream;

import com.google.common.reflect.TypeToken;

/**
 * A response type that includes a typed entity in its content.
 * 
 * @author awang
 *
 */
public interface ResponseWithTypedEntity extends IResponse {
    
    public <T> T getEntity(Class<T> type) throws Exception;
    
    public <T> T getEntity(TypeToken<T> type) throws Exception;
    
    public boolean hasEntity();
    
    public InputStream getInputStream() throws ClientException;
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/client/AsyncLoadBalancingClient.java;<<<<<<< MINE
package com.netflix.client;

import java.net.URI;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.netflix.loadbalancer.Server;
import com.netflix.loadbalancer.ServerStats;
import com.netflix.servo.monitor.Monitors;
import com.netflix.servo.monitor.Stopwatch;
import com.netflix.servo.monitor.Timer;

public class AsyncLoadBalancingClient<Request extends ClientRequest, Response extends ResponseWithTypedEntity>
        extends LoadBalancerContext implements AsyncClient<Request, Response> {
    
    private AsyncClient<Request, Response> asyncClient;
    private static final Logger logger = LoggerFactory.getLogger(AsyncLoadBalancingClient.class);
    private Timer tracer;


    public AsyncLoadBalancingClient(AsyncClient<Request, Response> asyncClient) {
        super();
        this.asyncClient = asyncClient;
    }

    protected AsyncLoadBalancingClient() {
    }

    @Override
    public void execute(final Request request, final ResponseCallback<Response> callback)
            throws ClientException {
        final AtomicInteger retries = new AtomicInteger(0);
        final boolean retryOkayOnOperation = isRetriable(request);

        final int numRetriesNextServer = getRetriesNextServer(request.getOverrideConfig()); 
        Request resolved = computeFinalUriWithLoadBalancer(request);
        asyncExecuteOnSingleServer(resolved, new ResponseCallback<Response>() {

            @Override
            public void onResponseReceived(Response response) {
                callback.onResponseReceived(response);
            }

            @Override
            public void onException(Throwable e) {
                boolean shouldRetry = false;
                if (e instanceof ClientException) {
                    // we dont want to retry for PUT/POST and DELETE, we can for GET
                    shouldRetry = retryOkayOnOperation && numRetriesNextServer > 0;
                }
                if (shouldRetry) {
                    if (retries.incrementAndGet() > numRetriesNextServer) {
                        callback.onException(new ClientException(
                                ClientException.ErrorType.NUMBEROF_RETRIES_NEXTSERVER_EXCEEDED,
                                "NUMBER_OF_RETRIES_NEXTSERVER_EXCEEDED :"
                                + numRetriesNextServer
                                + " retries, while making a RestClient call for:"
                                + request.getUri() + ":" +  getDeepestCause(e).getMessage(), e));
                    }
                    logger.error("Exception while executing request which is deemed retry-able, retrying ..., Next Server Retry Attempt#:"
                            + retries
                            + ", URI tried:"
                            + request.getUri());
                    try {
                        asyncExecuteOnSingleServer(computeFinalUriWithLoadBalancer(request), this);
                    } catch (ClientException e1) {
                        callback.onException(e1);
                    }
                } else {
                    if (e instanceof ClientException) {
                        callback.onException(e);
                    } else {
                        callback.onException(new ClientException(
                                ClientException.ErrorType.GENERAL,
                                "Unable to execute request for URI:" + request.getUri(),
                                e));
                    }
                }
            }
            
        });
    }

    
    
    private Timer getExecuteTracer() {
        if (tracer == null) {
            tracer = Monitors.newTimer(this.getClientName() + "_ExecutionTimer", TimeUnit.MILLISECONDS);
        }
        return tracer;
    }
    
    /**
     * Execute the request on single server after the final URI is calculated. This method takes care of
     * retries and update server stats.
     * @throws ClientException 
     *  
     */
    protected void asyncExecuteOnSingleServer(final Request request, final ResponseCallback<Response> callback) throws ClientException {
        final AtomicInteger retries = new AtomicInteger(0);

        final boolean retryOkayOnOperation = request.isRetriable()? true: okToRetryOnAllOperations;
        final int numRetries = getNumberRetriesOnSameServer(request.getOverrideConfig());
        final URI uri = request.getUri();
        Server server = new Server(uri.getHost(), uri.getPort());
        final ServerStats serverStats = getServerStats(server);
        final Stopwatch tracer = getExecuteTracer().start();
        noteOpenConnection(serverStats, request);
        asyncClient.execute(request, new ResponseCallback<Response>() {
            private Response thisResponse;
            private Throwable thisException;
            @Override
            public void onResponseReceived(Response response) {
                thisResponse = response;
                onComplete();
                callback.onResponseReceived(response);
            }

            @Override
            public void onException(Throwable e) {
                thisException = e;
                onComplete();
                if (serverStats != null) {
                    serverStats.addToFailureCount();
                }
                if (isCircuitBreakerException(e) && serverStats != null) {
                    serverStats.incrementSuccessiveConnectionFailureCount();
                }
                boolean shouldRetry = retryOkayOnOperation && numRetries > 0 && isRetriableException(e);
                if (shouldRetry) {
                    if (retries.incrementAndGet() > numRetries) {
                        callback.onException(new ClientException(ClientException.ErrorType.NUMBEROF_RETRIES_EXEEDED,
                                "NUMBEROFRETRIESEXEEDED :" + numRetries + " retries, while making a RestClient call for: " + uri,
                                e !=null? e: new RuntimeException()));
                    } else {
                        logger.error("Exception while executing request which is deemed retry-able, retrying ..., SAME Server Retry Attempt #:" +
                                retries.get() +
                                ", URI:" +
                                uri);
                        try {
                            Thread.sleep((int) Math.pow(2.0, retries.get()) * 100); 
                        } catch (InterruptedException ex) {
                        }
                        tracer.start();
                        noteOpenConnection(serverStats, request);
                        try {
                            asyncClient.execute(request, this);
                        } catch (ClientException ex) {
                            callback.onException(ex);
                        }
                    } 
                } else {
                    ClientException clientException = generateNIWSException(uri.toString(), e);
                    callback.onException(clientException);
                }
            }
            
            private void onComplete() {
                tracer.stop();
                long duration = tracer.getDuration(TimeUnit.MILLISECONDS);
                noteRequestCompletion(serverStats, request, thisResponse, thisException, duration);
            }            
        });
    }

    
    @Override
    protected boolean isCircuitBreakerException(Throwable e) {
        return true;
    }

    @Override
    protected boolean isRetriableException(Throwable e) {
        return true;
    }
}||||||| BASE
=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.client;

import java.io.IOException;
import java.net.URI;
import java.nio.ByteBuffer;
import java.util.List;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.Future;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicReference;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.collect.Lists;
import com.netflix.client.config.IClientConfig;
import com.netflix.loadbalancer.AvailabilityFilteringRule;
import com.netflix.loadbalancer.ILoadBalancer;
import com.netflix.loadbalancer.Server;
import com.netflix.loadbalancer.ServerStats;
import com.netflix.serialization.ContentTypeBasedSerializerKey;
import com.netflix.serialization.Deserializer;
import com.netflix.serialization.SerializationFactory;
import com.netflix.serialization.Serializer;
import com.netflix.servo.monitor.Stopwatch;

/**
 * An asynchronous client that is capable of load balancing with an {@link ILoadBalancer}. It delegates the 
 * asynchronous call to the {@link AsyncClient} passed in from the constructor. As with synchronous I/O client,
 * the URI in the request can be a partial URI without host name or port. The load balancer will be responsible
 * to choose a server and calculate the final URI. If multiple retries are configured, all intermediate failures
 * will be hidden from the caller of the APIs in this class. All call results will be feed back to the load balancer
 * as server statistics to help it choosing the next server, for example, avoiding servers with consecutive connection
 * or read failures or high concurrent requests given the {@link AvailabilityFilteringRule}.
 * 
 * @author awang
 *
 * @param <T> Request type
 * @param <S> Response type
 * @param <U> Type of storage used for delivering partial content, for example, {@link ByteBuffer}
 * @param <V> Type of key to find {@link Serializer} and {@link Deserializer} for the content. For example, for HTTP communication,
 *            the key type is {@link ContentTypeBasedSerializerKey}
 */
public class AsyncLoadBalancingClient<T extends ClientRequest, S extends IResponse, U, V>
        extends LoadBalancerContext<T, S> implements AsyncClient<T, S, U, V> {
    
    private AsyncClient<T, S, U, V> asyncClient;
    private static final Logger logger = LoggerFactory.getLogger(AsyncLoadBalancingClient.class);
    
    public AsyncLoadBalancingClient(AsyncClient<T, S, U, V> asyncClient) {
        super();
        this.asyncClient = asyncClient;
    }
    
    public AsyncLoadBalancingClient(AsyncClient<T, S, U, V> asyncClient, IClientConfig clientConfig) {
        super(clientConfig);
        this.asyncClient = asyncClient;
    }

    protected AsyncLoadBalancingClient() {
    }

    private Future<S> getFuture(final AtomicReference<Future<S>> futurePointer, final CallbackDelegate<S, ?> callbackDelegate) {
        return new Future<S>() {

            @Override
            public boolean cancel(boolean arg0) {
                Future<S> current = futurePointer.get();
                if (current != null) {                    
                    return current.cancel(arg0);
                } else {
                    return false;
                }
            }

            @Override
            public S get() throws InterruptedException, ExecutionException {
                return callbackDelegate.getCompletedResponse();
            }

            @Override
            public S get(long arg0, TimeUnit arg1)
                    throws InterruptedException, ExecutionException,
                    TimeoutException {
                return callbackDelegate.getCompletedResponse(arg0,  arg1);
            }

            @Override
            public boolean isCancelled() {
                Future<S> current = futurePointer.get();
                if (current != null) {                    
                    return current.isCancelled();
                } else {
                    return false;
                }
            }

            @Override
            public boolean isDone() {
                return callbackDelegate.isDone();
            }
            
        };
    }
    
    private static class CallbackDelegate<T extends IResponse, E> implements ResponseCallback<T, E> {

        private ResponseCallback<T, E> callback;

        public CallbackDelegate(ResponseCallback<T, E> callback) {
            this.callback = callback;
        }
        
        private CountDownLatch latch = new CountDownLatch(1);
        private volatile T completeResponse = null; 
        
        private volatile Throwable exception = null;
        
        T getCompletedResponse() throws InterruptedException, ExecutionException {
            latch.await();
            if (completeResponse != null) {
                return completeResponse;
            } else if (exception != null) {
                throw new ExecutionException(exception);
            } else {
                throw new IllegalStateException("No response or exception is received");
            }
        }

        T getCompletedResponse(long time, TimeUnit timeUnit) throws InterruptedException, TimeoutException, ExecutionException {
            if (latch.await(time, timeUnit)) {
                if (completeResponse != null) {
                    return completeResponse;
                } else if (exception != null) {
                    throw new ExecutionException(exception);
                } else {
                    throw new IllegalStateException("No response or exception is received");
                }
            } else {
                throw new TimeoutException();
            }
        }
               
        boolean isDone() {
            return latch.getCount() <= 0;
        }

        @Override
        public void completed(T response) {
            completeResponse = response;
            latch.countDown();
            if (callback != null) {
                callback.completed(response);
            }
        }

        @Override
        public void failed(Throwable e) {
            exception = e;
            latch.countDown();
            if (callback != null) {
                callback.failed(e);
            }
        }

        @Override
        public void cancelled() {
            latch.countDown();
            if (callback != null) {
                callback.cancelled();
            }
        }

        @Override
        public void responseReceived(T response) {
            if (callback != null) {
                callback.responseReceived(response);
            }
        }

        @Override
        public void contentReceived(E content) {
            if (callback != null) {
                callback.contentReceived(content);
            }
        }
    }

    /**
     * Execute a request with callback invoked after the full response is buffered. If multiple retries are configured,
     * all intermediate failures will be hidden from caller and only the last successful response or failure
     * will be used for callback.
     * 
     * @param request Request to execute. It can contain a partial URI without host or port as
     * the load balancer will calculate the final URI after choosing a server.
     */
    @Override
    public Future<S> execute(final T request, final BufferedResponseCallback<S> callback)
            throws ClientException {
        return execute(request, null, callback);
    }
    
    /**
     * Execute a request with callback. If multiple retries are configured,
     * all intermediate failures will be hidden from caller and only the last successful response or failure
     * will be used for callback.
     * 
     * @param request Request to execute. It can contain a partial URI without host or port as
     * the load balancer will calculate the final URI after choosing a server.
     */
    @Override
    public <E> Future<S> execute(final T request, final StreamDecoder<E, U> decoder, final ResponseCallback<S, E> callback)
            throws ClientException {
        final AtomicInteger retries = new AtomicInteger(0);
        final boolean retryOkayOnOperation = isRetriable(request);

        final int numRetriesNextServer = getRetriesNextServer(request.getOverrideConfig()); 
        T resolved = computeFinalUriWithLoadBalancer(request);
        
        final CallbackDelegate<S, E> delegate = new CallbackDelegate<S, E>(callback);
        final AtomicReference<Future<S>> currentRunningTask = new AtomicReference<Future<S>>();
        
        asyncExecuteOnSingleServer(resolved, decoder, new ResponseCallback<S, E>() {

            @Override
            public void completed(S response) {
                delegate.completed(response);
            }

            @Override
            public void failed(Throwable e) {
                boolean shouldRetry = retryOkayOnOperation && numRetriesNextServer > 0 && errorHandler.isRetriableException(request, e, false);
                if (shouldRetry) {
                    if (retries.incrementAndGet() > numRetriesNextServer) {
                        delegate.failed(new ClientException(
                                ClientException.ErrorType.NUMBEROF_RETRIES_NEXTSERVER_EXCEEDED,
                                "NUMBER_OF_RETRIES_NEXTSERVER_EXCEEDED :"
                                + numRetriesNextServer
                                + " retries, while making a RestClient call for:"
                                + request.getUri() + ":" +  getDeepestCause(e).getMessage(), e));
                        return;
                    }
                    try {
                        T newRequest = computeFinalUriWithLoadBalancer(request);
                        logger.debug("Exception while executing request which is deemed retry-able, retrying ..., Next Server Retry Attempt#: {}, URI: {}",
                                retries, newRequest.getUri());
                        asyncExecuteOnSingleServer(newRequest, decoder, this, currentRunningTask);
                    } catch (ClientException e1) {
                        delegate.failed(e1);
                    }
                } else {
                    delegate.failed(e);
                }
            }

            @Override
            public void cancelled() {
                delegate.cancelled();
            }

            @Override
            public void responseReceived(S response) {
                delegate.responseReceived(response);
            }

            @Override
            public void contentReceived(E content) {
                delegate.contentReceived(content);
            }
            
        }, currentRunningTask);
        return getFuture(currentRunningTask, delegate);
    }

    /**
     * Execute the request on single server after the final URI is calculated. This method takes care of
     * retries and update server stats.
     * @throws ClientException 
     *  
     */
    protected <E> void asyncExecuteOnSingleServer(final T request, final StreamDecoder<E, U> decoder, 
            final ResponseCallback<S, E> callback, final AtomicReference<Future<S>> currentRunningTask) throws ClientException {
        final AtomicInteger retries = new AtomicInteger(0);

        final boolean retryOkayOnOperation = request.isRetriable()? true: okToRetryOnAllOperations;
        final int numRetries = getNumberRetriesOnSameServer(request.getOverrideConfig());
        final URI uri = request.getUri();
        Server server = new Server(uri.getHost(), uri.getPort());
        final ServerStats serverStats = getServerStats(server);
        final Stopwatch tracer = getExecuteTracer().start();
        noteOpenConnection(serverStats, request);
        Future<S> future = asyncClient.execute(request, decoder, new ResponseCallback<S, E>() {
            private S thisResponse;
            private Throwable thisException;
            @Override
            public void completed(S response) {
                thisResponse = response;
                onComplete();
                callback.completed(response);
            }

            @Override
            public void failed(Throwable e) {
                thisException = e;
                onComplete();
                if (serverStats != null) {
                    serverStats.addToFailureCount();
                }
                if (errorHandler.isCircuitTrippingException(e) && serverStats != null) {
                    serverStats.incrementSuccessiveConnectionFailureCount();
                }
                boolean shouldRetry = retryOkayOnOperation && numRetries > 0 && errorHandler.isRetriableException(request, e, true);
                if (shouldRetry) {
                    if (!handleSameServerRetry(uri, retries.incrementAndGet(), numRetries, e)) {
                        callback.failed(new ClientException(ClientException.ErrorType.NUMBEROF_RETRIES_EXEEDED,
                                "NUMBEROFRETRIESEXEEDED :" + numRetries + " retries, while making a RestClient call for: " + uri, e));                        
                    } else {
                        tracer.start();
                        noteOpenConnection(serverStats, request);
                        try {
                            Future<S> future = asyncClient.execute(request, decoder, this);
                            currentRunningTask.set(future);
                        } catch (ClientException ex) {
                            callback.failed(ex);
                        }
                    } 
                } else {
                    callback.failed(e);
                }
            }
            
            private void onComplete() {
                tracer.stop();
                long duration = tracer.getDuration(TimeUnit.MILLISECONDS);
                noteRequestCompletion(serverStats, request, thisResponse, thisException, duration);
            }

            @Override
            public void cancelled() {
                onComplete();
                callback.cancelled();
            }

            @Override
            public void responseReceived(S response) {
                if (errorHandler.isCircuitTrippinErrorgResponse(response)) {
                    serverStats.incrementSuccessiveConnectionFailureCount();
                }
                callback.responseReceived(response);
            }

            @Override
            public void contentReceived(E content) {
                callback.contentReceived(content);
            }            
        });
        currentRunningTask.set(future);
    }

    /**
     * Execute the same request that might be sent to multiple servers (as back up requests) if
     * no response is received within the timeout. This method delegates to 
     * {@link AsyncBackupRequestsExecutor#executeWithBackupRequests(AsyncClient, List, long, TimeUnit, StreamDecoder, ResponseCallback)} 
     *
     * @param request Request to execute. It can contain a partial URI without host or port as
     * the load balancer will calculate the final URI after choosing a server.
     * @param numServers the maximal number of servers to try before getting a response
     */
    public <E> AsyncBackupRequestsExecutor.ExecutionResult<S> executeWithBackupRequests(final T request,
            final int numServers, long timeoutIntervalBetweenRequests, TimeUnit unit,
            final StreamDecoder<E, U> decoder,
            
            final ResponseCallback<S, E> callback)
            throws ClientException {
        final List<T> requests = Lists.newArrayList();
        for (int i = 0; i < numServers; i++) {
            requests.add(computeFinalUriWithLoadBalancer(request));
        }
        return AsyncBackupRequestsExecutor.executeWithBackupRequests(this,  requests, timeoutIntervalBetweenRequests, unit, decoder, callback);
    }

    
    @Override
    public void close() throws IOException {
        if (asyncClient != null) {
            asyncClient.close();
        }
    }

    @Override
    public void addSerializationFactory(SerializationFactory<V> factory) {
        asyncClient.addSerializationFactory(factory);
    }

    /**
     * Execute a request where the future will be ready when full response is buffered. If multiple retries are configured,
     * all intermediate failures will be hidden from caller and only the last successful response or failure
     * will be used for creating the future.
     * 
     * @param request Request to execute. It can contain a partial URI without host or port as
     * the load balancer will calculate the final URI after choosing a server.
     */

    @Override
    public Future<S> execute(T request) throws ClientException {
        return execute(request, null);
    }
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/main/java/com/netflix/client/ResponseCallback.java;<<<<<<< MINE
package com.netflix.client;

public interface ResponseCallback<R extends ResponseWithTypedEntity> {
    public void onResponseReceived(R response);

    public void onException(Throwable e);
}||||||| BASE
=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.client;

/**
 * Callback for asynchronous communication.
 * 
 * @author awang
 *
 * @param <T> Type of response, which is protocol specific
 * @param <E> Type of of object that can be formed from partial 
 *             content in the native stream. See {@link StreamDecoder}.
 */
public interface ResponseCallback<T extends IResponse, E> {
    /**
     * Invoked when all communications are successful and content is consumed.
     */
    public void completed(T response);

    /**
     * Invoked when any error happened in the communication or content consumption. 
     */
    public void failed(Throwable e);

    /**
     * Invoked if the I/O operation is cancelled after it is started.
     */
    public void cancelled();
    
    /**
     * Invoked when the initial response is received. For example, the status code and headers
     * of HTTP response is received.
     */
    public void responseReceived(T response);

    /**
     * Invoked when decoded content is delivered from {@link StreamDecoder}.
     */
    public void contentReceived(E content);    
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-core/src/test/java/com/netflix/serialization/JacksonSerializerTest.java;<<<<<<< MINE
package com.netflix.serialization;

import static org.junit.Assert.*;

import java.util.List;

import org.junit.Test;
import com.google.common.reflect.TypeToken;
import com.google.common.collect.Lists;

public class JacksonSerializerTest {    
    @Test
    public void testSerializeList() throws Exception {
        List<Person> people = Lists.newArrayList();
        for (int i = 0; i < 3; i++) {
            people.add(new Person("person " + i, i));
        }
        JacksonSerializationFactory factory = new JacksonSerializationFactory();
        ContentTypeBasedSerializerKey key = new ContentTypeBasedSerializerKey("application/json", new TypeToken<List<Person>>(){});
        Serializer serializer = factory.getSerializer(key).get();
        String content = new String(serializer.serialize(people), "UTF-8");
        Deserializer deserializer = factory.getDeserializer(key).get();
        List<Person> list = deserializer.deserialize(content.getBytes("UTF-8"), new TypeToken<List<Person>>(){});
        assertEquals(people, list);
        Person person = new Person("ribbon", 1);
        byte[] bytes = serializer.serialize(person);
        Person deserialized = deserializer.deserialize(bytes, TypeToken.of(Person.class));
        assertEquals(person, deserialized);
        deserialized = deserializer.deserialize(bytes, Person.class);
        assertEquals(person, deserialized);
    }
}

class Person {
    public String name;
    public int age;
    public Person() {}
    public Person(String name, int age) {
        super();
        this.name = name;
        this.age = age;
    }
    @Override
    public String toString() {
        return "Person [name=" + name + ", age=" + age + "]";
    }
    @Override
    public int hashCode() {
        final int prime = 31;
        int result = 1;
        result = prime * result + age;
        result = prime * result + ((name == null) ? 0 : name.hashCode());
        return result;
    }
    @Override
    public boolean equals(Object obj) {
        if (this == obj)
            return true;
        if (obj == null)
            return false;
        if (getClass() != obj.getClass())
            return false;
        Person other = (Person) obj;
        if (age != other.age)
            return false;
        if (name == null) {
            if (other.name != null)
                return false;
        } else if (!name.equals(other.name))
            return false;
        return true;
    }
}||||||| BASE
=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.serialization;

import static org.junit.Assert.*;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.util.List;

import org.codehaus.jackson.map.ObjectMapper;
import org.junit.Test;
import com.google.common.reflect.TypeToken;
import com.google.common.collect.Lists;

public class JacksonSerializerTest {    
    @SuppressWarnings("serial")
    @Test
    public void testSerializeList() throws Exception {
        List<Person> people = Lists.newArrayList();
        for (int i = 0; i < 3; i++) {
            people.add(new Person("person " + i, i));
        }
        JacksonSerializationFactory factory = new JacksonSerializationFactory();
        ContentTypeBasedSerializerKey key = new ContentTypeBasedSerializerKey("application/json", new TypeToken<List<Person>>(){});
        Serializer serializer = factory.getSerializer(key).get();
        String content = new String(serializeToBytes(people, serializer), "UTF-8");
        Deserializer deserializer = factory.getDeserializer(key).get();
        List<Person> list = deserializer.deserialize(new ByteArrayInputStream(content.getBytes("UTF-8")), new TypeToken<List<Person>>(){});
        assertEquals(people, list);
        Person person = new Person("ribbon", 1);
        byte[] bytes = serializeToBytes(person, serializer);
        Person deserialized = deserializer.deserialize(new ByteArrayInputStream(bytes), TypeToken.of(Person.class));
        assertEquals(person, deserialized);
        deserialized = deserializer.deserialize(new ByteArrayInputStream(bytes), TypeToken.of(Person.class));
        assertEquals(person, deserialized);
        
        ObjectMapper mapper = new ObjectMapper();
        deserialized = (Person) mapper.readValue(bytes, TypeToken.of(Person.class).getRawType());
        assertEquals(person, deserialized);
    }
    
    
    private byte[] serializeToBytes(Object obj, Serializer serializer) throws Exception {
        ByteArrayOutputStream bout = new ByteArrayOutputStream();
        serializer.serialize(bout, obj);
        return bout.toByteArray();
    }
}

class Person {
    public String name;
    public int age;
    public Person() {}
    public Person(String name, int age) {
        super();
        this.name = name;
        this.age = age;
    }
    @Override
    public String toString() {
        return "Person [name=" + name + ", age=" + age + "]";
    }
    @Override
    public int hashCode() {
        final int prime = 31;
        int result = 1;
        result = prime * result + age;
        result = prime * result + ((name == null) ? 0 : name.hashCode());
        return result;
    }
    @Override
    public boolean equals(Object obj) {
        if (this == obj)
            return true;
        if (obj == null)
            return false;
        if (getClass() != obj.getClass())
            return false;
        Person other = (Person) obj;
        if (age != other.age)
            return false;
        if (name == null) {
            if (other.name != null)
                return false;
        } else if (!name.equals(other.name))
            return false;
        return true;
    }
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-httpclient/src/main/java/com/netflix/niws/client/http/RestClient.java;<<<<<<< MINE
    protected boolean isRetriableException(Throwable e) {
        boolean shouldRetry = isConnectException(e) || isSocketException(e);
||||||| BASE
    protected boolean isRetriableException(Exception e) {
        boolean shouldRetry = isConnectException(e) || isSocketException(e);
=======
    protected boolean isRetriableException(Throwable e) {
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_b33a5ed_42c3e4c/rev_b33a5ed-42c3e4c/ribbon-httpclient/src/main/java/com/netflix/niws/client/http/RestClient.java;<<<<<<< MINE
    protected boolean isCircuitBreakerException(Throwable e) {
||||||| BASE
    protected boolean isCircuitBreakerException(Exception e) {
=======
    protected boolean isCircuitBreakerException(Throwable e) {
        if (e instanceof ClientException) {
            ClientException clientException = (ClientException) e;
            if (clientException.getErrorType() == ClientException.ErrorType.SERVER_THROTTLED) {
                return true;
            }
        }
>>>>>>> YOURS
/home/taes/taes/projects/atlas/revisions/rev_799f8e4_9f4a697/rev_799f8e4-9f4a697/atlas-gradle-plugin/dexpatch/src/main/java/com/taobao/android/apatch/ApkPatch.java;<<<<<<< MINE
            prepareClasses = buildPrepareClass(smaliDir2, newFiles, info);
||||||| BASE

            List finalFilterClasses = filterClasses;
            Collections.sort(classes, new Comparator<String>() {
                @Override
                public int compare(String o1, String o2) {
                    if (dexDiffer.getFilter() == null){
                        return 0;
                    }else {
                        return finalFilterClasses.indexOf(o1) - finalFilterClasses.indexOf(o2);
                    }
                }
            });



//            //dex
//            if (APatchTool.debug) {
//                PatchMethodTool.modifyMethod(dexFile.getAbsolutePath(), dexFile.getAbsolutePath(), true);
//            }
//
//            File smaliDir2 = new File(aptchFolder, "smali2");
//            if (!smaliDir2.exists()) {
//                smaliDir2.mkdirs();
//            }
//            try {
//                FileUtils.cleanDirectory(smaliDir2);
//            } catch (IOException e) {
//                throw new RuntimeException(e);
//            }
//            prepareClasses = buildPrepareClass(smaliDir2, newFiles, info);
=======

            List finalFilterClasses = filterClasses;
            Collections.sort(classes, new Comparator<String>() {
                @Override
                public int compare(String o1, String o2) {
                    if (dexDiffer.getFilter() == null){
                        return 0;
                    }else {
                        return finalFilterClasses.indexOf(o1) - finalFilterClasses.indexOf(o2);
                    }
                }
            });



//            //dex
//            if (APatchTool.debug) {
//                PatchMethodTool.modifyMethod(dexFile.getAbsolutePath(), dexFile.getAbsolutePath(), true);
//            }
//
            File smaliDir2 = new File(aptchFolder, "smali2");
            if (!smaliDir2.exists()) {
                smaliDir2.mkdirs();
            }
            try {
                FileUtils.cleanDirectory(smaliDir2);
            } catch (IOException e) {
                throw new RuntimeException(e);
            }
           prepareClasses = buildPrepareClass(smaliDir2, newFiles, info);
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_4be8bde_47e5ff8/rev_4be8bde-47e5ff8/ribbon-core/src/main/java/com/netflix/client/PrimeConnections.java;<<<<<<< MINE
||||||| BASE
/*
*
* Copyright 2013 Netflix, Inc.
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*
*/
package com.netflix.client;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.concurrent.Callable;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Future;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.ThreadFactory;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.netflix.client.config.CommonClientConfigKey;
import com.netflix.client.config.DefaultClientConfigImpl;
import com.netflix.client.config.IClientConfig;
import com.netflix.loadbalancer.Server;
import com.netflix.servo.monitor.Counter;
import com.netflix.servo.monitor.Monitors;
import com.netflix.servo.monitor.Stopwatch;
import com.netflix.servo.monitor.Timer;

/**
 * Prime the connections for a given Client (For those Client that
 * have a LoadBalancer that knows the set of Servers it will connect to) This is
 * mainly done to address those deployment environments (Read EC2) which benefit
 * from a firewall connection/path warmup prior to actual use for live requests.
 * <p>
 * This class is not protocol specific. Actual priming operation is delegated to 
 * instance of {@link IPrimeConnection}, which is instantiated using reflection
 * according to property {@link CommonClientConfigKey#PrimeConnectionsClassName}.
 * 
 * @author stonse
 * @author awang
 * 
 */
public class PrimeConnections {

    public static interface PrimeConnectionListener {
        public void primeCompleted(Server s, Throwable lastException);
    }
    
    static class PrimeConnectionCounters {
        final AtomicInteger numServersLeft;
        final AtomicInteger numServers;
        final AtomicInteger numServersSuccessful;    
        public PrimeConnectionCounters(int initialSize) {
            numServersLeft = new AtomicInteger(initialSize);
            numServers = new AtomicInteger(initialSize);
            numServersSuccessful = new AtomicInteger(0);
        }
    }
    
    private static final Logger logger = LoggerFactory.getLogger(PrimeConnections.class);

    // affordance to change the URI we connect to while "priming"
    // default of "/" is good for most - but if its heavy operation on
    // the server side, then a more lightweight URI can be chosen
    String primeConnectionsURIPath = "/";

    /**
     * Executor service for executing asynchronous requests.
     */

    private ExecutorService executorService;

    private int maxExecutorThreads = 5;

    private long executorThreadTimeout = 30000;

    private String name = "default";

    private int maxTasksPerExecutorQueue = 100;
    
    private float primeRatio = 1.0f;


    int maxRetries = 9;

    long maxTotalTimeToPrimeConnections = 30 * 1000; // default time

    long totalTimeTaken = 0; // Total time taken

    private boolean aSync = true;
        
    Counter totalCounter;
    Counter successCounter;
    Timer initialPrimeTimer;
    
    private IPrimeConnection connector;

    private PrimeConnections() {
    }

    public PrimeConnections(String name, IClientConfig niwsClientConfig) {
        int maxRetriesPerServerPrimeConnection = Integer.valueOf(DefaultClientConfigImpl.DEFAULT_MAX_RETRIES_PER_SERVER_PRIME_CONNECTION);
        long maxTotalTimeToPrimeConnections = Long.valueOf(DefaultClientConfigImpl.DEFAULT_MAX_TOTAL_TIME_TO_PRIME_CONNECTIONS);
        String primeConnectionsURI = DefaultClientConfigImpl.DEFAULT_PRIME_CONNECTIONS_URI;  
        String className = DefaultClientConfigImpl.DEFAULT_PRIME_CONNECTIONS_CLASS;
        try {
            maxRetriesPerServerPrimeConnection = Integer.parseInt(String.valueOf(niwsClientConfig.getProperty(
                    CommonClientConfigKey.MaxRetriesPerServerPrimeConnection, maxRetriesPerServerPrimeConnection)));
        } catch (Exception e) {
            logger.warn("Invalid maxRetriesPerServerPrimeConnection");
        }
        try {
            maxTotalTimeToPrimeConnections = Long.parseLong(String.valueOf(niwsClientConfig.getProperty(
                    CommonClientConfigKey.MaxTotalTimeToPrimeConnections,maxTotalTimeToPrimeConnections)));
        } catch (Exception e) {
            logger.warn("Invalid maxTotalTimeToPrimeConnections");
        }
        primeConnectionsURI = String.valueOf(niwsClientConfig.getProperty(CommonClientConfigKey.PrimeConnectionsURI, primeConnectionsURI));
        float primeRatio = Float.parseFloat(String.valueOf(niwsClientConfig.getProperty(CommonClientConfigKey.MinPrimeConnectionsRatio)));
        className = (String) niwsClientConfig.getProperty(CommonClientConfigKey.PrimeConnectionsClassName, 
        		DefaultClientConfigImpl.DEFAULT_PRIME_CONNECTIONS_CLASS);
        try {
            connector = (IPrimeConnection) Class.forName(className).newInstance();
            connector.initWithNiwsConfig(niwsClientConfig);
        } catch (Exception e) {
            throw new RuntimeException("Unable to initialize prime connections", e);
        }
        setUp(name, maxRetriesPerServerPrimeConnection, 
                maxTotalTimeToPrimeConnections, primeConnectionsURI, primeRatio);        
    }
        
    public PrimeConnections(String name, int maxRetries, 
            long maxTotalTimeToPrimeConnections, String primeConnectionsURI) {
        setUp(name, maxRetries, maxTotalTimeToPrimeConnections, primeConnectionsURI, DefaultClientConfigImpl.DEFAULT_MIN_PRIME_CONNECTIONS_RATIO);
    }

    public PrimeConnections(String name, int maxRetries, 
            long maxTotalTimeToPrimeConnections, String primeConnectionsURI, float primeRatio) {
        setUp(name, maxRetries, maxTotalTimeToPrimeConnections, primeConnectionsURI, primeRatio);
    }

    private void setUp(String name, int maxRetries, 
            long maxTotalTimeToPrimeConnections, String primeConnectionsURI, float primeRatio) {        
        this.name = name;
        this.maxRetries = maxRetries;
        this.maxTotalTimeToPrimeConnections = maxTotalTimeToPrimeConnections;
        this.primeConnectionsURIPath = primeConnectionsURI;        
        this.primeRatio = primeRatio;
        executorService = new ThreadPoolExecutor(1 /* minimum */,
                maxExecutorThreads /* max threads */,
                executorThreadTimeout /*
                                       * timeout - same property as create
                                       * timeout
                                       */, TimeUnit.MILLISECONDS,
                new LinkedBlockingQueue<Runnable>(maxTasksPerExecutorQueue)
                /* Bounded queue with FIFO- bounded to max tasks */,
                new ASyncPrimeConnectionsThreadFactory(name) /*
                                                              * So we can give
                                                              * our Thread a
                                                              * name
                                                              */
        );        
        totalCounter = Monitors.newCounter(name + "_PrimeConnection_TotalCounter");
        successCounter = Monitors.newCounter(name + "_PrimeConnection_SuccessCounter");
        initialPrimeTimer = Monitors.newTimer(name + "_initialPrimeConnectionsTimer", TimeUnit.MILLISECONDS);
        Monitors.registerObject(name + "_PrimeConnection", this);
    }
    
    /**
     * Prime connections, blocking until configured percentage (default is 100%) of target servers are primed 
     * or max time is reached.
     * 
     * @see CommonClientConfigKey#MinPrimeConnectionsRatio
     * @see CommonClientConfigKey#MaxTotalTimeToPrimeConnections
     * 
     */
    public void primeConnections(List<Server> servers) {
        if (servers == null || servers.size() == 0) {
            logger.debug("No server to prime");
            return;
        }
        for (Server server: servers) {
            server.setReadyToServe(false);
        }
        int totalCount = (int) (servers.size() * primeRatio); 
        final CountDownLatch latch = new CountDownLatch(totalCount);
        final AtomicInteger successCount = new AtomicInteger(0);
        final AtomicInteger failureCount= new AtomicInteger(0);
        primeConnectionsAsync(servers, new PrimeConnectionListener()  {            
            @Override
            public void primeCompleted(Server s, Throwable lastException) {
                if (lastException == null) {
                    successCount.incrementAndGet();
                    s.setReadyToServe(true);
                } else {
                    failureCount.incrementAndGet();
                }
                latch.countDown();
            }
        }); 
                
        Stopwatch stopWatch = initialPrimeTimer.start();
        try {
            latch.await(maxTotalTimeToPrimeConnections, TimeUnit.MILLISECONDS);
        } catch (InterruptedException e) {
            logger.error("Priming connection interrupted", e);
        } finally {
            stopWatch.stop();
        }
        printStats(totalCount, successCount.get(), failureCount.get(), stopWatch.getDuration(TimeUnit.MILLISECONDS));
    }
    
    private void printStats(int total, int success, int failure, long totalTime) {
        if (total != success) {
            logger.info("Priming Connections not fully successful");
        } else {
            logger.info("Priming connections fully successful");
        }
        logger.debug("numServers left to be 'primed'="
                + (total - success));
        logger.debug("numServers successfully 'primed'=" + success);
        logger
                .debug("numServers whose attempts not complete exclusively due to max time allocated="
                        + (total - (success + failure)));
        logger.debug("Total Time Taken=" + totalTime
                + " msecs, out of an allocated max of (msecs)="
                + maxTotalTimeToPrimeConnections);
    }

    /*
    private void makeConnectionsASync() {
        Callable<Void> ft = new Callable<Void>() {
            public Void call() throws Exception {
                logger.info("primeConnections ...");
                makeConnections();
                return null;
            }
        };
        outerExecutorService.submit(ft);
    }
    */
    
    /**
     * Prime servers asynchronously.
     * 
     * @param servers
     * @param listener
     */
    public List<Future<Boolean>> primeConnectionsAsync(final List<Server> servers, final PrimeConnectionListener listener) {
        if (servers == null) {
            return Collections.emptyList();
        }
        List<Server> allServers = new ArrayList<Server>();
        allServers.addAll(servers);
        if (allServers.size() == 0){
            logger.debug("RestClient:" + name + ". No nodes/servers to prime connections");
            return Collections.emptyList();
        }        

        logger.info("Priming Connections for RestClient:" + name
                + ", numServers:" + allServers.size());
        List<Future<Boolean>> ftList = new ArrayList<Future<Boolean>>();
        for (Server s : allServers) {
            // prevent the server to be used by load balancer
            // will be set to true when priming is done
            s.setReadyToServe(false);
            if (aSync) {
                Future<Boolean> ftC = null;
                try {
                    ftC = makeConnectionASync(s, listener);
                    ftList.add(ftC);

                } catch (Throwable e) { // NOPMD
                    // It does not really matter if there was an exception,
                    // the goal here is to attempt "priming/opening" the route
                    // in ec2 .. actual http results do not matter
                }
            } else {
                connectToServer(s, listener);
            }
        }   
        return ftList;
    }
    
    private Future<Boolean> makeConnectionASync(final Server s, 
            final PrimeConnectionListener listener) throws InterruptedException, ExecutionException {
        Callable<Boolean> ftConn = new Callable<Boolean>() {
            public Boolean call() throws Exception {
                logger.debug("calling primeconnections ...");
                return connectToServer(s, listener);
            }
        };
        return executorService.submit(ftConn);
    }

    void shutdown() {
        executorService.shutdown();
    }

    private Boolean connectToServer(final Server s, final PrimeConnectionListener listener) {
        int tryNum = 0;
        Exception lastException = null;
        totalCounter.increment();
        boolean success = false;
        do {
            try {
                logger.debug("Executing PrimeConnections request to server " + s + " with path " + primeConnectionsURIPath
                        + ", tryNum=" + tryNum);
                success = connector.connect(s, primeConnectionsURIPath);
                successCounter.increment();
                break;
            } catch (Exception e) {
                // It does not really matter if there was an exception,
                // the goal here is to attempt "priming/opening" the route
                // in ec2 .. actual http results do not matter
                logger.debug("Error connecting to server: {}", e.getMessage());
                lastException = e;
                sleepBeforeRetry(tryNum);
            } 
            logger.debug("server:" + s + ", result=" + success + ", tryNum="
                    + tryNum + ", maxRetries=" + maxRetries);
            tryNum++;
        } while (!success && (tryNum <= maxRetries));
        // set the alive flag so that it can be used by load balancers
        if (listener != null) {
            try {
                listener.primeCompleted(s, lastException);
            } catch (Throwable e) {
                logger.error("Error calling PrimeComplete listener", e);
            }
        }
        logger.debug("Either done, or quitting server:" + s + ", result="
                + success + ", tryNum=" + tryNum + ", maxRetries=" + maxRetries);        
        return success;
    }

    private void sleepBeforeRetry(int tryNum) {
        try {
            int sleep = (tryNum + 1) * 100;
            logger.debug("Sleeping for " + sleep + "ms ...");
            Thread.sleep(sleep); // making this seconds based is too slow
            // i.e. 200ms, 400 ms, 800ms, 1600ms etc.
        } catch (InterruptedException ex) {
        }
    }
    
    static class ASyncPrimeConnectionsThreadFactory implements ThreadFactory {
        private static final AtomicInteger groupNumber = new AtomicInteger(1);
        private final ThreadGroup group;
        private final AtomicInteger threadNumber = new AtomicInteger(1);
        private final String namePrefix;

        ASyncPrimeConnectionsThreadFactory(String name) {
            SecurityManager s = System.getSecurityManager();
            group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); // NOPMD
            namePrefix = "ASyncPrimeConnectionsThreadFactory-" + name + "-"
                    + groupNumber.getAndIncrement() + "-thread-";
        }

        public Thread newThread(Runnable r) {
            Thread t = new Thread(group, r, namePrefix
                    + threadNumber.getAndIncrement(), 0);
            if (!t.isDaemon())
                t.setDaemon(true);
            if (t.getPriority() != Thread.NORM_PRIORITY)
                t.setPriority(Thread.NORM_PRIORITY);
            return t;
        }
    }
}=======
/*
*
* Copyright 2013 Netflix, Inc.
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*
*/
package com.netflix.client;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.concurrent.Callable;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Future;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.ThreadFactory;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.netflix.client.config.CommonClientConfigKey;
import com.netflix.client.config.DefaultClientConfigImpl;
import com.netflix.client.config.IClientConfig;
import com.netflix.loadbalancer.Server;
import com.netflix.servo.monitor.Counter;
import com.netflix.servo.monitor.Monitors;
import com.netflix.servo.monitor.Stopwatch;
import com.netflix.servo.monitor.Timer;

/**
 * Prime the connections for a given Client (For those Client that
 * have a LoadBalancer that knows the set of Servers it will connect to) This is
 * mainly done to address those deployment environments (Read EC2) which benefit
 * from a firewall connection/path warmup prior to actual use for live requests.
 * <p>
 * This class is not protocol specific. Actual priming operation is delegated to 
 * instance of {@link IPrimeConnection}, which is instantiated using reflection
 * according to property {@link CommonClientConfigKey#PrimeConnectionsClassName}.
 * 
 * @author stonse
 * @author awang
 * 
 */
public class PrimeConnections {

    public static interface PrimeConnectionListener {
        public void primeCompleted(Server s, Throwable lastException);
    }
    
    static class PrimeConnectionCounters {
        final AtomicInteger numServersLeft;
        final AtomicInteger numServers;
        final AtomicInteger numServersSuccessful;    
        public PrimeConnectionCounters(int initialSize) {
            numServersLeft = new AtomicInteger(initialSize);
            numServers = new AtomicInteger(initialSize);
            numServersSuccessful = new AtomicInteger(0);
        }
    }
    
    private static final Logger logger = LoggerFactory.getLogger(PrimeConnections.class);

    // affordance to change the URI we connect to while "priming"
    // default of "/" is good for most - but if its heavy operation on
    // the server side, then a more lightweight URI can be chosen
    String primeConnectionsURIPath = "/";

    /**
     * Executor service for executing asynchronous requests.
     */

    private ExecutorService executorService;

    private int maxExecutorThreads = 5;

    private long executorThreadTimeout = 30000;

    private String name = "default";

    private int maxTasksPerExecutorQueue = 100;
    
    private float primeRatio = 1.0f;


    int maxRetries = 9;

    long maxTotalTimeToPrimeConnections = 30 * 1000; // default time

    long totalTimeTaken = 0; // Total time taken

    private boolean aSync = true;
        
    Counter totalCounter;
    Counter successCounter;
    Timer initialPrimeTimer;
    
    private IPrimeConnection connector;

    private PrimeConnections() {
    }

    public PrimeConnections(String name, IClientConfig niwsClientConfig) {
        int maxRetriesPerServerPrimeConnection = Integer.valueOf(DefaultClientConfigImpl.DEFAULT_MAX_RETRIES_PER_SERVER_PRIME_CONNECTION);
        long maxTotalTimeToPrimeConnections = Long.valueOf(DefaultClientConfigImpl.DEFAULT_MAX_TOTAL_TIME_TO_PRIME_CONNECTIONS);
        String primeConnectionsURI = DefaultClientConfigImpl.DEFAULT_PRIME_CONNECTIONS_URI;  
        String className = DefaultClientConfigImpl.DEFAULT_PRIME_CONNECTIONS_CLASS;
        try {
            maxRetriesPerServerPrimeConnection = Integer.parseInt(String.valueOf(niwsClientConfig.getProperty(
                    CommonClientConfigKey.MaxRetriesPerServerPrimeConnection, maxRetriesPerServerPrimeConnection)));
        } catch (Exception e) {
            logger.warn("Invalid maxRetriesPerServerPrimeConnection");
        }
        try {
            maxTotalTimeToPrimeConnections = Long.parseLong(String.valueOf(niwsClientConfig.getProperty(
                    CommonClientConfigKey.MaxTotalTimeToPrimeConnections,maxTotalTimeToPrimeConnections)));
        } catch (Exception e) {
            logger.warn("Invalid maxTotalTimeToPrimeConnections");
        }
        primeConnectionsURI = String.valueOf(niwsClientConfig.getProperty(CommonClientConfigKey.PrimeConnectionsURI, primeConnectionsURI));
        float primeRatio = Float.parseFloat(String.valueOf(niwsClientConfig.getProperty(CommonClientConfigKey.MinPrimeConnectionsRatio)));
        className = (String) niwsClientConfig.getProperty(CommonClientConfigKey.PrimeConnectionsClassName, 
        		DefaultClientConfigImpl.DEFAULT_PRIME_CONNECTIONS_CLASS);
        try {
            connector = (IPrimeConnection) Class.forName(className).newInstance();
            connector.initWithNiwsConfig(niwsClientConfig);
        } catch (Exception e) {
            throw new RuntimeException("Unable to initialize prime connections", e);
        }
        setUp(name, maxRetriesPerServerPrimeConnection, 
                maxTotalTimeToPrimeConnections, primeConnectionsURI, primeRatio);        
    }
        
    public PrimeConnections(String name, int maxRetries, 
            long maxTotalTimeToPrimeConnections, String primeConnectionsURI) {
        setUp(name, maxRetries, maxTotalTimeToPrimeConnections, primeConnectionsURI, DefaultClientConfigImpl.DEFAULT_MIN_PRIME_CONNECTIONS_RATIO);
    }

    public PrimeConnections(String name, int maxRetries, 
            long maxTotalTimeToPrimeConnections, String primeConnectionsURI, float primeRatio) {
        setUp(name, maxRetries, maxTotalTimeToPrimeConnections, primeConnectionsURI, primeRatio);
    }

    private void setUp(String name, int maxRetries, 
            long maxTotalTimeToPrimeConnections, String primeConnectionsURI, float primeRatio) {        
        this.name = name;
        this.maxRetries = maxRetries;
        this.maxTotalTimeToPrimeConnections = maxTotalTimeToPrimeConnections;
        this.primeConnectionsURIPath = primeConnectionsURI;        
        this.primeRatio = primeRatio;
        executorService = new ThreadPoolExecutor(1 /* minimum */,
                maxExecutorThreads /* max threads */,
                executorThreadTimeout /*
                                       * timeout - same property as create
                                       * timeout
                                       */, TimeUnit.MILLISECONDS,
                new LinkedBlockingQueue<Runnable>(maxTasksPerExecutorQueue)
                /* Bounded queue with FIFO- bounded to max tasks */,
                new ASyncPrimeConnectionsThreadFactory(name) /*
                                                              * So we can give
                                                              * our Thread a
                                                              * name
                                                              */
        );        
        totalCounter = Monitors.newCounter(name + "_PrimeConnection_TotalCounter");
        successCounter = Monitors.newCounter(name + "_PrimeConnection_SuccessCounter");
        initialPrimeTimer = Monitors.newTimer(name + "_initialPrimeConnectionsTimer", TimeUnit.MILLISECONDS);
        Monitors.registerObject(name + "_PrimeConnection", this);
    }
    
    /**
     * Prime connections, blocking until configured percentage (default is 100%) of target servers are primed 
     * or max time is reached.
     * 
     * @see CommonClientConfigKey#MinPrimeConnectionsRatio
     * @see CommonClientConfigKey#MaxTotalTimeToPrimeConnections
     * 
     */
    public void primeConnections(List<Server> servers) {
        if (servers == null || servers.size() == 0) {
            logger.debug("No server to prime");
            return;
        }
        for (Server server: servers) {
            server.setReadyToServe(false);
        }
        int totalCount = (int) (servers.size() * primeRatio); 
        final CountDownLatch latch = new CountDownLatch(totalCount);
        final AtomicInteger successCount = new AtomicInteger(0);
        final AtomicInteger failureCount= new AtomicInteger(0);
        primeConnectionsAsync(servers, new PrimeConnectionListener()  {            
            @Override
            public void primeCompleted(Server s, Throwable lastException) {
                if (lastException == null) {
                    successCount.incrementAndGet();
                    s.setReadyToServe(true);
                } else {
                    failureCount.incrementAndGet();
                }
                latch.countDown();
            }
        }); 
                
        Stopwatch stopWatch = initialPrimeTimer.start();
        try {
            latch.await(maxTotalTimeToPrimeConnections, TimeUnit.MILLISECONDS);
        } catch (InterruptedException e) {
            logger.error("Priming connection interrupted", e);
        } finally {
            stopWatch.stop();
        }
        printStats(totalCount, successCount.get(), failureCount.get(), stopWatch.getDuration(TimeUnit.MILLISECONDS));
    }
    
    private void printStats(int total, int success, int failure, long totalTime) {
        if (total != success) {
            logger.info("Priming Connections not fully successful");
        } else {
            logger.info("Priming connections fully successful");
        }
        logger.debug("numServers left to be 'primed'="
                + (total - success));
        logger.debug("numServers successfully 'primed'=" + success);
        logger
                .debug("numServers whose attempts not complete exclusively due to max time allocated="
                        + (total - (success + failure)));
        logger.debug("Total Time Taken=" + totalTime
                + " msecs, out of an allocated max of (msecs)="
                + maxTotalTimeToPrimeConnections);
    }

    /*
    private void makeConnectionsASync() {
        Callable<Void> ft = new Callable<Void>() {
            public Void call() throws Exception {
                logger.info("primeConnections ...");
                makeConnections();
                return null;
            }
        };
        outerExecutorService.submit(ft);
    }
    */
    
    /**
     * Prime servers asynchronously.
     * 
     * @param servers
     * @param listener
     */
    public List<Future<Boolean>> primeConnectionsAsync(final List<Server> servers, final PrimeConnectionListener listener) {
        if (servers == null) {
            return Collections.emptyList();
        }
        List<Server> allServers = new ArrayList<Server>();
        allServers.addAll(servers);
        if (allServers.size() == 0){
            logger.debug("RestClient:" + name + ". No nodes/servers to prime connections");
            return Collections.emptyList();
        }        

        logger.info("Priming Connections for RestClient:" + name
                + ", numServers:" + allServers.size());
        List<Future<Boolean>> ftList = new ArrayList<Future<Boolean>>();
        for (Server s : allServers) {
            // prevent the server to be used by load balancer
            // will be set to true when priming is done
            s.setReadyToServe(false);
            if (aSync) {
                Future<Boolean> ftC = null;
                try {
                    ftC = makeConnectionASync(s, listener);
                    ftList.add(ftC);

                } catch (Throwable e) { // NOPMD
                    // It does not really matter if there was an exception,
                    // the goal here is to attempt "priming/opening" the route
                    // in ec2 .. actual http results do not matter
                }
            } else {
                connectToServer(s, listener);
            }
        }   
        return ftList;
    }
    
    private Future<Boolean> makeConnectionASync(final Server s, 
            final PrimeConnectionListener listener) throws InterruptedException, ExecutionException {
        Callable<Boolean> ftConn = new Callable<Boolean>() {
            public Boolean call() throws Exception {
                logger.debug("calling primeconnections ...");
                return connectToServer(s, listener);
            }
        };
        return executorService.submit(ftConn);
    }

    public void shutdown() {
        executorService.shutdown();
        Monitors.unregisterObject(name + "_PrimeConnection", this);
    }

    private Boolean connectToServer(final Server s, final PrimeConnectionListener listener) {
        int tryNum = 0;
        Exception lastException = null;
        totalCounter.increment();
        boolean success = false;
        do {
            try {
                logger.debug("Executing PrimeConnections request to server " + s + " with path " + primeConnectionsURIPath
                        + ", tryNum=" + tryNum);
                success = connector.connect(s, primeConnectionsURIPath);
                successCounter.increment();
                break;
            } catch (Exception e) {
                // It does not really matter if there was an exception,
                // the goal here is to attempt "priming/opening" the route
                // in ec2 .. actual http results do not matter
                logger.debug("Error connecting to server: {}", e.getMessage());
                lastException = e;
                sleepBeforeRetry(tryNum);
            } 
            logger.debug("server:" + s + ", result=" + success + ", tryNum="
                    + tryNum + ", maxRetries=" + maxRetries);
            tryNum++;
        } while (!success && (tryNum <= maxRetries));
        // set the alive flag so that it can be used by load balancers
        if (listener != null) {
            try {
                listener.primeCompleted(s, lastException);
            } catch (Throwable e) {
                logger.error("Error calling PrimeComplete listener", e);
            }
        }
        logger.debug("Either done, or quitting server:" + s + ", result="
                + success + ", tryNum=" + tryNum + ", maxRetries=" + maxRetries);        
        return success;
    }

    private void sleepBeforeRetry(int tryNum) {
        try {
            int sleep = (tryNum + 1) * 100;
            logger.debug("Sleeping for " + sleep + "ms ...");
            Thread.sleep(sleep); // making this seconds based is too slow
            // i.e. 200ms, 400 ms, 800ms, 1600ms etc.
        } catch (InterruptedException ex) {
        }
    }
    
    static class ASyncPrimeConnectionsThreadFactory implements ThreadFactory {
        private static final AtomicInteger groupNumber = new AtomicInteger(1);
        private final ThreadGroup group;
        private final AtomicInteger threadNumber = new AtomicInteger(1);
        private final String namePrefix;

        ASyncPrimeConnectionsThreadFactory(String name) {
            SecurityManager s = System.getSecurityManager();
            group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); // NOPMD
            namePrefix = "ASyncPrimeConnectionsThreadFactory-" + name + "-"
                    + groupNumber.getAndIncrement() + "-thread-";
        }

        public Thread newThread(Runnable r) {
            Thread t = new Thread(group, r, namePrefix
                    + threadNumber.getAndIncrement(), 0);
            if (!t.isDaemon())
                t.setDaemon(true);
            if (t.getPriority() != Thread.NORM_PRIORITY)
                t.setPriority(Thread.NORM_PRIORITY);
            return t;
        }
    }
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_4be8bde_47e5ff8/rev_4be8bde-47e5ff8/ribbon-core/src/main/java/com/netflix/loadbalancer/BaseLoadBalancer.java;<<<<<<< MINE
||||||| BASE
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.loadbalancer;

import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.List;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.CopyOnWriteArrayList;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReadWriteLock;
import java.util.concurrent.locks.ReentrantReadWriteLock;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.collect.ImmutableList;
import com.netflix.client.ClientFactory;
import com.netflix.client.IClientConfigAware;
import com.netflix.client.PrimeConnections;
import com.netflix.client.config.CommonClientConfigKey;
import com.netflix.client.config.IClientConfig;
import com.netflix.servo.annotations.DataSourceType;
import com.netflix.servo.annotations.Monitor;
import com.netflix.servo.monitor.Counter;
import com.netflix.servo.monitor.Monitors;
import com.netflix.util.concurrent.ShutdownEnabledTimer;

/**
 * A basic implementation of the load balancer where an arbitrary list of
 * servers can be set as the server pool. A ping can be set to determine the
 * liveness of a server. Internally, this class maintains an "all" server list
 * and an "up" server list and use them depending on what the caller asks for.
 * 
 * @author stonse
 * 
 */
public class BaseLoadBalancer extends AbstractLoadBalancer implements
        PrimeConnections.PrimeConnectionListener, IClientConfigAware {

    private static Logger logger = LoggerFactory
            .getLogger(BaseLoadBalancer.class);
    private final static IRule DEFAULT_RULE = new RoundRobinRule();
    private static final String DEFAULT_NAME = "default";
    private static final String PREFIX = "LoadBalancer_";

    protected IRule rule = DEFAULT_RULE;

    protected IPing ping = null;

    @Monitor(name = PREFIX + "AllServerList", type = DataSourceType.INFORMATIONAL)
    protected volatile List<Server> allServerList = Collections
            .synchronizedList(new ArrayList<Server>());
    @Monitor(name = PREFIX + "UpServerList", type = DataSourceType.INFORMATIONAL)
    protected volatile List<Server> upServerList = Collections
            .synchronizedList(new ArrayList<Server>());

    protected ReadWriteLock allServerLock = new ReentrantReadWriteLock();
    protected ReadWriteLock upServerLock = new ReentrantReadWriteLock();

    protected String name = DEFAULT_NAME;

    protected Timer lbTimer = null;
    protected int pingIntervalSeconds = 10;
    protected int maxTotalPingTimeSeconds = 5;
    protected Comparator<Server> serverComparator = new ServerComparator();

    protected AtomicBoolean pingInProgress = new AtomicBoolean(false);

    protected LoadBalancerStats lbStats;

    private volatile Counter counter;

    private PrimeConnections primeConnections;

    private volatile boolean enablePrimingConnections = false;
    
    private IClientConfig config;
    
    private List<ServerListChangeListener> changeListeners = new CopyOnWriteArrayList<ServerListChangeListener>();

    /**
     * Default constructor which sets name as "default", sets null ping, and
     * {@link RoundRobinRule} as the rule.
     * <p>
     * This constructor is mainly used by {@link ClientFactory}. Calling this
     * constructor must be followed by calling {@link #init()} or
     * {@link #initWithNiwsConfig(IClientConfig)} to complete initialization.
     * This constructor is provided for reflection. When constructing
     * programatically, it is recommended to use other constructors.
     */
    public BaseLoadBalancer() {
        this.name = DEFAULT_NAME;
        this.ping = null;
        setRule(DEFAULT_RULE);
        setupPingTask();
        lbStats = new LoadBalancerStats(DEFAULT_NAME);
        counter = createCounter();
    }

    public BaseLoadBalancer(String lbName, IRule rule, LoadBalancerStats lbStats) {
        this(lbName, rule, lbStats, null);
    }

    public BaseLoadBalancer(IPing ping, IRule rule) {
        this(DEFAULT_NAME, rule, new LoadBalancerStats(DEFAULT_NAME), ping);
    }

    public BaseLoadBalancer(String name, IRule rule, LoadBalancerStats stats,
            IPing ping) {
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  initialized");
        }
        this.name = name;
        this.ping = ping;
        setRule(rule);
        setupPingTask();
        lbStats = stats;
        counter = createCounter();
        init();
    }

    public BaseLoadBalancer(IClientConfig config) {
        initWithNiwsConfig(config);
    }

    @Override
    public void initWithNiwsConfig(IClientConfig clientConfig) {
    	this.config = clientConfig;
        String clientName = clientConfig.getClientName();
        String ruleClassName = (String) clientConfig
                .getProperty(CommonClientConfigKey.NFLoadBalancerRuleClassName);
        String pingClassName = (String) clientConfig
                .getProperty(CommonClientConfigKey.NFLoadBalancerPingClassName);

        IRule rule;
        IPing ping;
        try {
            rule = (IRule) ClientFactory.instantiateInstanceWithClientConfig(
                    ruleClassName, clientConfig);
            ping = (IPing) ClientFactory.instantiateInstanceWithClientConfig(
                    pingClassName, clientConfig);
        } catch (Exception e) {
            throw new RuntimeException("Error initializing load balancer", e);
        }

        this.name = clientName;
        int pingIntervalTime = Integer.parseInt(""
                + clientConfig.getProperty(
                        CommonClientConfigKey.NFLoadBalancerPingInterval,
                        Integer.parseInt("30")));
        int maxTotalPingTime = Integer.parseInt(""
                + clientConfig.getProperty(
                        CommonClientConfigKey.NFLoadBalancerMaxTotalPingTime,
                        Integer.parseInt("2")));

        setPingInterval(pingIntervalTime);
        setMaxTotalPingTime(maxTotalPingTime);

        // cross associate with each other
        // i.e. Rule,Ping meet your container LB
        // LB, these are your Ping and Rule guys ...
        setRule(rule);
        setPing(ping);
        setLoadBalancerStats(new LoadBalancerStats(clientName));
        rule.setLoadBalancer(this);
        if (ping instanceof AbstractLoadBalancerPing) {
            ((AbstractLoadBalancerPing) ping).setLoadBalancer(this);
        }
        logger.info("Client:" + name + " instantiated a LoadBalancer:"
                + toString());
        boolean enablePrimeConnections = false;

        if (clientConfig
                .getProperty(CommonClientConfigKey.EnablePrimeConnections) != null) {
            Boolean bEnablePrimeConnections = Boolean.valueOf(""
                    + clientConfig.getProperty(
                            CommonClientConfigKey.EnablePrimeConnections,
                            "false"));
            enablePrimeConnections = bEnablePrimeConnections.booleanValue();
        }

        if (enablePrimeConnections) {
            this.setEnablePrimingConnections(true);
            PrimeConnections primeConnections = new PrimeConnections(
                    this.getName(), clientConfig);
            this.setPrimeConnections(primeConnections);
        }
        init();
    }

    public void addServerListChangeListener(ServerListChangeListener listener) {
        changeListeners.add(listener);
    }
    
    public void removeServerListChangeListener(ServerListChangeListener listener) {
        changeListeners.remove(listener);
    }

    public IClientConfig getClientConfig() {
    	return config;
    }
    
    private boolean canSkipPing() {
        if (ping == null
                || ping.getClass().getName().equals(DummyPing.class.getName())) {
            // default ping, no need to set up timer
            return true;
        } else {
            return false;
        }
    }

    private void setupPingTask() {
        if (canSkipPing()) {
            return;
        }
        if (lbTimer != null) {
            lbTimer.cancel();
        }
        lbTimer = new ShutdownEnabledTimer("NFLoadBalancer-PingTimer-" + name,
                true);
        lbTimer.schedule(new PingTask(), 0, pingIntervalSeconds * 1000);
        forceQuickPing();
    }

    /**
     * Set the name for the load balancer. This should not be called since name
     * should be immutable after initialization. Calling this method does not
     * guarantee that all other data structures that depend on this name will be
     * changed accordingly.
     */
    void setName(String name) {
        // and register
        this.name = name;
        if (lbStats == null) {
            lbStats = new LoadBalancerStats(name);
        } else {
            lbStats.setName(name);
        }
    }

    public String getName() {
        return name;
    }

    @Override
    public LoadBalancerStats getLoadBalancerStats() {
        return lbStats;
    }

    public void setLoadBalancerStats(LoadBalancerStats lbStats) {
        this.lbStats = lbStats;
    }

    public Lock lockAllServerList(boolean write) {
        Lock aproposLock = write ? allServerLock.writeLock() : allServerLock
                .readLock();
        aproposLock.lock();
        return aproposLock;
    }

    public Lock lockUpServerList(boolean write) {
        Lock aproposLock = write ? upServerLock.writeLock() : upServerLock
                .readLock();
        aproposLock.lock();
        return aproposLock;
    }

    public void setPingInterval(int pingIntervalSeconds) {
        if (pingIntervalSeconds < 1) {
            return;
        }

        this.pingIntervalSeconds = pingIntervalSeconds;
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  pingIntervalSeconds set to "
                    + this.pingIntervalSeconds);
        }
        setupPingTask(); // since ping data changed
    }

    public int getPingInterval() {
        return pingIntervalSeconds;
    }

    /*
     * Maximum time allowed for the ping cycle
     */
    public void setMaxTotalPingTime(int maxTotalPingTimeSeconds) {
        if (maxTotalPingTimeSeconds < 1) {
            return;
        }
        this.maxTotalPingTimeSeconds = maxTotalPingTimeSeconds;
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer: maxTotalPingTime set to "
                    + this.maxTotalPingTimeSeconds);
        }

    }

    public int getMaxTotalPingTime() {
        return maxTotalPingTimeSeconds;
    }

    public IPing getPing() {
        return ping;
    }

    public IRule getRule() {
        return rule;
    }

    public boolean isPingInProgress() {
        return pingInProgress.get();
    }

    /* Specify the object which is used to send pings. */

    public void setPing(IPing ping) {
        if (ping != null) {
            if (!ping.equals(this.ping)) {
                this.ping = ping;
                setupPingTask(); // since ping data changed
            }
        } else {
            this.ping = null;
            // cancel the timer task
            lbTimer.cancel();
        }
    }

    /* Ignore null rules */

    public void setRule(IRule rule) {
        if (rule != null) {
            this.rule = rule;
        } else {
            /* default rule */
            this.rule = new RoundRobinRule();
        }
        if (this.rule.getLoadBalancer() != this) {
            this.rule.setLoadBalancer(this);
        }
    }

    /**
     * get the count of servers.
     * 
     * @param onlyAvailable
     *            if true, return only up servers.
     */
    public int getServerCount(boolean onlyAvailable) {
        if (onlyAvailable) {
            return upServerList.size();
        } else {
            return allServerList.size();
        }
    }

    /**
     * Add a server to the 'allServer' list; does not verify uniqueness, so you
     * could give a server a greater share by adding it more than once.
     */
    public void addServer(Server newServer) {
        if (newServer != null) {
            try {
                ArrayList<Server> newList = new ArrayList<Server>();

                newList.addAll(allServerList);
                newList.add(newServer);
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding a newServer", e);
            }
        }
    }

    /**
     * Add a list of servers to the 'allServer' list; does not verify
     * uniqueness, so you could give a server a greater share by adding it more
     * than once
     */
    @Override
    public void addServers(List<Server> newServers) {
        if (newServers != null && newServers.size() > 0) {
            try {
                ArrayList<Server> newList = new ArrayList<Server>();
                newList.addAll(allServerList);
                newList.addAll(newServers);
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding Servers", e);
            }
        }
    }

    /*
     * Add a list of servers to the 'allServer' list; does not verify
     * uniqueness, so you could give a server a greater share by adding it more
     * than once USED by Test Cases only for legacy reason. DO NOT USE!!
     */
    void addServers(Object[] newServers) {
        if ((newServers != null) && (newServers.length > 0)) {

            try {
                ArrayList<Server> newList = new ArrayList<Server>();
                newList.addAll(allServerList);

                for (Object server : newServers) {
                    if (server != null) {
                        if (server instanceof String) {
                            server = new Server((String) server);
                        }
                        if (server instanceof Server) {
                            newList.add((Server) server);
                        }
                    }
                }
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding Servers", e);
            }
        }
    }

    /**
     * Set the list of servers used as the server pool. This overrides existing
     * server list.
     */
    public void setServersList(List lsrv) {
        Lock writeLock = allServerLock.writeLock();
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  clearing server list (SET op)");
        }
        ArrayList<Server> newServers = new ArrayList<Server>();
        writeLock.lock();
        try {
            ArrayList<Server> allServers = new ArrayList<Server>();
            for (Object server : lsrv) {
                if (server == null) {
                    continue;
                }

                if (server instanceof String) {
                    server = new Server((String) server);
                }

                if (server instanceof Server) {
                    if (logger.isDebugEnabled()) {
                        logger.debug("LoadBalancer:  addServer ["
                                + ((Server) server).getId() + "]");
                    }
                    allServers.add((Server) server);
                } else {
                    throw new IllegalArgumentException(
                            "Type String or Server expected, instead found:"
                                    + server.getClass());
                }

            }
            boolean listChanged = false;
            if (!allServerList.equals(allServers)) {
                listChanged = true;
                if (changeListeners != null && changeListeners.size() > 0) {
                   List<Server> oldList = ImmutableList.copyOf(allServerList);
                   List<Server> newList = ImmutableList.copyOf(allServers);                   
                   for (ServerListChangeListener l: changeListeners) {
                       try {
                           l.serverListChanged(oldList, newList);
                       } catch (Throwable e) {
                           logger.error("Error invoking server list change listener", e);
                       }
                   }
                }
            }
            if (isEnablePrimingConnections()) {
                for (Server server : allServers) {
                    if (!allServerList.contains(server)) {
                        server.setReadyToServe(false);
                        newServers.add((Server) server);
                    }
                }
                if (primeConnections != null) {
                    primeConnections.primeConnectionsAsync(newServers, this);
                }
            }
            // This will reset readyToServe flag to true on all servers
            // regardless whether
            // previous priming connections are success or not
            allServerList = allServers;
            if (canSkipPing()) {
                for (Server s : allServerList) {
                    s.setAlive(true);
                }
                upServerList = allServerList;
            } else if (listChanged) {
                forceQuickPing();
            }
        } finally {
            writeLock.unlock();
        }
    }

    /* List in string form. SETS, does not add. */
    void setServers(String srvString) {
        if (srvString != null) {

            try {
                String[] serverArr = srvString.split(",");
                ArrayList<Server> newList = new ArrayList<Server>();

                for (String serverString : serverArr) {
                    if (serverString != null) {
                        serverString = serverString.trim();
                        if (serverString.length() > 0) {
                            Server svr = new Server(serverString);
                            newList.add(svr);
                        }
                    }
                }
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding Servers", e);
            }
        }
    }

    /**
     * return the server
     * 
     * @param index
     * @param availableOnly
     */
    public Server getServerByIndex(int index, boolean availableOnly) {
        try {
            return (availableOnly ? upServerList.get(index) : allServerList
                    .get(index));
        } catch (Exception e) {
            return null;
        }
    }

    @Override
    public List<Server> getServerList(boolean availableOnly) {
        return (availableOnly ? Collections.unmodifiableList(upServerList) : 
        	Collections.unmodifiableList(allServerList));
    }

    @Override
    public List<Server> getServerList(ServerGroup serverGroup) {
        switch (serverGroup) {
        case ALL:
            return allServerList;
        case STATUS_UP:
            return upServerList;
        case STATUS_NOT_UP:
            ArrayList<Server> notAvailableServers = new ArrayList<Server>(
                    allServerList);
            ArrayList<Server> upServers = new ArrayList<Server>(upServerList);
            notAvailableServers.removeAll(upServers);
            return notAvailableServers;
        }
        return new ArrayList<Server>();
    }

    public void cancelPingTask() {
        if (lbTimer != null) {
            lbTimer.cancel();
        }
    }

    /**
     * TimerTask that keeps runs every X seconds to check the status of each
     * server/node in the Server List
     * 
     * @author stonse
     * 
     */
    class PingTask extends TimerTask {
        public void run() {
            Pinger ping = new Pinger();
            try {
                ping.runPinger();
            } catch (Throwable t) {
                logger.error("Throwable caught while running extends for "
                        + name, t);
            }
        }
    }

    /**
     * Class that contains the mechanism to "ping" all the instances
     * 
     * @author stonse
     *
     */
    class Pinger {

        public void runPinger() {

            if (pingInProgress.get()) {
                return; // Ping in progress - nothing to do
            } else {
                pingInProgress.set(true);
            }

            // we are "in" - we get to Ping

            Object[] allServers = null;
            boolean[] results = null;

            Lock allLock = null;
            Lock upLock = null;

            try {
                /*
                 * The readLock should be free unless an addServer operation is
                 * going on...
                 */
                allLock = allServerLock.readLock();
                allLock.lock();
                allServers = allServerList.toArray();
                allLock.unlock();

                int numCandidates = allServers.length;
                results = new boolean[numCandidates];

                if (logger.isDebugEnabled()) {
                    logger.debug("LoadBalancer:  PingTask executing ["
                            + numCandidates + "] servers configured");
                }

                for (int i = 0; i < numCandidates; i++) {
                    results[i] = false; /* Default answer is DEAD. */
                    try {
                        // NOTE: IFF we were doing a real ping
                        // assuming we had a large set of servers (say 15)
                        // the logic below will run them serially
                        // hence taking 15 times the amount of time it takes
                        // to ping each server
                        // A better method would be to put this in an executor
                        // pool
                        // But, at the time of this writing, we dont REALLY
                        // use a Real Ping (its mostly in memory eureka call)
                        // hence we can afford to simplify this design and run
                        // this
                        // serially
                        if (ping != null) {
                            results[i] = ping.isAlive((Server) allServers[i]);
                        }
                    } catch (Throwable t) {
                        logger.error("Exception while pinging Server:"
                                + allServers[i], t);
                    }
                }

                ArrayList<Server> newUpList = new ArrayList<Server>();

                for (int i = 0; i < numCandidates; i++) {
                    boolean isAlive = results[i];
                    Server svr = (Server) allServers[i];
                    boolean oldIsAlive = svr.isAlive();

                    svr.setAlive(isAlive);

                    if (oldIsAlive != isAlive && logger.isDebugEnabled()) {
                        logger.debug("LoadBalancer:  Server [" + svr.getId()
                                + "] status changed to "
                                + (isAlive ? "ALIVE" : "DEAD"));
                    }

                    if (isAlive) {
                        newUpList.add(svr);
                    }
                }
                // System.out.println(count + " servers alive");
                upLock = upServerLock.writeLock();
                upLock.lock();
                upServerList = newUpList;
                upLock.unlock();
            } catch (Throwable t) {
                logger.error("Throwable caught while running the Pinger-"
                        + name, t);
            } finally {
                pingInProgress.set(false);
            }
        }
    }

    private final Counter createCounter() {
        return Monitors.newCounter("LoadBalancer_ChooseServer");
    }

    /*
     * Get the alive server dedicated to key
     * 
     * @return the dedicated server
     */
    public Server chooseServer(Object key) {
        if (counter == null) {
            counter = createCounter();
        }
        counter.increment();
        if (rule == null) {
            return null;
        } else {
            try {
                return rule.choose(key);
            } catch (Throwable t) {
                return null;
            }
        }
    }

    /* Returns either null, or "server:port/servlet" */
    public String choose(Object key) {
        if (rule == null) {
            return null;
        } else {
            try {
                Server svr = rule.choose(key);
                return ((svr == null) ? null : svr.getId());
            } catch (Throwable t) {
                return null;
            }
        }
    }

    public void markServerDown(Server server) {
        if (server == null) {
            return;
        }

        if (!server.isAlive()) {
            return;
        }

        logger.error("LoadBalancer:  markServerDown called on ["
                + server.getId() + "]");
        server.setAlive(false);
        // forceQuickPing();
    }

    public void markServerDown(String id) {
        boolean triggered = false;

        id = Server.normalizeId(id);

        if (id == null) {
            return;
        }

        Lock writeLock = upServerLock.writeLock();

        try {

            for (Server svr : upServerList) {
                if (svr.isAlive() && (svr.getId().equals(id))) {
                    triggered = true;
                    svr.setAlive(false);
                }
            }

            if (triggered) {
                logger.error("LoadBalancer:  markServerDown called on [" + id
                        + "]");
            }

        } finally {
            try {
                writeLock.unlock();
            } catch (Exception e) { // NOPMD
            }
        }
    }

    /*
     * Force an immediate ping, if we're not currently pinging and don't have a
     * quick-ping already scheduled.
     */
    public void forceQuickPing() {
        if (canSkipPing()) {
            return;
        }
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  forceQuickPing invoked");
        }
        Pinger ping = new Pinger();
        try {
            ping.runPinger();
        } catch (Throwable t) {
            logger.error("Throwable caught while running forceQuickPing() for "
                    + name, t);
        }
    }

    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("{NFLoadBalancer:name=").append(this.getName())
                .append(",current list of Servers=").append(this.allServerList)
                .append(",Load balancer stats=")
                .append(this.lbStats.toString()).append("}");
        return sb.toString();
    }

    /**
     * Register with monitors and start priming connections if it is set.
     */
    protected void init() {
        Monitors.registerObject("LoadBalancer_" + name, this);
        // register the rule as it contains metric for available servers count
        Monitors.registerObject("Rule_" + name, this.getRule());
        if (enablePrimingConnections && primeConnections != null) {
            primeConnections.primeConnections(getServerList(true));
        }
    }

    public final PrimeConnections getPrimeConnections() {
        return primeConnections;
    }

    public final void setPrimeConnections(PrimeConnections primeConnections) {
        this.primeConnections = primeConnections;
    }

    @Override
    public void primeCompleted(Server s, Throwable lastException) {
        s.setReadyToServe(true);
    }

    public boolean isEnablePrimingConnections() {
        return enablePrimingConnections;
    }

    public final void setEnablePrimingConnections(
            boolean enablePrimingConnections) {
        this.enablePrimingConnections = enablePrimingConnections;
    }
}=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.loadbalancer;

import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.List;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.CopyOnWriteArrayList;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReadWriteLock;
import java.util.concurrent.locks.ReentrantReadWriteLock;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.collect.ImmutableList;
import com.netflix.client.ClientFactory;
import com.netflix.client.IClientConfigAware;
import com.netflix.client.PrimeConnections;
import com.netflix.client.config.CommonClientConfigKey;
import com.netflix.client.config.IClientConfig;
import com.netflix.servo.annotations.DataSourceType;
import com.netflix.servo.annotations.Monitor;
import com.netflix.servo.monitor.Counter;
import com.netflix.servo.monitor.Monitors;
import com.netflix.util.concurrent.ShutdownEnabledTimer;

/**
 * A basic implementation of the load balancer where an arbitrary list of
 * servers can be set as the server pool. A ping can be set to determine the
 * liveness of a server. Internally, this class maintains an "all" server list
 * and an "up" server list and use them depending on what the caller asks for.
 * 
 * @author stonse
 * 
 */
public class BaseLoadBalancer extends AbstractLoadBalancer implements
        PrimeConnections.PrimeConnectionListener, IClientConfigAware {

    private static Logger logger = LoggerFactory
            .getLogger(BaseLoadBalancer.class);
    private final static IRule DEFAULT_RULE = new RoundRobinRule();
    private static final String DEFAULT_NAME = "default";
    private static final String PREFIX = "LoadBalancer_";

    protected IRule rule = DEFAULT_RULE;

    protected IPing ping = null;

    @Monitor(name = PREFIX + "AllServerList", type = DataSourceType.INFORMATIONAL)
    protected volatile List<Server> allServerList = Collections
            .synchronizedList(new ArrayList<Server>());
    @Monitor(name = PREFIX + "UpServerList", type = DataSourceType.INFORMATIONAL)
    protected volatile List<Server> upServerList = Collections
            .synchronizedList(new ArrayList<Server>());

    protected ReadWriteLock allServerLock = new ReentrantReadWriteLock();
    protected ReadWriteLock upServerLock = new ReentrantReadWriteLock();

    protected String name = DEFAULT_NAME;

    protected Timer lbTimer = null;
    protected int pingIntervalSeconds = 10;
    protected int maxTotalPingTimeSeconds = 5;
    protected Comparator<Server> serverComparator = new ServerComparator();

    protected AtomicBoolean pingInProgress = new AtomicBoolean(false);

    protected LoadBalancerStats lbStats;

    private volatile Counter counter;

    private PrimeConnections primeConnections;

    private volatile boolean enablePrimingConnections = false;
    
    private IClientConfig config;
    
    private List<ServerListChangeListener> changeListeners = new CopyOnWriteArrayList<ServerListChangeListener>();

    /**
     * Default constructor which sets name as "default", sets null ping, and
     * {@link RoundRobinRule} as the rule.
     * <p>
     * This constructor is mainly used by {@link ClientFactory}. Calling this
     * constructor must be followed by calling {@link #init()} or
     * {@link #initWithNiwsConfig(IClientConfig)} to complete initialization.
     * This constructor is provided for reflection. When constructing
     * programatically, it is recommended to use other constructors.
     */
    public BaseLoadBalancer() {
        this.name = DEFAULT_NAME;
        this.ping = null;
        setRule(DEFAULT_RULE);
        setupPingTask();
        lbStats = new LoadBalancerStats(DEFAULT_NAME);
        counter = createCounter();
    }

    public BaseLoadBalancer(String lbName, IRule rule, LoadBalancerStats lbStats) {
        this(lbName, rule, lbStats, null);
    }

    public BaseLoadBalancer(IPing ping, IRule rule) {
        this(DEFAULT_NAME, rule, new LoadBalancerStats(DEFAULT_NAME), ping);
    }

    public BaseLoadBalancer(String name, IRule rule, LoadBalancerStats stats,
            IPing ping) {
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  initialized");
        }
        this.name = name;
        this.ping = ping;
        setRule(rule);
        setupPingTask();
        lbStats = stats;
        counter = createCounter();
        init();
    }

    public BaseLoadBalancer(IClientConfig config) {
        initWithNiwsConfig(config);
    }

    @Override
    public void initWithNiwsConfig(IClientConfig clientConfig) {
    	this.config = clientConfig;
        String clientName = clientConfig.getClientName();
        String ruleClassName = (String) clientConfig
                .getProperty(CommonClientConfigKey.NFLoadBalancerRuleClassName);
        String pingClassName = (String) clientConfig
                .getProperty(CommonClientConfigKey.NFLoadBalancerPingClassName);

        IRule rule;
        IPing ping;
        try {
            rule = (IRule) ClientFactory.instantiateInstanceWithClientConfig(
                    ruleClassName, clientConfig);
            ping = (IPing) ClientFactory.instantiateInstanceWithClientConfig(
                    pingClassName, clientConfig);
        } catch (Exception e) {
            throw new RuntimeException("Error initializing load balancer", e);
        }

        this.name = clientName;
        int pingIntervalTime = Integer.parseInt(""
                + clientConfig.getProperty(
                        CommonClientConfigKey.NFLoadBalancerPingInterval,
                        Integer.parseInt("30")));
        int maxTotalPingTime = Integer.parseInt(""
                + clientConfig.getProperty(
                        CommonClientConfigKey.NFLoadBalancerMaxTotalPingTime,
                        Integer.parseInt("2")));

        setPingInterval(pingIntervalTime);
        setMaxTotalPingTime(maxTotalPingTime);

        // cross associate with each other
        // i.e. Rule,Ping meet your container LB
        // LB, these are your Ping and Rule guys ...
        setRule(rule);
        setPing(ping);
        setLoadBalancerStats(new LoadBalancerStats(clientName));
        rule.setLoadBalancer(this);
        if (ping instanceof AbstractLoadBalancerPing) {
            ((AbstractLoadBalancerPing) ping).setLoadBalancer(this);
        }
        logger.info("Client:" + name + " instantiated a LoadBalancer:"
                + toString());
        boolean enablePrimeConnections = false;

        if (clientConfig
                .getProperty(CommonClientConfigKey.EnablePrimeConnections) != null) {
            Boolean bEnablePrimeConnections = Boolean.valueOf(""
                    + clientConfig.getProperty(
                            CommonClientConfigKey.EnablePrimeConnections,
                            "false"));
            enablePrimeConnections = bEnablePrimeConnections.booleanValue();
        }

        if (enablePrimeConnections) {
            this.setEnablePrimingConnections(true);
            PrimeConnections primeConnections = new PrimeConnections(
                    this.getName(), clientConfig);
            this.setPrimeConnections(primeConnections);
        }
        init();
    }

    public void addServerListChangeListener(ServerListChangeListener listener) {
        changeListeners.add(listener);
    }
    
    public void removeServerListChangeListener(ServerListChangeListener listener) {
        changeListeners.remove(listener);
    }

    public IClientConfig getClientConfig() {
    	return config;
    }
    
    private boolean canSkipPing() {
        if (ping == null
                || ping.getClass().getName().equals(DummyPing.class.getName())) {
            // default ping, no need to set up timer
            return true;
        } else {
            return false;
        }
    }

    private void setupPingTask() {
        if (canSkipPing()) {
            return;
        }
        if (lbTimer != null) {
            lbTimer.cancel();
        }
        lbTimer = new ShutdownEnabledTimer("NFLoadBalancer-PingTimer-" + name,
                true);
        lbTimer.schedule(new PingTask(), 0, pingIntervalSeconds * 1000);
        forceQuickPing();
    }

    /**
     * Set the name for the load balancer. This should not be called since name
     * should be immutable after initialization. Calling this method does not
     * guarantee that all other data structures that depend on this name will be
     * changed accordingly.
     */
    void setName(String name) {
        // and register
        this.name = name;
        if (lbStats == null) {
            lbStats = new LoadBalancerStats(name);
        } else {
            lbStats.setName(name);
        }
    }

    public String getName() {
        return name;
    }

    @Override
    public LoadBalancerStats getLoadBalancerStats() {
        return lbStats;
    }

    public void setLoadBalancerStats(LoadBalancerStats lbStats) {
        this.lbStats = lbStats;
    }

    public Lock lockAllServerList(boolean write) {
        Lock aproposLock = write ? allServerLock.writeLock() : allServerLock
                .readLock();
        aproposLock.lock();
        return aproposLock;
    }

    public Lock lockUpServerList(boolean write) {
        Lock aproposLock = write ? upServerLock.writeLock() : upServerLock
                .readLock();
        aproposLock.lock();
        return aproposLock;
    }

    public void setPingInterval(int pingIntervalSeconds) {
        if (pingIntervalSeconds < 1) {
            return;
        }

        this.pingIntervalSeconds = pingIntervalSeconds;
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  pingIntervalSeconds set to "
                    + this.pingIntervalSeconds);
        }
        setupPingTask(); // since ping data changed
    }

    public int getPingInterval() {
        return pingIntervalSeconds;
    }

    /*
     * Maximum time allowed for the ping cycle
     */
    public void setMaxTotalPingTime(int maxTotalPingTimeSeconds) {
        if (maxTotalPingTimeSeconds < 1) {
            return;
        }
        this.maxTotalPingTimeSeconds = maxTotalPingTimeSeconds;
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer: maxTotalPingTime set to "
                    + this.maxTotalPingTimeSeconds);
        }

    }

    public int getMaxTotalPingTime() {
        return maxTotalPingTimeSeconds;
    }

    public IPing getPing() {
        return ping;
    }

    public IRule getRule() {
        return rule;
    }

    public boolean isPingInProgress() {
        return pingInProgress.get();
    }

    /* Specify the object which is used to send pings. */

    public void setPing(IPing ping) {
        if (ping != null) {
            if (!ping.equals(this.ping)) {
                this.ping = ping;
                setupPingTask(); // since ping data changed
            }
        } else {
            this.ping = null;
            // cancel the timer task
            lbTimer.cancel();
        }
    }

    /* Ignore null rules */

    public void setRule(IRule rule) {
        if (rule != null) {
            this.rule = rule;
        } else {
            /* default rule */
            this.rule = new RoundRobinRule();
        }
        if (this.rule.getLoadBalancer() != this) {
            this.rule.setLoadBalancer(this);
        }
    }

    /**
     * get the count of servers.
     * 
     * @param onlyAvailable
     *            if true, return only up servers.
     */
    public int getServerCount(boolean onlyAvailable) {
        if (onlyAvailable) {
            return upServerList.size();
        } else {
            return allServerList.size();
        }
    }

    /**
     * Add a server to the 'allServer' list; does not verify uniqueness, so you
     * could give a server a greater share by adding it more than once.
     */
    public void addServer(Server newServer) {
        if (newServer != null) {
            try {
                ArrayList<Server> newList = new ArrayList<Server>();

                newList.addAll(allServerList);
                newList.add(newServer);
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding a newServer", e);
            }
        }
    }

    /**
     * Add a list of servers to the 'allServer' list; does not verify
     * uniqueness, so you could give a server a greater share by adding it more
     * than once
     */
    @Override
    public void addServers(List<Server> newServers) {
        if (newServers != null && newServers.size() > 0) {
            try {
                ArrayList<Server> newList = new ArrayList<Server>();
                newList.addAll(allServerList);
                newList.addAll(newServers);
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding Servers", e);
            }
        }
    }

    /*
     * Add a list of servers to the 'allServer' list; does not verify
     * uniqueness, so you could give a server a greater share by adding it more
     * than once USED by Test Cases only for legacy reason. DO NOT USE!!
     */
    void addServers(Object[] newServers) {
        if ((newServers != null) && (newServers.length > 0)) {

            try {
                ArrayList<Server> newList = new ArrayList<Server>();
                newList.addAll(allServerList);

                for (Object server : newServers) {
                    if (server != null) {
                        if (server instanceof String) {
                            server = new Server((String) server);
                        }
                        if (server instanceof Server) {
                            newList.add((Server) server);
                        }
                    }
                }
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding Servers", e);
            }
        }
    }

    /**
     * Set the list of servers used as the server pool. This overrides existing
     * server list.
     */
    public void setServersList(List lsrv) {
        Lock writeLock = allServerLock.writeLock();
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  clearing server list (SET op)");
        }
        ArrayList<Server> newServers = new ArrayList<Server>();
        writeLock.lock();
        try {
            ArrayList<Server> allServers = new ArrayList<Server>();
            for (Object server : lsrv) {
                if (server == null) {
                    continue;
                }

                if (server instanceof String) {
                    server = new Server((String) server);
                }

                if (server instanceof Server) {
                    if (logger.isDebugEnabled()) {
                        logger.debug("LoadBalancer:  addServer ["
                                + ((Server) server).getId() + "]");
                    }
                    allServers.add((Server) server);
                } else {
                    throw new IllegalArgumentException(
                            "Type String or Server expected, instead found:"
                                    + server.getClass());
                }

            }
            boolean listChanged = false;
            if (!allServerList.equals(allServers)) {
                listChanged = true;
                if (changeListeners != null && changeListeners.size() > 0) {
                   List<Server> oldList = ImmutableList.copyOf(allServerList);
                   List<Server> newList = ImmutableList.copyOf(allServers);                   
                   for (ServerListChangeListener l: changeListeners) {
                       try {
                           l.serverListChanged(oldList, newList);
                       } catch (Throwable e) {
                           logger.error("Error invoking server list change listener", e);
                       }
                   }
                }
            }
            if (isEnablePrimingConnections()) {
                for (Server server : allServers) {
                    if (!allServerList.contains(server)) {
                        server.setReadyToServe(false);
                        newServers.add((Server) server);
                    }
                }
                if (primeConnections != null) {
                    primeConnections.primeConnectionsAsync(newServers, this);
                }
            }
            // This will reset readyToServe flag to true on all servers
            // regardless whether
            // previous priming connections are success or not
            allServerList = allServers;
            if (canSkipPing()) {
                for (Server s : allServerList) {
                    s.setAlive(true);
                }
                upServerList = allServerList;
            } else if (listChanged) {
                forceQuickPing();
            }
        } finally {
            writeLock.unlock();
        }
    }

    /* List in string form. SETS, does not add. */
    void setServers(String srvString) {
        if (srvString != null) {

            try {
                String[] serverArr = srvString.split(",");
                ArrayList<Server> newList = new ArrayList<Server>();

                for (String serverString : serverArr) {
                    if (serverString != null) {
                        serverString = serverString.trim();
                        if (serverString.length() > 0) {
                            Server svr = new Server(serverString);
                            newList.add(svr);
                        }
                    }
                }
                setServersList(newList);
            } catch (Exception e) {
                logger.error("Exception while adding Servers", e);
            }
        }
    }

    /**
     * return the server
     * 
     * @param index
     * @param availableOnly
     */
    public Server getServerByIndex(int index, boolean availableOnly) {
        try {
            return (availableOnly ? upServerList.get(index) : allServerList
                    .get(index));
        } catch (Exception e) {
            return null;
        }
    }

    @Override
    public List<Server> getServerList(boolean availableOnly) {
        return (availableOnly ? Collections.unmodifiableList(upServerList) : 
        	Collections.unmodifiableList(allServerList));
    }

    @Override
    public List<Server> getServerList(ServerGroup serverGroup) {
        switch (serverGroup) {
        case ALL:
            return allServerList;
        case STATUS_UP:
            return upServerList;
        case STATUS_NOT_UP:
            ArrayList<Server> notAvailableServers = new ArrayList<Server>(
                    allServerList);
            ArrayList<Server> upServers = new ArrayList<Server>(upServerList);
            notAvailableServers.removeAll(upServers);
            return notAvailableServers;
        }
        return new ArrayList<Server>();
    }

    public void cancelPingTask() {
        if (lbTimer != null) {
            lbTimer.cancel();
        }
    }

    /**
     * TimerTask that keeps runs every X seconds to check the status of each
     * server/node in the Server List
     * 
     * @author stonse
     * 
     */
    class PingTask extends TimerTask {
        public void run() {
            Pinger ping = new Pinger();
            try {
                ping.runPinger();
            } catch (Throwable t) {
                logger.error("Throwable caught while running extends for "
                        + name, t);
            }
        }
    }

    /**
     * Class that contains the mechanism to "ping" all the instances
     * 
     * @author stonse
     *
     */
    class Pinger {

        public void runPinger() {

            if (pingInProgress.get()) {
                return; // Ping in progress - nothing to do
            } else {
                pingInProgress.set(true);
            }

            // we are "in" - we get to Ping

            Object[] allServers = null;
            boolean[] results = null;

            Lock allLock = null;
            Lock upLock = null;

            try {
                /*
                 * The readLock should be free unless an addServer operation is
                 * going on...
                 */
                allLock = allServerLock.readLock();
                allLock.lock();
                allServers = allServerList.toArray();
                allLock.unlock();

                int numCandidates = allServers.length;
                results = new boolean[numCandidates];

                if (logger.isDebugEnabled()) {
                    logger.debug("LoadBalancer:  PingTask executing ["
                            + numCandidates + "] servers configured");
                }

                for (int i = 0; i < numCandidates; i++) {
                    results[i] = false; /* Default answer is DEAD. */
                    try {
                        // NOTE: IFF we were doing a real ping
                        // assuming we had a large set of servers (say 15)
                        // the logic below will run them serially
                        // hence taking 15 times the amount of time it takes
                        // to ping each server
                        // A better method would be to put this in an executor
                        // pool
                        // But, at the time of this writing, we dont REALLY
                        // use a Real Ping (its mostly in memory eureka call)
                        // hence we can afford to simplify this design and run
                        // this
                        // serially
                        if (ping != null) {
                            results[i] = ping.isAlive((Server) allServers[i]);
                        }
                    } catch (Throwable t) {
                        logger.error("Exception while pinging Server:"
                                + allServers[i], t);
                    }
                }

                ArrayList<Server> newUpList = new ArrayList<Server>();

                for (int i = 0; i < numCandidates; i++) {
                    boolean isAlive = results[i];
                    Server svr = (Server) allServers[i];
                    boolean oldIsAlive = svr.isAlive();

                    svr.setAlive(isAlive);

                    if (oldIsAlive != isAlive && logger.isDebugEnabled()) {
                        logger.debug("LoadBalancer:  Server [" + svr.getId()
                                + "] status changed to "
                                + (isAlive ? "ALIVE" : "DEAD"));
                    }

                    if (isAlive) {
                        newUpList.add(svr);
                    }
                }
                // System.out.println(count + " servers alive");
                upLock = upServerLock.writeLock();
                upLock.lock();
                upServerList = newUpList;
                upLock.unlock();
            } catch (Throwable t) {
                logger.error("Throwable caught while running the Pinger-"
                        + name, t);
            } finally {
                pingInProgress.set(false);
            }
        }
    }

    private final Counter createCounter() {
        return Monitors.newCounter("LoadBalancer_ChooseServer");
    }

    /*
     * Get the alive server dedicated to key
     * 
     * @return the dedicated server
     */
    public Server chooseServer(Object key) {
        if (counter == null) {
            counter = createCounter();
        }
        counter.increment();
        if (rule == null) {
            return null;
        } else {
            try {
                return rule.choose(key);
            } catch (Throwable t) {
                return null;
            }
        }
    }

    /* Returns either null, or "server:port/servlet" */
    public String choose(Object key) {
        if (rule == null) {
            return null;
        } else {
            try {
                Server svr = rule.choose(key);
                return ((svr == null) ? null : svr.getId());
            } catch (Throwable t) {
                return null;
            }
        }
    }

    public void markServerDown(Server server) {
        if (server == null) {
            return;
        }

        if (!server.isAlive()) {
            return;
        }

        logger.error("LoadBalancer:  markServerDown called on ["
                + server.getId() + "]");
        server.setAlive(false);
        // forceQuickPing();
    }

    public void markServerDown(String id) {
        boolean triggered = false;

        id = Server.normalizeId(id);

        if (id == null) {
            return;
        }

        Lock writeLock = upServerLock.writeLock();

        try {

            for (Server svr : upServerList) {
                if (svr.isAlive() && (svr.getId().equals(id))) {
                    triggered = true;
                    svr.setAlive(false);
                }
            }

            if (triggered) {
                logger.error("LoadBalancer:  markServerDown called on [" + id
                        + "]");
            }

        } finally {
            try {
                writeLock.unlock();
            } catch (Exception e) { // NOPMD
            }
        }
    }

    /*
     * Force an immediate ping, if we're not currently pinging and don't have a
     * quick-ping already scheduled.
     */
    public void forceQuickPing() {
        if (canSkipPing()) {
            return;
        }
        if (logger.isDebugEnabled()) {
            logger.debug("LoadBalancer:  forceQuickPing invoked");
        }
        Pinger ping = new Pinger();
        try {
            ping.runPinger();
        } catch (Throwable t) {
            logger.error("Throwable caught while running forceQuickPing() for "
                    + name, t);
        }
    }

    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("{NFLoadBalancer:name=").append(this.getName())
                .append(",current list of Servers=").append(this.allServerList)
                .append(",Load balancer stats=")
                .append(this.lbStats.toString()).append("}");
        return sb.toString();
    }

    /**
     * Register with monitors and start priming connections if it is set.
     */
    protected void init() {
        Monitors.registerObject("LoadBalancer_" + name, this);
        // register the rule as it contains metric for available servers count
        Monitors.registerObject("Rule_" + name, this.getRule());
        if (enablePrimingConnections && primeConnections != null) {
            primeConnections.primeConnections(getServerList(true));
        }
    }

    public final PrimeConnections getPrimeConnections() {
        return primeConnections;
    }

    public final void setPrimeConnections(PrimeConnections primeConnections) {
        this.primeConnections = primeConnections;
    }

    @Override
    public void primeCompleted(Server s, Throwable lastException) {
        s.setReadyToServe(true);
    }

    public boolean isEnablePrimingConnections() {
        return enablePrimingConnections;
    }

    public final void setEnablePrimingConnections(
            boolean enablePrimingConnections) {
        this.enablePrimingConnections = enablePrimingConnections;
    }
    
    public void shutdown() {
        cancelPingTask();
        if (primeConnections != null) {
            primeConnections.shutdown();
        }
        Monitors.unregisterObject("LoadBalancer_" + name, this);
        Monitors.unregisterObject("Rule_" + name, this.getRule());
    }
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_4be8bde_47e5ff8/rev_4be8bde-47e5ff8/ribbon-core/src/main/java/com/netflix/loadbalancer/DynamicServerListLoadBalancer.java;<<<<<<< MINE
||||||| BASE
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.loadbalancer;

import java.util.Date;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ScheduledFuture;
import java.util.concurrent.ScheduledThreadPoolExecutor;
import java.util.concurrent.ThreadFactory;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicLong;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.netflix.client.ClientFactory;
import com.netflix.client.config.CommonClientConfigKey;
import com.netflix.client.config.DefaultClientConfigImpl;
import com.netflix.client.config.IClientConfig;
import com.netflix.config.DynamicIntProperty;
import com.netflix.config.DynamicProperty;
import com.netflix.servo.annotations.DataSourceType;
import com.netflix.servo.annotations.Monitor;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.util.concurrent.ThreadFactoryBuilder;

/**
 * A LoadBalancer that has the capabilities to obtain the candidate list of
 * servers using a dynamic source. i.e. The list of servers can potentially be
 * changed at Runtime. It also contains facilities wherein the list of servers
 * can be passed through a Filter criteria to filter out servers that do not
 * meet the desired criteria.
 * 
 * @author stonse
 * 
 */
public class DynamicServerListLoadBalancer<T extends Server> extends
        BaseLoadBalancer {
    private static final Logger LOGGER = LoggerFactory
            .getLogger(DynamicServerListLoadBalancer.class);

    boolean isSecure = false;
    boolean useTunnel = false;
    private static Thread _shutdownThread;

    // to keep track of modification of server lists
    protected AtomicBoolean serverListUpdateInProgress = new AtomicBoolean(
            false);

    private static long LISTOFSERVERS_CACHE_UPDATE_DELAY = 1000; // msecs;
    private static long LISTOFSERVERS_CACHE_REPEAT_INTERVAL = 30 * 1000; // msecs;
                                                                         // //
                                                                         // every
                                                                         // 30
                                                                         // secs

    private static ScheduledThreadPoolExecutor _serverListRefreshExecutor = null;

    private long refeshIntervalMills = LISTOFSERVERS_CACHE_REPEAT_INTERVAL;

    volatile ServerList<T> serverListImpl;

    volatile ServerListFilter<T> filter;
    
    private AtomicLong lastUpdated = new AtomicLong(System.currentTimeMillis());
    
    protected volatile boolean serverRefreshEnabled = false;
    private final static String CORE_THREAD = "DynamicServerListLoadBalancer.ThreadPoolSize";
    private final static DynamicIntProperty poolSizeProp = new DynamicIntProperty(CORE_THREAD, 2);
    
    private volatile ScheduledFuture<?> scheduledFuture;

    static {
        int coreSize = poolSizeProp.get();
        ThreadFactory factory = (new ThreadFactoryBuilder()).setDaemon(true).build();
        _serverListRefreshExecutor = new ScheduledThreadPoolExecutor(coreSize, factory);
        poolSizeProp.addCallback(new Runnable() {
            @Override
            public void run() {
                _serverListRefreshExecutor.setCorePoolSize(poolSizeProp.get());
            }
        
        });
        _shutdownThread = new Thread(new Runnable() {
            public void run() {
                LOGGER.info("Shutting down the Executor Pool for DynamicServerListLoadBalancer");
                shutdownExecutorPool();
            }
        });
        Runtime.getRuntime().addShutdownHook(_shutdownThread);
    }
    
    public DynamicServerListLoadBalancer() {
        super();
    }

    public DynamicServerListLoadBalancer(IClientConfig niwsClientConfig) {
        initWithNiwsConfig(niwsClientConfig);
    }

    @Override
    public void initWithNiwsConfig(IClientConfig clientConfig) {
        try {
            super.initWithNiwsConfig(clientConfig);
            String niwsServerListClassName = clientConfig.getProperty(
                    CommonClientConfigKey.NIWSServerListClassName,
                    DefaultClientConfigImpl.DEFAULT_SEVER_LIST_CLASS)
                    .toString();

            ServerList<T> niwsServerListImpl = (ServerList<T>) ClientFactory
                    .instantiateInstanceWithClientConfig(
                            niwsServerListClassName, clientConfig);
            this.serverListImpl = niwsServerListImpl;

            if (niwsServerListImpl instanceof AbstractServerList) {
                AbstractServerListFilter<T> niwsFilter = ((AbstractServerList) niwsServerListImpl)
                        .getFilterImpl(clientConfig);
                niwsFilter.setLoadBalancerStats(getLoadBalancerStats());
                this.filter = niwsFilter;
            }

            refeshIntervalMills = Integer.valueOf(clientConfig.getProperty(
                    CommonClientConfigKey.ServerListRefreshInterval,
                    LISTOFSERVERS_CACHE_REPEAT_INTERVAL).toString());

            boolean primeConnection = this.isEnablePrimingConnections();
            // turn this off to avoid duplicated asynchronous priming done in BaseLoadBalancer.setServerList()
            this.setEnablePrimingConnections(false);
            enableAndInitLearnNewServersFeature();

            updateListOfServers();
            if (primeConnection && this.getPrimeConnections() != null) {
                this.getPrimeConnections()
                        .primeConnections(getServerList(true));
            }
            this.setEnablePrimingConnections(primeConnection);

        } catch (Exception e) {
            throw new RuntimeException(
                    "Exception while initializing NIWSDiscoveryLoadBalancer:"
                            + clientConfig.getClientName()
                            + ", niwsClientConfig:" + clientConfig, e);
        }
    }

    @Override
    public void setServersList(List lsrv) {
        super.setServersList(lsrv);
        List<T> serverList = (List<T>) lsrv;
        Map<String, List<Server>> serversInZones = new HashMap<String, List<Server>>();
        for (Server server : serverList) {
            // make sure ServerStats is created to avoid creating them on hot
            // path
            getLoadBalancerStats().getSingleServerStat(server);
            String zone = server.getZone();
            if (zone != null) {
                zone = zone.toLowerCase();
                List<Server> servers = serversInZones.get(zone);
                if (servers == null) {
                    servers = new ArrayList<Server>();
                    serversInZones.put(zone, servers);
                }
                servers.add(server);
            }
        }
        setServerListForZones(serversInZones);
    }

    protected void setServerListForZones(
            Map<String, List<Server>> zoneServersMap) {
        LOGGER.debug("Setting server list for zones: {}", zoneServersMap);
        getLoadBalancerStats().updateZoneServerMapping(zoneServersMap);
    }

    public ServerList<T> getServerListImpl() {
        return serverListImpl;
    }

    public void setServerListImpl(ServerList<T> niwsServerList) {
        this.serverListImpl = niwsServerList;
    }

    @Override
    public void setPing(IPing ping) {
        this.ping = ping;
    }

    public ServerListFilter<T> getFilter() {
        return filter;
    }

    public void setFilter(ServerListFilter<T> filter) {
        this.filter = filter;
    }

    @Override
    /**
     * Makes no sense to ping an inmemory disc client
     * 
     */
    public void forceQuickPing() {
        // no-op
    }

    /**
     * Feature that lets us add new instances (from AMIs) to the list of
     * existing servers that the LB will use Call this method if you want this
     * feature enabled
     */
    public void enableAndInitLearnNewServersFeature() {
        keepServerListUpdated();
        serverRefreshEnabled = true;
    }

    private String getIdentifier() {
        return this.getClientConfig().getClientName();
    }

    private void keepServerListUpdated() {
        scheduledFuture = _serverListRefreshExecutor.scheduleAtFixedRate(
                new ServerListRefreshExecutorThread(),
                LISTOFSERVERS_CACHE_UPDATE_DELAY, refeshIntervalMills,
                TimeUnit.MILLISECONDS);
    }

    private static void shutdownExecutorPool() {
        if (_serverListRefreshExecutor != null) {
            _serverListRefreshExecutor.shutdown();

            if (_shutdownThread != null) {
                try {
                    Runtime.getRuntime().removeShutdownHook(_shutdownThread);
                } catch (IllegalStateException ise) { // NOPMD
                    // this can happen if we're in the middle of a real
                    // shutdown,
                    // and that's 'ok'
                }
            }

        }
    }

    public void stopServerListRefreshing() {
        serverRefreshEnabled = false;
        if (scheduledFuture != null) {
            scheduledFuture.cancel(true);
        }
    }
    
    /**
     * Class that updates the list of Servers This is based on the method used
     * by the client * Appropriate Filters are applied before coming up with the
     * right set of servers
     * 
     * @author stonse
     * 
     */
    class ServerListRefreshExecutorThread implements Runnable {

        public void run() {
            if (!serverRefreshEnabled) {
                return;
            }
            try {
                updateListOfServers();

            } catch (Throwable e) {
                LOGGER.error(
                        "Exception while updating List of Servers obtained from Discovery client",
                        e);
                // e.printStackTrace();
            }
        }

    }

    @VisibleForTesting
    public void updateListOfServers() {
        List<T> servers = new ArrayList<T>();
        if (serverListImpl != null) {
            servers = serverListImpl.getUpdatedListOfServers();
            LOGGER.debug("List of Servers for {} obtained from Discovery client: {}",
                    getIdentifier(), servers);

            if (filter != null) {
                servers = filter.getFilteredListOfServers(servers);
                LOGGER.debug("Filtered List of Servers for {} obtained from Discovery client: {}",
                        getIdentifier(), servers);
            }
        }
        lastUpdated.set(System.currentTimeMillis());
        updateAllServerList(servers);
    }

    /**
     * Update the AllServer list in the LoadBalancer if necessary and enabled
     * 
     * @param ls
     */
    protected void updateAllServerList(List<T> ls) {
        // other threads might be doing this - in which case, we pass
        if (!serverListUpdateInProgress.get()) {
            serverListUpdateInProgress.set(true);
            for (T s : ls) {
                s.setAlive(true); // set so that clients can start using these
                                  // servers right away instead
                // of having to wait out the ping cycle.
            }
            setServersList(ls);
            super.forceQuickPing();
            serverListUpdateInProgress.set(false);
        }
    }

    @Monitor(name="NumUpdateCyclesMissed", type=DataSourceType.GAUGE)
    public int getNumberMissedCycles() {
        if (!serverRefreshEnabled) {
            return 0;
        }
        return (int) ((int) (System.currentTimeMillis() - lastUpdated.get()) / refeshIntervalMills);
    }
    
    @Monitor(name="LastUpdated", type=DataSourceType.INFORMATIONAL)
    public String getLastUpdate() {
        return new Date(lastUpdated.get()).toString();
    }
    
    @Monitor(name="NumThreads", type=DataSourceType.GAUGE) 
    public int getCoreThreads() {
        if (_serverListRefreshExecutor != null) {
            return _serverListRefreshExecutor.getCorePoolSize();
        } else {
            return 0;
        }
    }
    
    public String toString() {
        StringBuilder sb = new StringBuilder("DynamicServerListLoadBalancer:");
        sb.append(super.toString());
        sb.append("ServerList:" + String.valueOf(serverListImpl));
        return sb.toString();
    }
}=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.loadbalancer;

import java.util.Date;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ScheduledFuture;
import java.util.concurrent.ScheduledThreadPoolExecutor;
import java.util.concurrent.ThreadFactory;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicLong;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.netflix.client.ClientFactory;
import com.netflix.client.config.CommonClientConfigKey;
import com.netflix.client.config.DefaultClientConfigImpl;
import com.netflix.client.config.IClientConfig;
import com.netflix.config.DynamicIntProperty;
import com.netflix.config.DynamicProperty;
import com.netflix.servo.annotations.DataSourceType;
import com.netflix.servo.annotations.Monitor;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.util.concurrent.ThreadFactoryBuilder;

/**
 * A LoadBalancer that has the capabilities to obtain the candidate list of
 * servers using a dynamic source. i.e. The list of servers can potentially be
 * changed at Runtime. It also contains facilities wherein the list of servers
 * can be passed through a Filter criteria to filter out servers that do not
 * meet the desired criteria.
 * 
 * @author stonse
 * 
 */
public class DynamicServerListLoadBalancer<T extends Server> extends
        BaseLoadBalancer {
    private static final Logger LOGGER = LoggerFactory
            .getLogger(DynamicServerListLoadBalancer.class);

    boolean isSecure = false;
    boolean useTunnel = false;
    private static Thread _shutdownThread;

    // to keep track of modification of server lists
    protected AtomicBoolean serverListUpdateInProgress = new AtomicBoolean(
            false);

    private static long LISTOFSERVERS_CACHE_UPDATE_DELAY = 1000; // msecs;
    private static long LISTOFSERVERS_CACHE_REPEAT_INTERVAL = 30 * 1000; // msecs;
                                                                         // //
                                                                         // every
                                                                         // 30
                                                                         // secs

    private static ScheduledThreadPoolExecutor _serverListRefreshExecutor = null;

    private long refeshIntervalMills = LISTOFSERVERS_CACHE_REPEAT_INTERVAL;

    volatile ServerList<T> serverListImpl;

    volatile ServerListFilter<T> filter;
    
    private AtomicLong lastUpdated = new AtomicLong(System.currentTimeMillis());
    
    protected volatile boolean serverRefreshEnabled = false;
    private final static String CORE_THREAD = "DynamicServerListLoadBalancer.ThreadPoolSize";
    private final static DynamicIntProperty poolSizeProp = new DynamicIntProperty(CORE_THREAD, 2);
    
    private volatile ScheduledFuture<?> scheduledFuture;

    static {
        int coreSize = poolSizeProp.get();
        ThreadFactory factory = (new ThreadFactoryBuilder()).setDaemon(true).build();
        _serverListRefreshExecutor = new ScheduledThreadPoolExecutor(coreSize, factory);
        poolSizeProp.addCallback(new Runnable() {
            @Override
            public void run() {
                _serverListRefreshExecutor.setCorePoolSize(poolSizeProp.get());
            }
        
        });
        _shutdownThread = new Thread(new Runnable() {
            public void run() {
                LOGGER.info("Shutting down the Executor Pool for DynamicServerListLoadBalancer");
                shutdownExecutorPool();
            }
        });
        Runtime.getRuntime().addShutdownHook(_shutdownThread);
    }
    
    public DynamicServerListLoadBalancer() {
        super();
    }

    public DynamicServerListLoadBalancer(IClientConfig niwsClientConfig) {
        initWithNiwsConfig(niwsClientConfig);
    }

    @Override
    public void initWithNiwsConfig(IClientConfig clientConfig) {
        try {
            super.initWithNiwsConfig(clientConfig);
            String niwsServerListClassName = clientConfig.getProperty(
                    CommonClientConfigKey.NIWSServerListClassName,
                    DefaultClientConfigImpl.DEFAULT_SEVER_LIST_CLASS)
                    .toString();

            ServerList<T> niwsServerListImpl = (ServerList<T>) ClientFactory
                    .instantiateInstanceWithClientConfig(
                            niwsServerListClassName, clientConfig);
            this.serverListImpl = niwsServerListImpl;

            if (niwsServerListImpl instanceof AbstractServerList) {
                AbstractServerListFilter<T> niwsFilter = ((AbstractServerList) niwsServerListImpl)
                        .getFilterImpl(clientConfig);
                niwsFilter.setLoadBalancerStats(getLoadBalancerStats());
                this.filter = niwsFilter;
            }

            refeshIntervalMills = Integer.valueOf(clientConfig.getProperty(
                    CommonClientConfigKey.ServerListRefreshInterval,
                    LISTOFSERVERS_CACHE_REPEAT_INTERVAL).toString());

            boolean primeConnection = this.isEnablePrimingConnections();
            // turn this off to avoid duplicated asynchronous priming done in BaseLoadBalancer.setServerList()
            this.setEnablePrimingConnections(false);
            enableAndInitLearnNewServersFeature();

            updateListOfServers();
            if (primeConnection && this.getPrimeConnections() != null) {
                this.getPrimeConnections()
                        .primeConnections(getServerList(true));
            }
            this.setEnablePrimingConnections(primeConnection);

        } catch (Exception e) {
            throw new RuntimeException(
                    "Exception while initializing NIWSDiscoveryLoadBalancer:"
                            + clientConfig.getClientName()
                            + ", niwsClientConfig:" + clientConfig, e);
        }
    }

    @Override
    public void setServersList(List lsrv) {
        super.setServersList(lsrv);
        List<T> serverList = (List<T>) lsrv;
        Map<String, List<Server>> serversInZones = new HashMap<String, List<Server>>();
        for (Server server : serverList) {
            // make sure ServerStats is created to avoid creating them on hot
            // path
            getLoadBalancerStats().getSingleServerStat(server);
            String zone = server.getZone();
            if (zone != null) {
                zone = zone.toLowerCase();
                List<Server> servers = serversInZones.get(zone);
                if (servers == null) {
                    servers = new ArrayList<Server>();
                    serversInZones.put(zone, servers);
                }
                servers.add(server);
            }
        }
        setServerListForZones(serversInZones);
    }

    protected void setServerListForZones(
            Map<String, List<Server>> zoneServersMap) {
        LOGGER.debug("Setting server list for zones: {}", zoneServersMap);
        getLoadBalancerStats().updateZoneServerMapping(zoneServersMap);
    }

    public ServerList<T> getServerListImpl() {
        return serverListImpl;
    }

    public void setServerListImpl(ServerList<T> niwsServerList) {
        this.serverListImpl = niwsServerList;
    }

    @Override
    public void setPing(IPing ping) {
        this.ping = ping;
    }

    public ServerListFilter<T> getFilter() {
        return filter;
    }

    public void setFilter(ServerListFilter<T> filter) {
        this.filter = filter;
    }

    @Override
    /**
     * Makes no sense to ping an inmemory disc client
     * 
     */
    public void forceQuickPing() {
        // no-op
    }

    /**
     * Feature that lets us add new instances (from AMIs) to the list of
     * existing servers that the LB will use Call this method if you want this
     * feature enabled
     */
    public void enableAndInitLearnNewServersFeature() {
        keepServerListUpdated();
        serverRefreshEnabled = true;
    }

    private String getIdentifier() {
        return this.getClientConfig().getClientName();
    }

    private void keepServerListUpdated() {
        scheduledFuture = _serverListRefreshExecutor.scheduleAtFixedRate(
                new ServerListRefreshExecutorThread(),
                LISTOFSERVERS_CACHE_UPDATE_DELAY, refeshIntervalMills,
                TimeUnit.MILLISECONDS);
    }

    private static void shutdownExecutorPool() {
        if (_serverListRefreshExecutor != null) {
            _serverListRefreshExecutor.shutdown();

            if (_shutdownThread != null) {
                try {
                    Runtime.getRuntime().removeShutdownHook(_shutdownThread);
                } catch (IllegalStateException ise) { // NOPMD
                    // this can happen if we're in the middle of a real
                    // shutdown,
                    // and that's 'ok'
                }
            }

        }
    }

    public void stopServerListRefreshing() {
        serverRefreshEnabled = false;
        if (scheduledFuture != null) {
            scheduledFuture.cancel(true);
        }
    }
    
    /**
     * Class that updates the list of Servers This is based on the method used
     * by the client * Appropriate Filters are applied before coming up with the
     * right set of servers
     * 
     * @author stonse
     * 
     */
    class ServerListRefreshExecutorThread implements Runnable {

        public void run() {
            if (!serverRefreshEnabled) {
                if (scheduledFuture != null) {
                    scheduledFuture.cancel(true);
                }
                return;
            }
            try {
                updateListOfServers();

            } catch (Throwable e) {
                LOGGER.error(
                        "Exception while updating List of Servers obtained from Discovery client",
                        e);
                // e.printStackTrace();
            }
        }

    }

    @VisibleForTesting
    public void updateListOfServers() {
        List<T> servers = new ArrayList<T>();
        if (serverListImpl != null) {
            servers = serverListImpl.getUpdatedListOfServers();
            LOGGER.debug("List of Servers for {} obtained from Discovery client: {}",
                    getIdentifier(), servers);

            if (filter != null) {
                servers = filter.getFilteredListOfServers(servers);
                LOGGER.debug("Filtered List of Servers for {} obtained from Discovery client: {}",
                        getIdentifier(), servers);
            }
        }
        lastUpdated.set(System.currentTimeMillis());
        updateAllServerList(servers);
    }

    /**
     * Update the AllServer list in the LoadBalancer if necessary and enabled
     * 
     * @param ls
     */
    protected void updateAllServerList(List<T> ls) {
        // other threads might be doing this - in which case, we pass
        if (!serverListUpdateInProgress.get()) {
            serverListUpdateInProgress.set(true);
            for (T s : ls) {
                s.setAlive(true); // set so that clients can start using these
                                  // servers right away instead
                // of having to wait out the ping cycle.
            }
            setServersList(ls);
            super.forceQuickPing();
            serverListUpdateInProgress.set(false);
        }
    }

    @Monitor(name="NumUpdateCyclesMissed", type=DataSourceType.GAUGE)
    public int getNumberMissedCycles() {
        if (!serverRefreshEnabled) {
            return 0;
        }
        return (int) ((int) (System.currentTimeMillis() - lastUpdated.get()) / refeshIntervalMills);
    }
    
    @Monitor(name="LastUpdated", type=DataSourceType.INFORMATIONAL)
    public String getLastUpdate() {
        return new Date(lastUpdated.get()).toString();
    }
    
    @Monitor(name="NumThreads", type=DataSourceType.GAUGE) 
    public int getCoreThreads() {
        if (_serverListRefreshExecutor != null) {
            return _serverListRefreshExecutor.getCorePoolSize();
        } else {
            return 0;
        }
    }
    
    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder("DynamicServerListLoadBalancer:");
        sb.append(super.toString());
        sb.append("ServerList:" + String.valueOf(serverListImpl));
        return sb.toString();
    }
    
    @Override 
    public void shutdown() {
        super.shutdown();
        stopServerListRefreshing();
    }
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_4be8bde_47e5ff8/rev_4be8bde-47e5ff8/ribbon-core/src/test/java/com/netflix/client/LoadBalancerContextTest.java;<<<<<<< MINE
||||||| BASE
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.client;

import static org.junit.Assert.*;

import java.net.URLEncoder;

import org.junit.Test;

import com.netflix.client.http.HttpRequest;
import com.netflix.client.http.HttpResponse;
import com.netflix.loadbalancer.BaseLoadBalancer;
import com.netflix.loadbalancer.Server;

public class LoadBalancerContextTest {

    static BaseLoadBalancer lb = new BaseLoadBalancer() {

        @Override
        public Server chooseServer(Object key) {
            return new Server("www.example.com:8080");
        }
    };
    
    
    private MyLoadBalancerContext context;
    
    public LoadBalancerContextTest() {
        context = new MyLoadBalancerContext();
        context.setLoadBalancer(lb);
    }
    
    @Test
    public void testComputeFinalUriWithLoadBalancer() throws ClientException {
        HttpRequest request = HttpRequest.newBuilder().uri("/test?abc=xyz").build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals("http://www.example.com:8080/test?abc=xyz", newRequest.getUri().toString());
    }
    
    @Test
    public void testEncodedPath() throws ClientException {
        String uri = "http://localhost:8080/resources/abc%2Fxyz";
        HttpRequest request = HttpRequest.newBuilder().uri(uri).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals(uri, newRequest.getUri().toString());
    }
    
    @Test
    public void testPreservesUserInfo() throws ClientException {
        // %3A == ":" -- ensure user info is not decoded
        String uri = "http://us%3Aer:pass@localhost:8080?foo=bar";
        HttpRequest request = HttpRequest.newBuilder().uri(uri).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals(uri, newRequest.getUri().toString());
    }
    
    @Test
    public void testQueryWithoutPath() throws ClientException {
        String uri = "?foo=bar";
        HttpRequest request = HttpRequest.newBuilder().uri(uri).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals("http://www.example.com:8080?foo=bar", newRequest.getUri().toString());
    }
    
    @Test
    public void testEncodedPathAndHostChange() throws ClientException {
        String uri = "/abc%2Fxyz";
        HttpRequest request = HttpRequest.newBuilder().uri(uri).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals("http://www.example.com:8080" + uri, newRequest.getUri().toString());
    }

    
    @Test
    public void testEncodedQuery() throws Exception {
        String uri = "http://localhost:8080/resources/abc?";
        String queryString = "name=" + URLEncoder.encode("&=*%!@#$%^&*()", "UTF-8");   
        HttpRequest request = HttpRequest.newBuilder().uri(uri + queryString).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals(uri + queryString, newRequest.getUri().toString());        
    }
}

class MyLoadBalancerContext extends LoadBalancerContext<HttpRequest, HttpResponse> {
}=======
/*
 *
 * Copyright 2013 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.client;

import static org.junit.Assert.*;

import java.net.URLEncoder;

import org.junit.Test;

import com.netflix.client.http.HttpRequest;
import com.netflix.client.http.HttpResponse;
import com.netflix.loadbalancer.BaseLoadBalancer;
import com.netflix.loadbalancer.Server;

public class LoadBalancerContextTest {

    static BaseLoadBalancer lb = new BaseLoadBalancer() {

        @Override
        public Server chooseServer(Object key) {
            return new Server("www.example.com:8080");
        }
    };
    
    
    private MyLoadBalancerContext context;
    
    public LoadBalancerContextTest() {
        context = new MyLoadBalancerContext();
        context.setLoadBalancer(lb);
    }
    
    @Test
    public void testComputeFinalUriWithLoadBalancer() throws ClientException {
        HttpRequest request = HttpRequest.newBuilder().uri("/test?abc=xyz").build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals("http://www.example.com:8080/test?abc=xyz", newRequest.getUri().toString());
    }
    
    @Test
    public void testEncodedPath() throws ClientException {
        String uri = "http://localhost:8080/resources/abc%2Fxyz";
        HttpRequest request = HttpRequest.newBuilder().uri(uri).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals(uri, newRequest.getUri().toString());
    }
    
    @Test
    public void testPreservesUserInfo() throws ClientException {
        // %3A == ":" -- ensure user info is not decoded
        String uri = "http://us%3Aer:pass@localhost:8080?foo=bar";
        HttpRequest request = HttpRequest.newBuilder().uri(uri).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals(uri, newRequest.getUri().toString());
    }
    
    @Test
    public void testQueryWithoutPath() throws ClientException {
        String uri = "?foo=bar";
        HttpRequest request = HttpRequest.newBuilder().uri(uri).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals("http://www.example.com:8080?foo=bar", newRequest.getUri().toString());
    }
    
    @Test
    public void testEncodedPathAndHostChange() throws ClientException {
        String uri = "/abc%2Fxyz";
        HttpRequest request = HttpRequest.newBuilder().uri(uri).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals("http://www.example.com:8080" + uri, newRequest.getUri().toString());
    }

    
    @Test
    public void testEncodedQuery() throws Exception {
        String uri = "http://localhost:8080/resources/abc?";
        String queryString = "name=" + URLEncoder.encode("&=*%!@#$%^&*()", "UTF-8");   
        HttpRequest request = HttpRequest.newBuilder().uri(uri + queryString).build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals(uri + queryString, newRequest.getUri().toString());        
    }
    
    @Test
    public void testComputeFinalUriWithLoadBalancer_regressionRaw() throws ClientException {
        HttpRequest request = HttpRequest.newBuilder().uri("/test?ampersand=foo%26bar").build();
        HttpRequest newRequest = context.computeFinalUriWithLoadBalancer(request);
        assertEquals("http://www.example.com:8080/test?ampersand=foo%26bar", newRequest.getUri().toString());
    }
}

class MyLoadBalancerContext extends LoadBalancerContext<HttpRequest, HttpResponse> {
}>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_4be8bde_47e5ff8/rev_4be8bde-47e5ff8/ribbon-httpclient/src/main/java/com/netflix/niws/client/http/RestClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_4be8bde_47e5ff8/rev_4be8bde-47e5ff8/ribbon-httpclient/src/main/java/com/netflix/niws/client/http/RestClient.java;null
/home/taes/taes/projects/ribbon/revisions/rev_4be8bde_47e5ff8/rev_4be8bde-47e5ff8/ribbon-httpclient/src/main/java/com/netflix/niws/client/http/RestClient.java;<<<<<<< MINE
import com.netflix.client.ClientFactory;
import com.netflix.client.RequestSpecificRetryHandler;
||||||| BASE
import com.netflix.client.ClientRequest;
=======
import com.netflix.client.ClientFactory;
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_4be8bde_47e5ff8/rev_4be8bde-47e5ff8/ribbon-httpclient/src/main/java/com/netflix/niws/client/http/RestClient.java;<<<<<<< MINE
import com.netflix.loadbalancer.ILoadBalancer;
||||||| BASE
=======
import com.netflix.loadbalancer.BaseLoadBalancer;
import com.netflix.loadbalancer.ILoadBalancer;
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_4be8bde_47e5ff8/rev_4be8bde-47e5ff8/ribbon-httpclient/src/main/java/com/netflix/niws/client/http/RestClient.java;<<<<<<< MINE
import com.netflix.serialization.SerializationUtils;
import com.netflix.serialization.Serializer;
import com.netflix.serialization.TypeDef;
||||||| BASE
import com.netflix.niws.client.http.HttpClientRequest.Verb;
=======
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_4be8bde_47e5ff8/rev_4be8bde-47e5ff8/ribbon-httpclient/src/main/java/com/netflix/niws/client/http/RestClient.java;<<<<<<< MINE

    @Override
    public RequestSpecificRetryHandler getRequestSpecificRetryHandler(
            HttpRequest request, IClientConfig requestConfig) {
        if (!request.isRetriable()) {
            return new RequestSpecificRetryHandler(false, false, this.getErrorHandler(), requestConfig);
        }
        if (request.getVerb() != HttpRequest.Verb.GET) {
            return new RequestSpecificRetryHandler(true, false, this.getErrorHandler(), requestConfig);
        } else {
            return new RequestSpecificRetryHandler(true, true, this.getErrorHandler(), requestConfig);
        } 
    }
||||||| BASE
=======
	
	public void shutdown() {
	    ILoadBalancer lb = this.getLoadBalancer();
	    if (lb instanceof BaseLoadBalancer) {
	        ((BaseLoadBalancer) lb).shutdown();
	    }
	    NFHttpClientFactory.shutdownNFHttpClient(restClientName);
	}
>>>>>>> YOURS
/home/taes/taes/projects/archaius/revisions/rev_450add0_b99c9ae/rev_450add0-b99c9ae/archaius-aws/src/test/java/com/netflix/config/sources/DynamoDbConfigurationSourceTest.java;<<<<<<< MINE

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Ignore;
import org.junit.Test;
||||||| BASE

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;
=======
import static org.mockito.Mockito.*;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_d005ea3_539efc9/rev_d005ea3-539efc9/src/java/com/twitter/elephantbird/util/Protobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_d005ea3_539efc9/rev_d005ea3-539efc9/src/java/com/twitter/elephantbird/util/Protobufs.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_f875c0b_8acda1f/rev_f875c0b-8acda1f/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;<<<<<<< MINE
import com.twitter.elephantbird.thrift.test.TestName;
import com.twitter.elephantbird.thrift.test.TestPerson;
import com.twitter.elephantbird.thrift.test.TestPhoneType;
||||||| BASE
=======
import com.twitter.elephantbird.thrift.TStructDescriptor.Field;
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_aebc2c4_89ee7d8/rev_aebc2c4-89ee7d8/ribbon-client-extensions/src/main/java/com/netflix/ribbonclientextensions/Ribbon.java;<<<<<<< MINE
import com.netflix.ribbonclientextensions.http.HttpRequestTemplate;

import com.netflix.ribbonclientextensions.typedclient.RibbonDynamicProxy;
import io.reactivex.netty.protocol.http.client.HttpClient;
||||||| BASE
import com.netflix.ribbonclientextensions.http.HttpRequestTemplate;

import io.reactivex.netty.protocol.http.client.HttpClient;
=======
import com.netflix.ribbonclientextensions.http.HttpResourceGroup;
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_aebc2c4_89ee7d8/rev_aebc2c4-89ee7d8/ribbon-client-extensions/src/main/java/com/netflix/ribbonclientextensions/Ribbon.java;<<<<<<< MINE
 
    public static <I, O, T> T from(Class<T> contract, HttpClient<I, O> transportClient) {
        return RibbonDynamicProxy.newInstance(contract, transportClient);
||||||| BASE
 
    public static <I, O, T> T from(Class<T> contract, HttpClient<I, O> transportClient) {
        return null;
=======

    public static <T> T from(Class<T> contract) {
        return null;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_3ef3270_e8d76ea/rev_3ef3270-e8d76ea/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;<<<<<<< MINE
import com.twitter.elephantbird.thrift.TStructDescriptor.Field;
||||||| BASE
=======
import com.twitter.elephantbird.thrift.test.TestName;
import com.twitter.elephantbird.thrift.test.TestPerson;
import com.twitter.elephantbird.thrift.test.TestPhoneType;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/PigToThrift.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ProtobufTuple.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/AbstractLazyTuple.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/util/ProtobufToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoThriftB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoThriftB64LinePigLoader.java;<<<<<<< MINE
import com.twitter.elephantbird.pig.util.ProjectedThriftTuple;
||||||| BASE
=======
import com.twitter.elephantbird.pig.util.ProjectedThriftTupleFactory;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoThriftB64LinePigLoader.java;<<<<<<< MINE
  protected final TypeRef<M> typeRef_;
  private ProjectedThriftTuple<M> tupleTemplate;
||||||| BASE
  private final TypeRef<M> typeRef_;
  private final ThriftToPig<M> thriftToPig_;
=======
  protected final TypeRef<M> typeRef_;
  private ProjectedThriftTupleFactory<M> tupleTemplate;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoThriftB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoThriftB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoThriftBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoThriftBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoThriftBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoThriftBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoThriftBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoBaseLoadFunc.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoBaseLoadFunc.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoBaseLoadFunc.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoBaseLoadFunc.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoBaseLoadFunc.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoBaseLoadFunc.java;<<<<<<< MINE
||||||| BASE
  protected final String LZO_EXTENSION = new LzopCodec().getDefaultExtension();

=======
  protected final String LZO_EXTENSION = new LzopCodec().getDefaultExtension();

  @SuppressWarnings("unchecked")
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoBaseLoadFunc.java;<<<<<<< MINE
  protected String contextSignature;
  protected static final String projectionSuffix = "_LzoBaseLoadFunc_projectedFields";

  protected RequiredFieldList requiredFieldList = null;
||||||| BASE
=======
  protected String contextSignature;
  protected static final String projectionKey = "LzoBaseLoadFunc_projectedFields";

  protected RequiredFieldList requiredFieldList = null;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoBaseLoadFunc.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoBaseLoadFunc.java;<<<<<<< MINE
  // LoadPushDown implementation:

  @Override
  public List<OperatorSet> getFeatures() {
    return Arrays.asList(LoadPushDown.OperatorSet.PROJECTION);
  }

  @Override
  public RequiredFieldResponse pushProjection(
      RequiredFieldList requiredFieldList) throws FrontendException {
    // the default implementation disables the feature.
    // a projection needs to be explicitly supported by sub classes.
    return null;
  }

  /**
   * A helper method for implementing
   * {@link LoadPushDown#pushProjection(RequiredFieldList)}. <p>
   *
   * Stores requiredFieldList in context. The requiredFields are read from
   * context on the backend inside {@link #setLocation(String, Job)}.
   */
  protected RequiredFieldResponse pushProjectionHelper(
                                          RequiredFieldList requiredFieldList)
                                          throws FrontendException {
    try {
      getUDFProperties().setProperty(
                          contextSignature + projectionSuffix,
                          ObjectSerializer.serialize(requiredFieldList));
    } catch (IOException e) { // not expected
      throw new FrontendException(e);
    }

    return new RequiredFieldResponse(true);
  }

||||||| BASE
=======
  // LoadPushDown implementation:

  @Override
  public List<OperatorSet> getFeatures() {
    return Arrays.asList(LoadPushDown.OperatorSet.PROJECTION);
  }

  @Override
  public RequiredFieldResponse pushProjection(
      RequiredFieldList requiredFieldList) throws FrontendException {
    // the default implementation disables the feature.
    // a projection needs to be explicitly supported by sub classes.
    return null;
  }

  /**
   * A helper method for implementing
   * {@link LoadPushDown#pushProjection(RequiredFieldList)}. <p>
   *
   * Stores requiredFieldList in context. The requiredFields are read from
   * context on the backend (in side {@link #setLocation(String, Job)}).
   */
  protected RequiredFieldResponse pushProjectionHelper(
                                          RequiredFieldList requiredFieldList)
                                          throws FrontendException {
    try {
      getUDFProperties().setProperty(projectionKey,
                                     ObjectSerializer.serialize(requiredFieldList));
    } catch (IOException e) { // not expected
      throw new FrontendException(e);
    }

    return new RequiredFieldResponse(true);
  }

>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoProtobufBlockPigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;<<<<<<< MINE
import com.twitter.elephantbird.pig.util.ProjectedProtoTuple;
||||||| BASE
=======
import com.twitter.elephantbird.pig.util.ProjectedProtobufTupleFactory;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;<<<<<<< MINE
  protected TypeRef<M> typeRef_ = null;
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();
  private ProjectedProtoTuple<M> tupleTemplate = null;
||||||| BASE
  private TypeRef<M> typeRef_ = null;
  private final ProtobufToPig protoToPig_ = new ProtobufToPig();
=======
  protected TypeRef<M> typeRef = null;
  private final ProtobufToPig protoToPig = new ProtobufToPig();
  private ProjectedProtobufTupleFactory<M> tupleTemplate = null;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/java/com/twitter/elephantbird/pig/load/LzoProtobufB64LinePigLoader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/test/com/twitter/elephantbird/pig/piggybank/TestThriftToPig.java;<<<<<<< MINE
import com.twitter.elephantbird.pig.util.ProjectedThriftTuple;
||||||| BASE
=======
import com.twitter.elephantbird.pig.util.ProjectedThriftTupleFactory;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/test/com/twitter/elephantbird/pig/piggybank/TestProtoToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/test/com/twitter/elephantbird/pig/piggybank/TestProtoToPig.java;<<<<<<< MINE
import com.twitter.elephantbird.pig.util.PigUtil;
import com.twitter.elephantbird.pig.util.ProjectedProtoTuple;
||||||| BASE
=======
import com.twitter.elephantbird.pig.util.PigUtil;
import com.twitter.elephantbird.pig.util.ProjectedProtobufTupleFactory;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/test/com/twitter/elephantbird/pig/piggybank/TestProtoToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/test/com/twitter/elephantbird/pig/piggybank/TestProtoToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_b784885_342f336/rev_b784885-342f336/src/test/com/twitter/elephantbird/pig/piggybank/TestProtoToPig.java;null
/home/taes/taes/projects/ribbon/revisions/rev_786e464_200cffe/rev_786e464-200cffe/ribbon-transport/src/main/java/com/netflix/client/netty/http/NettyHttpClient.java;<<<<<<< MINE
||||||| BASE
import rx.functions.Func1;
=======
import rx.Subscription;
import rx.functions.Func1;
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_786e464_200cffe/rev_786e464-200cffe/ribbon-transport/src/test/java/com/netflix/client/netty/http/NettyClientTest.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_13f2963_3a82539/rev_13f2963-3a82539/src/java/com/twitter/elephantbird/cascading2/scheme/LzoTextDelimited.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_13f2963_3a82539/rev_13f2963-3a82539/src/java/com/twitter/elephantbird/cascading2/scheme/LzoTextDelimited.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_13f2963_3a82539/rev_13f2963-3a82539/src/java/com/twitter/elephantbird/cascading2/scheme/LzoTextDelimited.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_13f2963_3a82539/rev_13f2963-3a82539/src/java/com/twitter/elephantbird/cascading2/scheme/LzoTextDelimited.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_13f2963_3a82539/rev_13f2963-3a82539/src/java/com/twitter/elephantbird/cascading2/scheme/LzoTextLine.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_13f2963_3a82539/rev_13f2963-3a82539/src/java/com/twitter/elephantbird/cascading2/scheme/LzoTextLine.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_13f2963_3a82539/rev_13f2963-3a82539/src/java/com/twitter/elephantbird/cascading2/scheme/LzoTextLine.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_13f2963_3a82539/rev_13f2963-3a82539/src/java/com/twitter/elephantbird/cascading2/scheme/LzoTextLine.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_13f2963_3a82539/rev_13f2963-3a82539/src/java/com/twitter/elephantbird/mapred/output/DeprecatedLzoThriftB64LineOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_13f2963_3a82539/rev_13f2963-3a82539/src/java/com/twitter/elephantbird/mapred/output/DeprecatedLzoOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_13f2963_3a82539/rev_13f2963-3a82539/src/java/com/twitter/elephantbird/mapred/output/DeprecatedLzoOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_13f2963_3a82539/rev_13f2963-3a82539/src/java/com/twitter/elephantbird/mapred/output/DeprecatedLzoOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_13f2963_3a82539/rev_13f2963-3a82539/src/java/com/twitter/elephantbird/mapred/output/DeprecatedLzoOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_13f2963_3a82539/rev_13f2963-3a82539/src/java/com/twitter/elephantbird/mapred/output/DeprecatedLzoTextOutputFormat.java;<<<<<<< MINE
package com.twitter.elephantbird.mapred.output;

import java.io.IOException;

import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.RecordWriter;
import org.apache.hadoop.mapred.TextOutputFormat;
import org.apache.hadoop.util.Progressable;

import com.hadoop.compression.lzo.LzopCodec;
import com.twitter.elephantbird.util.LzoUtils;

@Deprecated
public class DeprecatedLzoTextOutputFormat<K, V> extends TextOutputFormat<K, V> {

  //non-deprecated LzoTextOutputFormat should also extend TextOutputFormat.

  @Override
  public RecordWriter<K, V> getRecordWriter(FileSystem ignored, JobConf job,
      String name, Progressable progress) throws IOException {

    Path file = getPathForCustomFile(job,  "part");
    file = file.suffix(LzopCodec.DEFAULT_LZO_EXTENSION);

    return new LineRecordWriter<K, V>(
                  LzoUtils.getIndexedLzoOutputStream(job, file),
                  job.get("mapred.textoutputformat.separator", "\t"));
  }

}||||||| BASE
=======
package com.twitter.elephantbird.mapred.output;

import java.io.DataOutputStream;
import java.io.IOException;

import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.RecordWriter;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.util.Progressable;

public class DeprecatedLzoTextOutputFormat
            extends DeprecatedLzoOutputFormat<NullWritable, Text> {

  @Override
  public RecordWriter<NullWritable, Text> getRecordWriter(FileSystem ignored,
      JobConf job, String name, Progressable progress) throws IOException {

    final DataOutputStream out = getOutputStream(job);

    return new RecordWriter<NullWritable, Text>() {

      public void close(Reporter reporter) throws IOException {
        out.close();
      }

      public void write(NullWritable key, Text value) throws IOException {
        out.write(value.getBytes(), 0, value.getLength());
        out.write('\n');
      }
    };
  }

}>>>>>>> YOURS
/home/taes/taes/projects/curator/revisions/rev_74434b8_8cd8347/rev_74434b8-8cd8347/curator-framework/src/main/java/com/netflix/curator/framework/listen/ListenerContainer.java;null
/home/taes/taes/projects/curator/revisions/rev_74434b8_8cd8347/rev_74434b8-8cd8347/curator-framework/src/main/java/com/netflix/curator/framework/listen/ListenerContainer.java;null
/home/taes/taes/projects/curator/revisions/rev_d4c0be0_ff4ec29/rev_d4c0be0-ff4ec29/curator-framework/src/main/java/com/netflix/curator/framework/imps/CreateBuilderImpl.java;null
/home/taes/taes/projects/curator/revisions/rev_d4c0be0_ff4ec29/rev_d4c0be0-ff4ec29/curator-framework/src/main/java/com/netflix/curator/framework/imps/CreateBuilderImpl.java;null
/home/taes/taes/projects/curator/revisions/rev_d4c0be0_ff4ec29/rev_d4c0be0-ff4ec29/curator-framework/src/main/java/com/netflix/curator/framework/imps/CreateBuilderImpl.java;null
/home/taes/taes/projects/curator/revisions/rev_d4c0be0_ff4ec29/rev_d4c0be0-ff4ec29/curator-framework/src/main/java/com/netflix/curator/framework/imps/CuratorFrameworkImpl.java;null
/home/taes/taes/projects/curator/revisions/rev_d4c0be0_ff4ec29/rev_d4c0be0-ff4ec29/curator-framework/src/main/java/com/netflix/curator/framework/imps/CuratorFrameworkImpl.java;null
/home/taes/taes/projects/curator/revisions/rev_d4c0be0_ff4ec29/rev_d4c0be0-ff4ec29/curator-framework/src/main/java/com/netflix/curator/framework/imps/CuratorFrameworkImpl.java;null
/home/taes/taes/projects/curator/revisions/rev_d4c0be0_ff4ec29/rev_d4c0be0-ff4ec29/curator-framework/src/main/java/com/netflix/curator/framework/imps/CuratorFrameworkImpl.java;null
/home/taes/taes/projects/curator/revisions/rev_d4c0be0_ff4ec29/rev_d4c0be0-ff4ec29/curator-framework/src/main/java/com/netflix/curator/framework/imps/CuratorFrameworkImpl.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_723eb3c_11dfa53/rev_723eb3c-11dfa53/src/java/com/twitter/elephantbird/thrift/TStructDescriptor.java;null
/home/taes/taes/projects/archaius/revisions/rev_1ad0c78_5296b30/rev_1ad0c78-5296b30/archaius-core/src/main/java/com/netflix/config/DynamicStringSetProperty.java;<<<<<<< MINE
/*
 * Copyright 2014 Netflix, Inc.
||||||| BASE
/*
 *
 * Copyright 2013 Netflix, Inc.
=======
/**
 * Copyright 2014 Netflix, Inc.
>>>>>>> YOURS
/home/taes/taes/projects/archaius/revisions/rev_1ad0c78_5296b30/rev_1ad0c78-5296b30/archaius-core/src/main/java/com/netflix/config/DynamicStringSetProperty.java;null
/home/taes/taes/projects/archaius/revisions/rev_1ad0c78_5296b30/rev_1ad0c78-5296b30/archaius-core/src/main/java/com/netflix/config/DynamicStringSetProperty.java;null
/home/taes/taes/projects/ribbon/revisions/rev_5c67188_f3e8cdf/rev_5c67188-f3e8cdf/ribbon/src/main/java/com/netflix/ribbon/http/HttpResourceGroup.java;null
/home/taes/taes/projects/ribbon/revisions/rev_5c67188_f3e8cdf/rev_5c67188-f3e8cdf/ribbon/src/main/java/com/netflix/ribbon/HttpResourceGroupFactory.java;<<<<<<< MINE
||||||| BASE
package com.netflix.ribbon;

import com.netflix.client.config.IClientConfig;
import com.netflix.ribbon.http.HttpResourceGroup;
import com.netflix.ribbon.proxy.RibbonDynamicProxy;

/**
 * Factory for creating an HttpResourceGroup.  For DI either bind DefaultHttpResourceGroupFactory
 * or implement your own to customize or override HttpResourceGroup.
 * 
 * @author elandau
 */
public interface HttpResourceGroupFactory {
    HttpResourceGroup createHttpResourceGroup(String name, ClientOptions options);
    
    HttpResourceGroup createHttpResourceGroup(String name, IClientConfig config);
    
    <T> T from(Class<T> classType);
    
    <T> T from(Class<T> classType, HttpResourceGroup resourceGroup);
}=======
package com.netflix.ribbon;

import com.netflix.client.config.IClientConfig;
import com.netflix.ribbon.http.HttpResourceGroup;

/**
 * Factory for creating an HttpResourceGroup.  For DI either bind DefaultHttpResourceGroupFactory
 * or implement your own to customize or override HttpResourceGroup.
 * 
 * @author elandau
 */
public interface HttpResourceGroupFactory {
    HttpResourceGroup createHttpResourceGroup(String name, ClientOptions options);
    
    HttpResourceGroup createHttpResourceGroup(String name, IClientConfig config);
    
    <T> T from(Class<T> classType);
    
    <T> T from(Class<T> classType, HttpResourceGroup resourceGroup);
}>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/RestMethodInfo.java;null
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/RestMethodInfo.java;null
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/RestMethodInfo.java;<<<<<<< MINE
  // Method-level details
  final boolean isSynchronous;
  Type responseObjectType;
  RequestType requestType = RequestType.SIMPLE;
  String requestMethod;
  boolean requestHasBody;
  String requestUrl;
  Set<String> requestUrlParamNames;
  String requestQuery;

  // Parameter-level details
  String[] requestUrlParam;
  String[] requestQueryName;
  boolean hasQueryParams = false;
  String[] requestFormPair;
  String[] requestMultipartPart;
  int bodyIndex = NO_BODY;
||||||| BASE
  Type type;
  RestMethod restMethod;
  String path;
  Set<String> pathParams;
  QueryParam[] pathQueryParams;
  String[] namedParams;
  int singleEntityArgumentIndex = NO_SINGLE_ENTITY;
  boolean isMultipart = false;
=======
  Type type;
  RestMethod restMethod;
  String path;
  Set<String> pathParams;
  QueryParam[] pathQueryParams;
  List<HeaderPair> headers;
  String[] headerParams;
  String[] namedParams;
  int singleEntityArgumentIndex = NO_SINGLE_ENTITY;
  boolean isMultipart = false;
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/RestMethodInfo.java;<<<<<<< MINE
  /** Loads {@link #requestMethod} and {@link #requestType}. */
||||||| BASE
  /**
   * Loads {@link #restMethod}, {@link #path}, {@link #pathParams}, {@link #pathQueryParams}, and
   * {@link #isMultipart}.
   */
=======
  /**
   * Loads {@link #restMethod}, {@link #path}, {@link #pathParams}, {@link #pathQueryParams},
   * {@link headers}, and {@link #isMultipart}.
   */
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/RestMethodInfo.java;<<<<<<< MINE
  /** Loads {@link #responseObjectType}. Returns {@code true} if method is synchronous. */
||||||| BASE
  /** Loads {@link #type}. Returns true if the method is synchronous. */
=======
  private List<HeaderPair> parseHeaders(String[] headersToParse) {
    List<HeaderPair> headers = new ArrayList<HeaderPair>();
    for (String headerToParse: headersToParse) {
      int colon = headerToParse.indexOf(':');
      headers.add(new HeaderPair(headerToParse.substring(0, colon),
                                 headerToParse.substring(colon + 2)));

    }
    return headers;
  }

  /** Loads {@link #type}. Returns true if the method is synchronous. */
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/RestMethodInfo.java;<<<<<<< MINE
   * Loads {@link #requestUrlParam}, {@link #requestQueryName}, {@link #requestFormPair}, and
   * {@link #requestMultipartPart}. Must be called after {@link #parseMethodAnnotations()}.
||||||| BASE
   * Loads {@link #namedParams}, {@link #singleEntityArgumentIndex}. Must be called after
   * {@link #parseMethodAnnotations()}}.
=======
   * Loads {@link #namedParams}, {@link headerParams}, {@link #singleEntityArgumentIndex},
   * Must be called after {@link #parseMethodAnnotations()}}.
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/RestMethodInfo.java;<<<<<<< MINE
      if (parameterAnnotations != null) {
        for (Annotation parameterAnnotation : parameterAnnotations) {
          Class<? extends Annotation> annotationType = parameterAnnotation.annotationType();

          if (annotationType == Path.class) {
            hasRetrofitAnnotation = true;
            String name = ((Path) parameterAnnotation).value();

            // Verify URL replacement name is actually present in the URL path.
            if (!requestUrlParamNames.contains(name)) {
              throw new IllegalStateException(
                  "Method path \"" + requestUrl + "\" does not contain {" + name + "}.");
            }

            urlParam[i] = name;
          } else if (annotationType == Query.class) {
            hasRetrofitAnnotation = true;
            hasQueryParams = true;
            String name = ((Query) parameterAnnotation).value();

            // TODO verify query name not already used in URL?

            queryName[i] = name;
          } else if (annotationType == Pair.class) {
            if (requestType != RequestType.FORM_ENCODED) {
              throw new IllegalStateException(
                  "@Pair parameters can only be used with form encoding.");
            }

            gotPair = true;
            hasRetrofitAnnotation = true;
            String name = ((Pair) parameterAnnotation).value();

            // TODO verify name not already used?

            formValue[i] = name;
          } else if (annotationType == Part.class) {
            if (requestType != RequestType.MULTIPART) {
              throw new IllegalStateException(
                  "@Part parameters can only be used with multipart encoding.");
            }

            gotPart = true;
            hasRetrofitAnnotation = true;
            String name = ((Part) parameterAnnotation).value();

            // TODO verify name not already used?

            multipartPart[i] = name;
          } else if (annotationType == Body.class) {
            if (requestType != RequestType.SIMPLE) {
              throw new IllegalStateException(
                  "@Body parameters cannot be used with form or multi-part encoding.");
            }
            if (bodyIndex != NO_BODY) {
              throw new IllegalStateException(
                  "Method annotated with multiple Body method annotations: " + method);
            }

            hasRetrofitAnnotation = true;
            bodyIndex = i;
||||||| BASE
      if (parameterAnnotations == null || parameterAnnotations.length == 0) {
        throw new IllegalStateException("Argument " + i + " lacks annotation.");
      }
      for (Annotation parameterAnnotation : parameterAnnotations) {
        Class<? extends Annotation> annotationType = parameterAnnotation.annotationType();
        if (annotationType == Name.class) {
          String name = ((Name) parameterAnnotation).value();
          namedParams[i] = name;
          boolean isPathParam = pathParams.contains(name);
          if (parameterType == TypedOutput.class && (isPathParam || !restMethod.hasBody())) {
            throw new IllegalStateException("TypedOutput cannot be used as URL parameter.");
          }
          if (!isPathParam && !isMultipart && restMethod.hasBody()) {
            throw new IllegalStateException(
                "Non-path params can only be used in multipart request.");
          }
        } else if (annotationType == SingleEntity.class) {
          if (isMultipart) {
            throw new IllegalStateException("SingleEntity cannot be used with multipart request.");
=======
      if (parameterAnnotations == null || parameterAnnotations.length == 0) {
        throw new IllegalStateException("Argument " + i + " lacks annotation.");
      }
      for (Annotation parameterAnnotation : parameterAnnotations) {
        Class<? extends Annotation> annotationType = parameterAnnotation.annotationType();
        if (annotationType == Name.class) {
          String name = ((Name) parameterAnnotation).value();
          namedParams[i] = name;
          boolean isPathParam = pathParams.contains(name);
          if (parameterType == TypedOutput.class && (isPathParam || !restMethod.hasBody())) {
            throw new IllegalStateException("TypedOutput cannot be used as URL parameter.");
          }
          if (!isPathParam && !isMultipart && restMethod.hasBody()) {
            throw new IllegalStateException(
                "Non-path params can only be used in multipart request.");
          }
        } else if (annotationType == Header.class) {
          String header = ((Header) parameterAnnotation).value();
          headerParams[i] = header;
          if (parameterType != String.class) {
            throw new IllegalStateException(
                "Expected @Header parameter type to be String: " + header);
          }
        } else if (annotationType == SingleEntity.class) {
          if (isMultipart) {
            throw new IllegalStateException("SingleEntity cannot be used with multipart request.");
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/client/UrlConnectionClient.java;<<<<<<< MINE
||||||| BASE
import retrofit.http.Header;
=======
import retrofit.http.HeaderPair;
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/client/ApacheClient.java;<<<<<<< MINE
||||||| BASE
import retrofit.http.Header;
=======
import retrofit.http.HeaderPair;
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/client/Request.java;<<<<<<< MINE
||||||| BASE
import retrofit.http.Header;
=======
import retrofit.http.HeaderPair;
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/client/Response.java;<<<<<<< MINE
||||||| BASE
import retrofit.http.Header;
=======
import retrofit.http.HeaderPair;
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/Headers.java;<<<<<<< MINE
||||||| BASE
// Copyright 2013 Square, Inc.
package retrofit.http;

import java.util.Collections;
import java.util.List;

/** Manages headers for each request. */
public interface Headers {
  /**
   * Get a list of headers for a request. This method will be called once for each request allowing
   * you to change the list as the state of your application changes.
   */
  List<Header> get();

  /** Empty header list. */
  Headers NONE = new Headers() {
    @Override public List<Header> get() {
      return Collections.emptyList();
    }
  };
}=======
// Copyright 2013 Square, Inc.
package retrofit.http;

import static java.lang.annotation.ElementType.METHOD;
import static java.lang.annotation.RetentionPolicy.RUNTIME;

import java.lang.annotation.Retention;
import java.lang.annotation.Target;

/**
 * Adds headers literally supplied in the {@code value}.
 *
 * <p/>
 * ex.
 *
 * <pre>
 * @Headers("Cache-Control: max-age=640000")
 * @GET("/")
 * ...
 *
 * @Headers({
 *   "X-Foo: Bar",
 *   "X-Ping: Pong"
 * })
 * @GET("/")
 * ...
 * </pre>
 *
 * @author Adrian Cole (adrianc@netflix.com)
 */
@Target(METHOD) @Retention(RUNTIME)
public @interface Headers {
  String[] value();
}>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/RequestBuilder.java;<<<<<<< MINE
||||||| BASE
import java.util.ArrayList;
import java.util.LinkedHashSet;
=======
import java.util.ArrayList;
import java.util.Iterator;
import java.util.LinkedHashSet;
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/RequestBuilder.java;<<<<<<< MINE
  /** A list of custom headers. */
  RequestBuilder headers(List<Header> headers) {
||||||| BASE
  RequestBuilder setHeaders(List<Header> headers) {
=======
  RequestBuilder setHeaders(List<HeaderPair> headers) {
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/RequestBuilder.java;<<<<<<< MINE
  /**
   * Construct a {@link Request} from the supplied information. You <strong>must</strong> call
   * {@link #methodInfo}, {@link #apiUrl}, {@link #args}, and {@link #headers} before invoking this
   * method.
   */
  Request build() throws UnsupportedEncodingException {
    String apiUrl = this.apiUrl;

    StringBuilder url = new StringBuilder(apiUrl);
    if (apiUrl.endsWith("/")) {
      // We enforce relative paths to start with '/'. Prevent a double-slash.
      url.deleteCharAt(url.length() - 1);
    }

    // Append the method relative URL.
    url.append(buildRelativeUrl());

    // Append query parameters, if needed.
    if (methodInfo.hasQueryParams) {
      boolean first = true;
      String requestQuery = methodInfo.requestQuery;
      if (requestQuery != null) {
        url.append(requestQuery);
        first = false;
      }
      String[] requestQueryName = methodInfo.requestQueryName;
      for (int i = 0; i < requestQueryName.length; i++) {
        String query = requestQueryName[i];
        if (query != null) {
          String value = URLEncoder.encode(String.valueOf(args[i]), "UTF-8");
          url.append(first ? '?' : '&').append(query).append('=').append(value);
          first = false;
        }
||||||| BASE
  /** List of all URL parameters. Return value will be mutated. */
  private List<Parameter> createParamList() {
    List<Parameter> params = new ArrayList<Parameter>();

    // Add arguments as parameters.
    String[] pathNamedParams = methodInfo.namedParams;
    int singleEntityArgumentIndex = methodInfo.singleEntityArgumentIndex;
    for (int i = 0; i < pathNamedParams.length; i++) {
      Object arg = args[i];
      if (arg == null) continue;
      if (i != singleEntityArgumentIndex) {
        params.add(new Parameter(pathNamedParams[i], arg, arg.getClass()));
=======
  /** List of all URL parameters. Return value will be mutated. */
  private List<Parameter> createParamList() {
    List<Parameter> params = new ArrayList<Parameter>();

    // Add arguments as parameters.
    String[] pathNamedParams = methodInfo.namedParams;
    int singleEntityArgumentIndex = methodInfo.singleEntityArgumentIndex;
    for (int i = 0; i < pathNamedParams.length; i++) {
      Object arg = args[i];
      if (arg == null || pathNamedParams[i] == null) continue;
      if (i != singleEntityArgumentIndex) {
        params.add(new Parameter(pathNamedParams[i], arg, arg.getClass()));
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/RequestBuilder.java;<<<<<<< MINE
||||||| BASE
    }

    return new Request(methodInfo.restMethod.value(), url.toString(), headers, body);
  }
=======
    }

    List<HeaderPair> headers = new ArrayList<HeaderPair>();
    if (this.headers != null) {
      headers.addAll(this.headers);
    }
    if (methodInfo.headers != null) {
      headers.addAll(methodInfo.headers);
    }
    // RFC 2616: Field names are case-insensitive
    List<String> lcHeadersToRemove = new ArrayList<String>();
    if (methodInfo.headerParams != null) {
      for (int i = 0; i < methodInfo.headerParams.length; i++) {
        String name = methodInfo.headerParams[i];
        if (name == null) continue;
        Object arg = args[i];
        if (arg != null) {
          headers.add(new HeaderPair(name, arg.toString()));
        } else {
          lcHeadersToRemove.add(name.toLowerCase());
        }
      }
    }
    for (Iterator<HeaderPair> header = headers.iterator(); header.hasNext();) {
      // RFC 2616: Field names are case-insensitive
      if (lcHeadersToRemove.contains(header.next().getName().toLowerCase()))
        header.remove();
    }
    return new Request(methodInfo.restMethod.value(), url.toString(), headers, body);
  }
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/Header.java;<<<<<<< MINE
||||||| BASE
// Copyright 2012 Square, Inc.
package retrofit.http;

/** Represents an HTTP header name/value pair. */
public final class Header {
  private final String name;
  private final String value;

  public Header(String name, String value) {
    this.name = name;
    this.value = value;
  }

  public String getName() {
    return name;
  }

  public String getValue() {
    return value;
  }

  @Override public boolean equals(Object o) {
    if (this == o) return true;
    if (o == null || getClass() != o.getClass()) return false;

    Header header = (Header) o;

    if (name != null ? !name.equals(header.name) : header.name != null) return false;
    if (value != null ? !value.equals(header.value) : header.value != null) return false;

    return true;
  }

  @Override public int hashCode() {
    int result = name != null ? name.hashCode() : 0;
    result = 31 * result + (value != null ? value.hashCode() : 0);
    return result;
  }

  @Override public String toString() {
    return (name != null ? name : "") + ": " + (value != null ? value : "");
  }
}=======
// Copyright 2013 Square, Inc.
package retrofit.http;

import java.lang.annotation.Retention;
import java.lang.annotation.Target;

import static java.lang.annotation.ElementType.PARAMETER;
import static java.lang.annotation.RetentionPolicy.RUNTIME;

/**
 * Replaces the header with the the value of its target. If the target is null,
 * the header is removed.
 *
 * <p/>
 * ex.
 *
 * <pre>
 * @GET("/")
 * void foo(@Header("Auth-Token") String token, ..);
 * </pre>
 *
 * @author Adrian Cole (adrianc@netflix.com)
 */
@Retention(RUNTIME) @Target(PARAMETER)
public @interface Header {
  String value();
}>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/RestAdapter.java;<<<<<<< MINE
  private final RequestHeaders requestHeaders;
||||||| BASE
  private final Headers headers;
=======
  private final HeaderPairs headers;
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/RestAdapter.java;<<<<<<< MINE
      Executor callbackExecutor, RequestHeaders requestHeaders, Converter converter,
      Profiler profiler, Log log, boolean debug) {
||||||| BASE
      Executor callbackExecutor, Headers headers, Converter converter, Profiler profiler, Log log,
      boolean debug) {
=======
      Executor callbackExecutor, HeaderPairs headers, Converter converter, Profiler profiler,
      Log log, boolean debug) {
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/RestAdapter.java;<<<<<<< MINE
    private RequestHeaders requestHeaders;
||||||| BASE
    private Headers headers;
=======
    private HeaderPairs headers;
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/main/java/retrofit/http/RestAdapter.java;<<<<<<< MINE
    public Builder setRequestHeaders(RequestHeaders requestHeaders) {
      if (requestHeaders == null) throw new NullPointerException("requestHeaders");
      this.requestHeaders = requestHeaders;
||||||| BASE
    public Builder setHeaders(Headers headers) {
      if (headers == null) throw new NullPointerException("headers");
      this.headers = headers;
=======
    public Builder setHeaders(HeaderPairs headers) {
      if (headers == null) throw new NullPointerException("headers");
      this.headers = headers;
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/test/java/retrofit/http/client/UrlConnectionClientTest.java;<<<<<<< MINE
||||||| BASE
import retrofit.http.Header;
=======
import retrofit.http.HeaderPair;
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/test/java/retrofit/http/client/ApacheClientTest.java;<<<<<<< MINE
||||||| BASE
import retrofit.http.Header;
=======
import retrofit.http.HeaderPair;
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/test/java/retrofit/http/RequestBuilderTest.java;<<<<<<< MINE
||||||| BASE
import java.util.Map;
import java.util.Set;
=======
import java.util.Set;
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/test/java/retrofit/http/RequestBuilderTest.java;<<<<<<< MINE
    private String query;
    private final List<String> pathParams = new ArrayList<String>();
    private final List<String> queryParams = new ArrayList<String>();
    private final List<String> pairParams = new ArrayList<String>();
    private final List<String> partParams = new ArrayList<String>();
||||||| BASE
    private Set<String> pathParams;
    private final List<QueryParam> queryParams = new ArrayList<QueryParam>();
    private final List<String> namedParams = new ArrayList<String>();
=======
    private Set<String> pathParams;
    private final List<QueryParam> queryParams = new ArrayList<QueryParam>();
    private final List<String> headerParams = new ArrayList<String>();
    private final List<String> namedParams = new ArrayList<String>();
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/test/java/retrofit/http/RequestBuilderTest.java;<<<<<<< MINE
    private final List<Header> headers = new ArrayList<Header>();
    private int bodyIndex = NO_BODY;
||||||| BASE
    private final List<Header> headers = new ArrayList<Header>();
    private int singleEntityArgumentIndex = NO_SINGLE_ENTITY;
=======
    private final List<HeaderPair> headers = new ArrayList<HeaderPair>();
    private final List<HeaderPair> methodHeaders = new ArrayList<HeaderPair>();
    private int singleEntityArgumentIndex = NO_SINGLE_ENTITY;
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_2ef7ca0_941ae85/rev_2ef7ca0-941ae85/retrofit/src/test/java/retrofit/http/RequestBuilderTest.java;<<<<<<< MINE
    Helper setBody(Object value) {
      addParam(null, null, null, null, value);
      bodyIndex = args.size() - 1;
||||||| BASE
    Helper addSingleEntityParam(Object value) {
      if (singleEntityArgumentIndex != NO_SINGLE_ENTITY) {
        throw new IllegalStateException("Single entity param already added.");
      }
      // Relying on the fact that this is already less one.
      singleEntityArgumentIndex = namedParams.size();
      namedParams.add(null);
      args.add(value);
=======
    Helper addHeaderParam(String name, Object value) {
      if (name == null) {
        throw new IllegalArgumentException("Name can not be null.");
      }
      headerParams.add(name);
      args.add(value);
      return this;
    }

    Helper addSingleEntityParam(Object value) {
      if (singleEntityArgumentIndex != NO_SINGLE_ENTITY) {
        throw new IllegalStateException("Single entity param already added.");
      }
      // Relying on the fact that this is already less one.
      singleEntityArgumentIndex = namedParams.size();
      namedParams.add(null);
      args.add(value);
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_2e05eec_21dbeee/rev_2e05eec-21dbeee/src/java/com/twitter/elephantbird/util/ThriftUtils.java;<<<<<<< MINE
import java.nio.ByteBuffer;
import java.util.Collection;
import java.util.Map;
import java.util.Map.Entry;
||||||| BASE
import java.lang.reflect.Field;
=======
import java.lang.reflect.Field;
import java.lang.reflect.Method;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_2e05eec_21dbeee/rev_2e05eec-21dbeee/src/java/com/twitter/elephantbird/util/ThriftUtils.java;<<<<<<< MINE
      java.lang.reflect.Field field = containingClass.getDeclaredField(fieldName);
      return field.getType();
    } catch (NoSuchFieldException e) {
      throw new RuntimeException("while trying to find " + fieldName + " in "
                                 + containingClass, e);
||||||| BASE
      Field field = containingClass.getDeclaredField(fieldName);
      return field.getType();
    } catch (NoSuchFieldException e) {
      throw new RuntimeException("while trying to find " + fieldName + " in "
                                 + containingClass, e);
=======
      // checking the return type of get method works for union as well.
      String getMethodName = "get"
                             + fieldName.substring(0, 1).toUpperCase()
                             + fieldName.substring(1);
      Method method = containingClass.getDeclaredMethod(getMethodName);
      return method.getReturnType();
    } catch (NoSuchMethodException e) {
      throw new RuntimeException("while trying to find type for " + fieldName +
                                 " in " + containingClass, e);
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_2e05eec_21dbeee/rev_2e05eec-21dbeee/src/java/com/twitter/elephantbird/pig/load/LzoBaseLoadFunc.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_2e05eec_21dbeee/rev_2e05eec-21dbeee/src/java/com/twitter/elephantbird/pig/load/LzoBaseLoadFunc.java;null
/home/taes/taes/projects/retrofit/revisions/rev_d665e56_bf94f53/rev_d665e56-bf94f53/retrofit/src/main/java/retrofit/http/mime/FormUrlEncodedTypedOutput.java;<<<<<<< MINE
/*
 * Copyright (C) 2013 Square, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package retrofit.http.mime;

import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.OutputStream;
import java.net.URLEncoder;

public final class FormUrlEncodedTypedOutput implements TypedOutput {
  final ByteArrayOutputStream content = new ByteArrayOutputStream();

  public void addField(String name, String value) {
    if (name == null) {
      throw new NullPointerException("name");
    }
    if (value == null) {
      throw new NullPointerException("value");
    }
    if (content.size() > 0) {
      content.write('&');
    }
    try {
      name = URLEncoder.encode(name, "UTF-8");
      value = URLEncoder.encode(value, "UTF-8");

      content.write(name.getBytes("UTF-8"));
      content.write('=');
      content.write(value.getBytes("UTF-8"));
    } catch (IOException e) {
      throw new RuntimeException(e);
    }
  }

  @Override public String fileName() {
    return null;
  }

  @Override public String mimeType() {
    return "application/x-www-form-urlencoded; charset=UTF-8";
  }

  @Override public long length() {
    return content.size();
  }

  @Override public void writeTo(OutputStream out) throws IOException {
    out.write(content.toByteArray());
  }
}||||||| BASE
/*
 * Copyright (C) 2013 Square, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package retrofit.http.mime;

import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.OutputStream;
import java.net.URLEncoder;

public final class FormUrlEncodedTypedOutput implements TypedOutput {
  final ByteArrayOutputStream content = new ByteArrayOutputStream();

  public void addPair(String name, String value) {
    if (name == null) {
      throw new NullPointerException("name");
    }
    if (value == null) {
      throw new NullPointerException("value");
    }
    if (content.size() > 0) {
      content.write('&');
    }
    try {
      name = URLEncoder.encode(name, "UTF-8");
      value = URLEncoder.encode(value, "UTF-8");

      content.write(name.getBytes("UTF-8"));
      content.write('=');
      content.write(value.getBytes("UTF-8"));
    } catch (IOException e) {
      throw new RuntimeException(e);
    }
  }

  @Override public String fileName() {
    return null;
  }

  @Override public String mimeType() {
    return "application/x-www-form-urlencoded; charset=UTF-8";
  }

  @Override public long length() {
    return content.size();
  }

  @Override public void writeTo(OutputStream out) throws IOException {
    out.write(content.toByteArray());
  }
}=======
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_d665e56_bf94f53/rev_d665e56-bf94f53/retrofit/src/main/java/retrofit/http/RequestBuilder.java;<<<<<<< MINE
/*
 * Copyright (C) 2012 Square, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package retrofit.http;

import java.io.UnsupportedEncodingException;
import java.net.URLEncoder;
import java.util.ArrayList;
import java.util.List;
import retrofit.http.client.Header;
import retrofit.http.client.Request;
import retrofit.http.mime.FormUrlEncodedTypedOutput;
import retrofit.http.mime.MultipartTypedOutput;
import retrofit.http.mime.TypedOutput;

import static retrofit.http.RestMethodInfo.NO_BODY;

/** Builds HTTP requests from Java method invocations. */
final class RequestBuilder {
  private final Converter converter;

  private RestMethodInfo methodInfo;
  private Object[] args;
  private String apiUrl;
  private List<retrofit.http.client.Header> headers;

  RequestBuilder(Converter converter) {
    this.converter = converter;
  }

  /** Supply cached method metadata info. */
  RequestBuilder methodInfo(RestMethodInfo methodDetails) {
    this.methodInfo = methodDetails;
    return this;
  }

  /** Base API url. */
  RequestBuilder apiUrl(String apiUrl) {
    this.apiUrl = apiUrl;
    return this;
  }

  /** Arguments from method invocation. */
  RequestBuilder args(Object[] args) {
    this.args = args;
    return this;
  }

  /** A list of custom headers. */
  RequestBuilder headers(List<retrofit.http.client.Header> headers) {
    this.headers = headers;
    return this;
  }

  /**
   * Construct a {@link Request} from the supplied information. You <strong>must</strong> call
   * {@link #methodInfo}, {@link #apiUrl}, {@link #args}, and {@link #headers} before invoking this
   * method.
   */
  Request build() throws UnsupportedEncodingException {
    String apiUrl = this.apiUrl;

    StringBuilder url = new StringBuilder(apiUrl);
    if (apiUrl.endsWith("/")) {
      // We require relative paths to start with '/'. Prevent a double-slash.
      url.deleteCharAt(url.length() - 1);
    }

    // Append the method relative URL.
    url.append(buildRelativeUrl());

    // Append query parameters, if needed.
    if (methodInfo.hasQueryParams) {
      boolean first = true;
      String requestQuery = methodInfo.requestQuery;
      if (requestQuery != null) {
        url.append(requestQuery);
        first = false;
      }
      String[] requestQueryName = methodInfo.requestQueryName;
      for (int i = 0; i < requestQueryName.length; i++) {
        String query = requestQueryName[i];
        if (query != null) {
          String value = URLEncoder.encode(String.valueOf(args[i]), "UTF-8");
          url.append(first ? '?' : '&').append(query).append('=').append(value);
          first = false;
        }
      }
    }

    List<retrofit.http.client.Header> headers = new ArrayList<retrofit.http.client.Header>();
    if (this.headers != null) {
      headers.addAll(this.headers);
    }
    List<Header> methodHeaders = methodInfo.headers;
    if (methodHeaders != null) {
      headers.addAll(methodHeaders);
    }
    // RFC 2616: Header names are case-insensitive.
    String[] requestParamHeader = methodInfo.requestParamHeader;
    if (requestParamHeader != null) {
      for (int i = 0; i < requestParamHeader.length; i++) {
        String name = requestParamHeader[i];
        if (name == null) continue;
        Object arg = args[i];
        if (arg != null) {
          headers.add(new retrofit.http.client.Header(name, String.valueOf(arg)));
        }
      }
    }

    return new Request(methodInfo.requestMethod, url.toString(), headers, buildBody());
  }

  /** Create the final relative URL by performing parameter replacement. */
  private String buildRelativeUrl() throws UnsupportedEncodingException {
    String replacedPath = methodInfo.requestUrl;
    String[] requestUrlParam = methodInfo.requestUrlParam;
    for (int i = 0; i < requestUrlParam.length; i++) {
      String param = requestUrlParam[i];
      if (param != null) {
        String value = URLEncoder.encode(String.valueOf(args[i]), "UTF-8");
        replacedPath = replacedPath.replace("{" + param + "}", value);
      }
    }
    return replacedPath;
  }

  /** Create the request body using the method info and invocation arguments. */
  private TypedOutput buildBody() {
    switch (methodInfo.requestType) {
      case SIMPLE: {
        int bodyIndex = methodInfo.bodyIndex;
        if (bodyIndex == NO_BODY) {
          return null;
        }
        Object body = args[bodyIndex];
        if (body instanceof TypedOutput) {
          return (TypedOutput) body;
        } else {
          return converter.toBody(body);
        }
      }

      case FORM_URL_ENCODED: {
        FormUrlEncodedTypedOutput body = new FormUrlEncodedTypedOutput();
        String[] requestFormPair = methodInfo.requestFormPair;
        for (int i = 0; i < requestFormPair.length; i++) {
          String name = requestFormPair[i];
          if (name != null) {
            body.addField(name, String.valueOf(args[i]));
          }
        }
        return body;
      }

      case MULTIPART: {
        MultipartTypedOutput body = new MultipartTypedOutput();
        String[] requestMultipartPart = methodInfo.requestMultipartPart;
        for (int i = 0; i < requestMultipartPart.length; i++) {
          String name = requestMultipartPart[i];
          if (name != null) {
            Object value = args[i];
            if (value instanceof TypedOutput) {
              body.addPart(name, (TypedOutput) value);
            } else {
              body.addPart(name, converter.toBody(value));
            }
          }
        }
        return body;
      }

      default:
        throw new IllegalArgumentException("Unknown request type " + methodInfo.requestType);
    }
  }
}||||||| BASE
/*
 * Copyright (C) 2012 Square, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package retrofit.http;

import java.io.UnsupportedEncodingException;
import java.net.URLEncoder;
import java.util.ArrayList;
import java.util.List;
import retrofit.http.client.Header;
import retrofit.http.client.Request;
import retrofit.http.mime.FormUrlEncodedTypedOutput;
import retrofit.http.mime.MultipartTypedOutput;
import retrofit.http.mime.TypedOutput;

import static retrofit.http.RestMethodInfo.NO_BODY;

/** Builds HTTP requests from Java method invocations. */
final class RequestBuilder {
  private final Converter converter;

  private RestMethodInfo methodInfo;
  private Object[] args;
  private String apiUrl;
  private List<retrofit.http.client.Header> headers;

  RequestBuilder(Converter converter) {
    this.converter = converter;
  }

  /** Supply cached method metadata info. */
  RequestBuilder methodInfo(RestMethodInfo methodDetails) {
    this.methodInfo = methodDetails;
    return this;
  }

  /** Base API url. */
  RequestBuilder apiUrl(String apiUrl) {
    this.apiUrl = apiUrl;
    return this;
  }

  /** Arguments from method invocation. */
  RequestBuilder args(Object[] args) {
    this.args = args;
    return this;
  }

  /** A list of custom headers. */
  RequestBuilder headers(List<retrofit.http.client.Header> headers) {
    this.headers = headers;
    return this;
  }

  /**
   * Construct a {@link Request} from the supplied information. You <strong>must</strong> call
   * {@link #methodInfo}, {@link #apiUrl}, {@link #args}, and {@link #headers} before invoking this
   * method.
   */
  Request build() throws UnsupportedEncodingException {
    String apiUrl = this.apiUrl;

    StringBuilder url = new StringBuilder(apiUrl);
    if (apiUrl.endsWith("/")) {
      // We require relative paths to start with '/'. Prevent a double-slash.
      url.deleteCharAt(url.length() - 1);
    }

    // Append the method relative URL.
    url.append(buildRelativeUrl());

    // Append query parameters, if needed.
    if (methodInfo.hasQueryParams) {
      boolean first = true;
      String requestQuery = methodInfo.requestQuery;
      if (requestQuery != null) {
        url.append(requestQuery);
        first = false;
      }
      String[] requestQueryName = methodInfo.requestQueryName;
      for (int i = 0; i < requestQueryName.length; i++) {
        String query = requestQueryName[i];
        if (query != null) {
          String value = URLEncoder.encode(String.valueOf(args[i]), "UTF-8");
          url.append(first ? '?' : '&').append(query).append('=').append(value);
          first = false;
        }
      }
    }

    List<retrofit.http.client.Header> headers = new ArrayList<retrofit.http.client.Header>();
    if (this.headers != null) {
      headers.addAll(this.headers);
    }
    List<Header> methodHeaders = methodInfo.headers;
    if (methodHeaders != null) {
      headers.addAll(methodHeaders);
    }
    // RFC 2616: Header names are case-insensitive.
    String[] requestParamHeader = methodInfo.requestParamHeader;
    if (requestParamHeader != null) {
      for (int i = 0; i < requestParamHeader.length; i++) {
        String name = requestParamHeader[i];
        if (name == null) continue;
        Object arg = args[i];
        if (arg != null) {
          headers.add(new retrofit.http.client.Header(name, String.valueOf(arg)));
        }
      }
    }

    return new Request(methodInfo.requestMethod, url.toString(), headers, buildBody());
  }

  /** Create the final relative URL by performing parameter replacement. */
  private String buildRelativeUrl() throws UnsupportedEncodingException {
    String replacedPath = methodInfo.requestUrl;
    String[] requestUrlParam = methodInfo.requestUrlParam;
    for (int i = 0; i < requestUrlParam.length; i++) {
      String param = requestUrlParam[i];
      if (param != null) {
        String value = URLEncoder.encode(String.valueOf(args[i]), "UTF-8");
        replacedPath = replacedPath.replace("{" + param + "}", value);
      }
    }
    return replacedPath;
  }

  /** Create the request body using the method info and invocation arguments. */
  private TypedOutput buildBody() {
    switch (methodInfo.requestType) {
      case SIMPLE: {
        int bodyIndex = methodInfo.bodyIndex;
        if (bodyIndex == NO_BODY) {
          return null;
        }
        Object body = args[bodyIndex];
        if (body instanceof TypedOutput) {
          return (TypedOutput) body;
        } else {
          return converter.toBody(body);
        }
      }

      case FORM_URL_ENCODED: {
        FormUrlEncodedTypedOutput body = new FormUrlEncodedTypedOutput();
        String[] requestFormPair = methodInfo.requestFormPair;
        for (int i = 0; i < requestFormPair.length; i++) {
          String name = requestFormPair[i];
          if (name != null) {
            body.addPair(name, String.valueOf(args[i]));
          }
        }
        return body;
      }

      case MULTIPART: {
        MultipartTypedOutput body = new MultipartTypedOutput();
        String[] requestMultipartPart = methodInfo.requestMultipartPart;
        for (int i = 0; i < requestMultipartPart.length; i++) {
          String name = requestMultipartPart[i];
          if (name != null) {
            Object value = args[i];
            if (value instanceof TypedOutput) {
              body.addPart(name, (TypedOutput) value);
            } else {
              body.addPart(name, converter.toBody(value));
            }
          }
        }
        return body;
      }

      default:
        throw new IllegalArgumentException("Unknown request type " + methodInfo.requestType);
    }
  }
}=======
>>>>>>> YOURS
/home/taes/taes/projects/retrofit/revisions/rev_d665e56_bf94f53/rev_d665e56-bf94f53/retrofit/src/test/java/retrofit/http/mime/FormUrlEncodingTypedOutputTest.java;<<<<<<< MINE
// Copyright 2013 Square, Inc.
package retrofit.http.mime;

import java.io.ByteArrayOutputStream;
import org.junit.Test;

import static org.fest.assertions.api.Assertions.assertThat;

public class FormUrlEncodingTypedOutputTest {
  @Test public void urlEncoding() throws Exception {
    FormUrlEncodedTypedOutput fe = new FormUrlEncodedTypedOutput();
    fe.addField("a&b", "c=d");
    fe.addField("space, the", "final frontier");

    ByteArrayOutputStream out = new ByteArrayOutputStream();
    fe.writeTo(out);
    String actual = new String(out.toByteArray(), "UTF-8");
    assertThat(actual).isEqualTo("a%26b=c%3Dd&space%2C+the=final+frontier");
  }

  @Test public void utf8encoding() throws Exception {
    FormUrlEncodedTypedOutput fe = new FormUrlEncodedTypedOutput();
    fe.addField("oo", "q");

    ByteArrayOutputStream out = new ByteArrayOutputStream();
    fe.writeTo(out);
    String actual = new String(out.toByteArray(), "UTF-8");
    assertThat(actual).isEqualTo("oo%C9%9F=%C9%B9%C9%90q");
  }

  @Test public void encodedPairs() throws Exception {
    FormUrlEncodedTypedOutput fe = new FormUrlEncodedTypedOutput();
    fe.addField("sim", "ple");

    ByteArrayOutputStream out1 = new ByteArrayOutputStream();
    fe.writeTo(out1);
    String actual1 = new String(out1.toByteArray(), "UTF-8");
    assertThat(actual1).isEqualTo("sim=ple");

    fe.addField("hey", "there");
    fe.addField("help", "me");

    ByteArrayOutputStream out2 = new ByteArrayOutputStream();
    fe.writeTo(out2);
    String actual2 = new String(out2.toByteArray(), "UTF-8");
    assertThat(actual2).isEqualTo("sim=ple&hey=there&help=me");
  }
}||||||| BASE
// Copyright 2013 Square, Inc.
package retrofit.http.mime;

import java.io.ByteArrayOutputStream;
import org.junit.Test;

import static org.fest.assertions.api.Assertions.assertThat;

public class FormUrlEncodingTypedOutputTest {
  @Test public void urlEncoding() throws Exception {
    FormUrlEncodedTypedOutput fe = new FormUrlEncodedTypedOutput();
    fe.addPair("a&b", "c=d");
    fe.addPair("space, the", "final frontier");

    ByteArrayOutputStream out = new ByteArrayOutputStream();
    fe.writeTo(out);
    String actual = new String(out.toByteArray(), "UTF-8");
    assertThat(actual).isEqualTo("a%26b=c%3Dd&space%2C+the=final+frontier");
  }

  @Test public void utf8encoding() throws Exception {
    FormUrlEncodedTypedOutput fe = new FormUrlEncodedTypedOutput();
    fe.addPair("oo", "q");

    ByteArrayOutputStream out = new ByteArrayOutputStream();
    fe.writeTo(out);
    String actual = new String(out.toByteArray(), "UTF-8");
    assertThat(actual).isEqualTo("oo%C9%9F=%C9%B9%C9%90q");
  }

  @Test public void encodedPairs() throws Exception {
    FormUrlEncodedTypedOutput fe = new FormUrlEncodedTypedOutput();
    fe.addPair("sim", "ple");

    ByteArrayOutputStream out1 = new ByteArrayOutputStream();
    fe.writeTo(out1);
    String actual1 = new String(out1.toByteArray(), "UTF-8");
    assertThat(actual1).isEqualTo("sim=ple");

    fe.addPair("hey", "there");
    fe.addPair("help", "me");

    ByteArrayOutputStream out2 = new ByteArrayOutputStream();
    fe.writeTo(out2);
    String actual2 = new String(out2.toByteArray(), "UTF-8");
    assertThat(actual2).isEqualTo("sim=ple&hey=there&help=me");
  }
}=======
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_a7db88d_d84d866/rev_a7db88d-d84d866/src/java/com/twitter/elephantbird/mapreduce/input/MultiInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_a7db88d_d84d866/rev_a7db88d-d84d866/src/java/com/twitter/elephantbird/mapred/input/DeprecatedInputFormatWrapper.java;<<<<<<< MINE
  public static void setInputFormat(Class<?> realInputFormatClass,
                                    JobConf jobConf) {
||||||| BASE
  public static void setInputFormat(Class<? extends InputFormat<?, ?>> realInputFormatClass,
                                    JobConf jobConf) {
=======
  public static void setInputFormat(Class<?> realInputFormatClass, JobConf jobConf) {
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_879cae7_4bd4aa3/rev_879cae7-4bd4aa3/ribbon/src/main/java/com/netflix/ribbon/http/HttpResourceGroup.java;<<<<<<< MINE
    public static class Builder {
        public Builder(ClientConfigFactory configFactory, RibbonTransportFactory transportFactory) {
        }

        public void withClientOptions(ClientOptions options) {

        }

        public void withHeader() {

        }

        public void withClientConfig(IClientConfig config) {

        }

    }

    public HttpResourceGroup(String groupName) {
        this(groupName, null);
||||||| BASE
    public HttpResourceGroup(String groupName) {
        this(groupName, null);
=======
    public static class Builder {
        private ClientOptions clientOptions;
        private HttpHeaders httpHeaders = new DefaultHttpHeaders();
        private ClientConfigFactory clientConfigFactory;
        private RibbonTransportFactory transportFactory;
        private String name;

        private Builder(String name, ClientConfigFactory configFactory, RibbonTransportFactory transportFactory) {
            this.name = name;
            this.clientConfigFactory = configFactory;
            this.transportFactory = transportFactory;
        }

        public static Builder newBuilder(String groupName, ClientConfigFactory configFactory, RibbonTransportFactory transportFactory) {
            return new Builder(groupName, configFactory, transportFactory);
        }

        public Builder withClientOptions(ClientOptions options) {
            this.clientOptions = options;
            return this;
        }

        public Builder withHeader(String name, String value) {
            httpHeaders.add(name, value);
            return this;
        }

        public HttpResourceGroup build() {
            return new HttpResourceGroup(name, clientOptions, clientConfigFactory, transportFactory, httpHeaders);
        }
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_879cae7_4bd4aa3/rev_879cae7-4bd4aa3/ribbon/src/main/java/com/netflix/ribbon/proxy/MethodTemplateExecutor.java;null
/home/taes/taes/projects/ribbon/revisions/rev_879cae7_4bd4aa3/rev_879cae7-4bd4aa3/ribbon/src/main/java/com/netflix/ribbon/proxy/MethodTemplateExecutor.java;null
/home/taes/taes/projects/ribbon/revisions/rev_879cae7_4bd4aa3/rev_879cae7-4bd4aa3/ribbon/src/main/java/com/netflix/ribbon/proxy/MethodTemplateExecutor.java;<<<<<<< MINE
||||||| BASE
    private final HttpRequestTemplate<?> httpRequestTemplate;
    private final EvCacheProviderPool evCacheProviderPool;
=======
    private final Builder<?> httpRequestTemplateBuilder;
    private final EvCacheProviderPool evCacheProviderPool;
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_879cae7_4bd4aa3/rev_879cae7-4bd4aa3/ribbon/src/main/java/com/netflix/ribbon/proxy/MethodTemplateExecutor.java;<<<<<<< MINE
        httpRequestTemplate = createHttpRequestTemplate();
||||||| BASE
        this.evCacheProviderPool = evCacheProviderPool;
        httpRequestTemplate = createHttpRequestTemplate();
=======
        this.evCacheProviderPool = evCacheProviderPool;
        httpRequestTemplateBuilder = createHttpRequestTemplateBuilder();
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_879cae7_4bd4aa3/rev_879cae7-4bd4aa3/ribbon/src/main/java/com/netflix/ribbon/proxy/MethodTemplateExecutor.java;<<<<<<< MINE
    private HttpRequestTemplate<?> createHttpRequestTemplate() {
        HttpRequestTemplate<?> httpRequestTemplate = createBaseHttpRequestTemplate(httpResourceGroup);
        for (AnnotationProcessor processor: proxyAnnotations.getProcessors()) {
            processor.process(httpRequestTemplate, methodTemplate.getMethod());
        }
        return httpRequestTemplate;
||||||| BASE
    private HttpRequestTemplate<?> createHttpRequestTemplate() {
        HttpRequestTemplate<?> httpRequestTemplate = createBaseHttpRequestTemplate(httpResourceGroup);
        withRequestUriBase(httpRequestTemplate);
        withHttpHeaders(httpRequestTemplate);
        withHystrixHandlers(httpRequestTemplate);
        withCacheProviders(httpRequestTemplate);
        return httpRequestTemplate;
=======
    private Builder<?> createHttpRequestTemplateBuilder() {
        Builder<?> httpRequestTemplateBuilder = createBaseHttpRequestTemplate(httpResourceGroup);
        withRequestUriBase(httpRequestTemplateBuilder);
        withHttpHeaders(httpRequestTemplateBuilder);
        withHystrixHandlers(httpRequestTemplateBuilder);
        withCacheProviders(httpRequestTemplateBuilder);
        return httpRequestTemplateBuilder;
>>>>>>> YOURS
/home/taes/taes/projects/ribbon/revisions/rev_352cb58_00824ee/rev_352cb58-00824ee/ribbon-transport/src/test/java/com/netflix/client/netty/udp/UdpClientTest.java;<<<<<<< MINE
/*
 *
 * Copyright 2014 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.client.netty.udp;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;
import static org.junit.Assert.fail;
import io.netty.channel.socket.DatagramPacket;
import io.reactivex.netty.channel.ObservableConnection;
import io.reactivex.netty.client.RxClient;

import java.nio.charset.Charset;
import java.util.concurrent.TimeoutException;

import org.junit.Rule;
import org.junit.Test;

import rx.Observable;
import rx.functions.Func1;

import com.google.common.collect.Lists;
import com.netflix.client.config.DefaultClientConfigImpl;
import com.netflix.client.netty.MyUDPClient;
import com.netflix.client.netty.RibbonTransport;
import com.netflix.loadbalancer.BaseLoadBalancer;
import com.netflix.loadbalancer.Server;

/**
 * Created by awang on 8/5/14.
 */
public class UdpClientTest {

    @Rule
    public HelloUdpServerExternalResource server = new HelloUdpServerExternalResource();
    
    @Test
    public void testUdpClientWithoutTimeout() throws Exception {
        server.start();
        
        BaseLoadBalancer lb = new BaseLoadBalancer();
        lb.setServersList(Lists.newArrayList(new Server("localhost", server.getServerPort())));
        RxClient<DatagramPacket, DatagramPacket> client = RibbonTransport.newUdpClient(lb,
                DefaultClientConfigImpl.getClientConfigWithDefaultValues());
        
        String response = client.connect().flatMap(new Func1<ObservableConnection<DatagramPacket, DatagramPacket>,
                Observable<DatagramPacket>>() {
            @Override
            public Observable<DatagramPacket> call(ObservableConnection<DatagramPacket, DatagramPacket> connection) {
                connection.writeStringAndFlush("Is there anybody out there?");
                return connection.getInput();
            }
        }).take(1)
                .map(new Func1<DatagramPacket, String>() {
                    @Override
                    public String call(DatagramPacket datagramPacket) {
                        return datagramPacket.content().toString(Charset.defaultCharset());
                    }
                })
                .toBlocking()
                .first();
        assertEquals(HelloUdpServerExternalResource.WELCOME_MSG, response);
    }

    @Test
    public void testUdpClientTimeout() throws Exception {
        server.setTimeout(5000);
        server.start();
        
        BaseLoadBalancer lb = new BaseLoadBalancer();
        Server myServer = new Server("localhost", server.getServerPort());
        lb.setServersList(Lists.newArrayList(myServer));
        MyUDPClient client = new MyUDPClient(lb, DefaultClientConfigImpl.getClientConfigWithDefaultValues());
        try {
            String response = client.submit("Is there anybody out there?")
                    .map(new Func1<DatagramPacket, String>() {
                        @Override
                        public String call(DatagramPacket datagramPacket) {
                            return datagramPacket.content().toString(Charset.defaultCharset());
                        }
                    })
                    .toBlocking()
                    .first();
            fail("Exception expected");
        } catch (Exception e) {
            assertTrue(e.getCause() instanceof TimeoutException);
            assertEquals(1, client.getLoadBalancerContext().getServerStats(myServer).getSuccessiveConnectionFailureCount());
        }
    }

}||||||| BASE
/*
 *
 * Copyright 2014 Netflix, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
package com.netflix.client.netty.udp;

import com.google.common.collect.Lists;
import com.netflix.client.config.DefaultClientConfigImpl;
import com.netflix.client.netty.MyUDPClient;
import com.netflix.client.netty.RibbonTransport;
import com.netflix.loadbalancer.BaseLoadBalancer;
import com.netflix.loadbalancer.Server;
import io.netty.channel.socket.DatagramPacket;
import io.reactivex.netty.channel.ObservableConnection;
import io.reactivex.netty.client.RxClient;
import io.reactivex.netty.protocol.udp.server.UdpServer;
import org.junit.Test;
import rx.Observable;
import rx.functions.Func1;

import java.net.DatagramSocket;
import java.net.SocketException;
import java.nio.charset.Charset;
import java.util.concurrent.TimeoutException;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;
import static org.junit.Assert.fail;

/**
 * Created by awang on 8/5/14.
 */
public class UdpClientTest {

    public int choosePort() throws SocketException {
        DatagramSocket serverSocket = new DatagramSocket();
        int port = serverSocket.getLocalPort();
        serverSocket.close();
        return port;
    }

    @Test
    public void testUdpClientWithoutTimeout() throws Exception {
        int port = choosePort();
        UdpServer<DatagramPacket, DatagramPacket> server = new HelloUdpServer(port, 0).createServer();
        server.start();
        BaseLoadBalancer lb = new BaseLoadBalancer();
        lb.setServersList(Lists.newArrayList(new Server("localhost", port)));
        RxClient<DatagramPacket, DatagramPacket> client = RibbonTransport.newUdpClient(lb,
                DefaultClientConfigImpl.getClientConfigWithDefaultValues());
        try {
            String response = client.connect().flatMap(new Func1<ObservableConnection<DatagramPacket, DatagramPacket>,
                    Observable<DatagramPacket>>() {
                @Override
                public Observable<DatagramPacket> call(ObservableConnection<DatagramPacket, DatagramPacket> connection) {
                    connection.writeStringAndFlush("Is there anybody out there?");
                    return connection.getInput();
                }
            }).take(1)
                    .map(new Func1<DatagramPacket, String>() {
                        @Override
                        public String call(DatagramPacket datagramPacket) {
                            return datagramPacket.content().toString(Charset.defaultCharset());
                        }
                    })
                    .toBlocking()
                    .first();
            assertEquals(HelloUdpServer.WELCOME_MSG, response);
        } finally {
            server.shutdown();
        }
    }

    @Test
    public void testUdpClientTimeout() throws Exception {
        int port = choosePort();
        UdpServer<DatagramPacket, DatagramPacket> server = new HelloUdpServer(port, 5000).createServer();
        server.start();
        BaseLoadBalancer lb = new BaseLoadBalancer();
        Server myServer = new Server("localhost", port);
        lb.setServersList(Lists.newArrayList(myServer));
        MyUDPClient client = new MyUDPClient(lb, DefaultClientConfigImpl.getClientConfigWithDefaultValues());
        try {
            String response = client.submit("Is there anybody out there?")
                    .map(new Func1<DatagramPacket, String>() {
                        @Override
                        public String call(DatagramPacket datagramPacket) {
                            return datagramPacket.content().toString(Charset.defaultCharset());
                        }
                    })
                    .toBlocking()
                    .first();
            fail("Exception expected");
        } catch (Exception e) {
            assertTrue(e.getCause() instanceof TimeoutException);
            assertEquals(1, client.getLoadBalancerContext().getServerStats(myServer).getSuccessiveConnectionFailureCount());
        }
        finally {
            server.shutdown();
        }
    }

}=======
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_9a3bd70_20697f0/rev_9a3bd70-20697f0/src/java/com/twitter/elephantbird/pig/store/SequenceFileStorage.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.store;

import java.io.IOException;

import org.apache.commons.cli.Option;
import org.apache.commons.cli.OptionBuilder;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionCodecFactory;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.OutputFormat;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
import org.apache.pig.LoadFunc;
import org.apache.pig.ResourceSchema;
import org.apache.pig.ResourceSchema.ResourceFieldSchema;
import org.apache.pig.StoreFunc;
import org.apache.pig.StoreFuncInterface;
import org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.util.UDFContext;

import com.google.common.base.Preconditions;
import com.twitter.elephantbird.pig.load.SequenceFileLoader;
import com.twitter.elephantbird.pig.util.GenericWritableConverter;
import com.twitter.elephantbird.pig.util.PigCounterHelper;
import com.twitter.elephantbird.pig.util.WritableConverter;

/**
 * Pig StoreFunc supporting conversion between Pig tuples and arbitrary key-value pairs stored
 * within {@link SequenceFile}s. Example usage:
 *
 * <pre>
 * pairs = LOAD '$INPUT' AS (key: int, value: chararray);
 *
 * STORE pairs INTO '$OUTPUT' USING com.twitter.elephantbird.pig.store.SequenceFileStorage (
 *   '-c com.twitter.elephantbird.pig.util.IntWritableConverter',
 *   '-c com.twitter.elephantbird.pig.util.TextConverter'
 * );
 * </pre>
 *
 * @author Andy Schlaikjer
 */
public class SequenceFileStorage<K extends Writable, V extends Writable> extends
    SequenceFileLoader<K, V> implements StoreFuncInterface {
  /**
   * Failure modes for use with {@link PigCounterHelper} to keep track of runtime error counts.
   *
   * @author Andy Schlaikjer
   */
  public static enum Error {
    /**
     * Null tuple was supplied to {@link SequenceFileStorage#putNext(Tuple)}.
     */
    NULL_TUPLE,
    /**
     * Tuple supplied to {@link SequenceFileStorage#putNext(Tuple)} whose length is not 2.
     */
    TUPLE_SIZE,
    /**
     * Null key was supplied to {@link SequenceFileStorage#putNext(Tuple)} and key type is not
     * {@link NullWritable}.
     */
    NULL_KEY,
    /**
     * Null value was supplied to {@link SequenceFileStorage#putNext(Tuple)} and value type is not
     * {@link NullWritable}.
     */
    NULL_VALUE;
  }

  public static final String TYPE_PARAM = "type";
  private final PigCounterHelper counterHelper = new PigCounterHelper();
  private Class<K> keyClass;
  private Class<V> valueClass;
  private RecordWriter<K, V> writer;

  /**
   * Parses key and value options from argument strings. Available options for both key and value
   * argument strings match those supported by
   * {@link SequenceFileLoader#SequenceFileLoader(String, String)}, as well as:
   * <dl>
   * <dt>-t|--type cls</dt>
   * <dd>{@link Writable} implementation class of data. If Writable class reported by
   * {@link WritableConverter#getWritableClass()} is null (e.g. when using
   * {@link GenericWritableConverter}), this option must be specified.</dd>
   * </dl>
   *
   * @param keyArgs
   * @param valueArgs
   * @throws ParseException
   * @throws IOException
   * @throws ClassNotFoundException
   */
  public SequenceFileStorage(String keyArgs, String valueArgs) throws ParseException, IOException,
      ClassNotFoundException {
    super(keyArgs, valueArgs);
  }

  /**
   * Default constructor which uses default options for key and value.
   *
   * @throws ClassNotFoundException
   * @throws IOException
   * @throws ParseException
   * @see #SequenceFileStorage(String, String)
   */
  public SequenceFileStorage() throws ParseException, IOException, ClassNotFoundException {
    this("", "");
  }

  @Override
  protected Options getOptions() {
    @SuppressWarnings("static-access")
    Option typeOption =
        OptionBuilder
            .withLongOpt(TYPE_PARAM)
            .hasArg()
            .withArgName("cls")
            .withDescription(
                "Writable type of data. Defaults to type returned by getWritableClass()"
                    + " method of configured WritableConverter.").create("t");
    return super.getOptions().addOption(typeOption);
  }

  @Override
  protected void initialize() throws IOException {
    /*
     * Attempt to initialize key, value classes using arguments. If user doesn't specify '--type'
     * arg, then class will be null.
     */
    keyClass = getWritableClass(keyArguments.getOptionValue(TYPE_PARAM));
    valueClass = getWritableClass(valueArguments.getOptionValue(TYPE_PARAM));

    // initialize key, value converters
    keyConverter.initialize(keyClass);
    valueConverter.initialize(valueClass);

    // allow converters to define writable classes if not already defined
    if (keyClass == null) {
      keyClass = keyConverter.getWritableClass();
    }
    if (valueClass == null) {
      valueClass = valueConverter.getWritableClass();
    }
  }

  /**
   * @param writableClassName
   * @return {@code null} if writableClassName is {@code null}, otherwise the Class instance named
   * by writableClassName.
   * @throws IOException
   */
  @SuppressWarnings("unchecked")
  private static <W extends Writable> Class<W> getWritableClass(String writableClassName)
      throws IOException {
    if (writableClassName == null) {
      return null;
    }
    try {
      return PigContext.resolveClassName(writableClassName);
    } catch (Exception e) {
      throw new IOException(String.format("Failed to load Writable class '%s'", writableClassName),
          e);
    }
  }

  @Override
  public void setStoreFuncUDFContextSignature(String signature) {
    this.signature = signature;
  }

  @Override
  public void checkSchema(ResourceSchema schema) throws IOException {
    Preconditions.checkNotNull(schema, "Schema is null");
    ResourceFieldSchema[] fields = schema.getFields();
    Preconditions.checkNotNull(fields, "Schema fields are undefined");
    Preconditions.checkArgument(2 == fields.length,
        "Expecting 2 schema fields but found %s", fields.length);
    keyConverter.checkStoreSchema(fields[0]);
    valueConverter.checkStoreSchema(fields[1]);
  }

  @Override
  public String relToAbsPathForStoreLocation(String location, Path cwd) throws IOException {
    return LoadFunc.getAbsolutePath(location, cwd);
  }

  @SuppressWarnings("unchecked")
  @Override
  public void setStoreLocation(String location, Job job) throws IOException {
    ensureUDFContext(job.getConfiguration());
    verifyWritableClass(keyClass, true, keyConverter);
    verifyWritableClass(valueClass, false, valueConverter);
    job.setOutputKeyClass(keyClass);
    job.setOutputValueClass(valueClass);
    FileOutputFormat.setOutputPath(job, new Path(location));
    if ("true".equals(job.getConfiguration().get("output.compression.enabled"))) {
      FileOutputFormat.setCompressOutput(job, true);
      String codec = job.getConfiguration().get("output.compression.codec");
      FileOutputFormat.setOutputCompressorClass(job,
          PigContext.resolveClassName(codec).asSubclass(CompressionCodec.class));
    } else {
      // This makes it so that storing to a directory ending with ".gz" or ".bz2" works.
      setCompression(new Path(location), job);
    }
  }

  private void ensureUDFContext(Configuration conf) throws IOException {
    if (UDFContext.getUDFContext().isUDFConfEmpty()
        && conf.get("pig.udf.context") != null) {
      MapRedUtil.setupUDFContext(conf);
    }
  }

  /**
   * Tests validity of Writable class, ensures consistent error message for both key and value
   * tests.
   *
   * @param writableClass class being tested.
   * @param isKeyClass {@code true} if testing keyClass, {@code false} otherwise.
   * @param writableConverter associated WritableConverter instance.
   */
  private static <W extends Writable> void verifyWritableClass(Class<W> writableClass,
      boolean isKeyClass, WritableConverter<W> writableConverter) {
    Preconditions.checkNotNull(writableClass, "%s Writable class is undefined;"
        + " WritableConverter of type '%s' does not define default Writable type,"
        + " and no type was specified by user", isKeyClass ? "Key" : "Value", writableConverter
        .getClass().getName());
  }

  /**
   * @param path
   * @param job
   */
  private void setCompression(Path path, Job job) {
    CompressionCodecFactory codecFactory = new CompressionCodecFactory(job.getConfiguration());
    CompressionCodec codec = codecFactory.getCodec(path);
    if (codec != null) {
      FileOutputFormat.setCompressOutput(job, true);
      FileOutputFormat.setOutputCompressorClass(job, codec.getClass());
    } else {
      FileOutputFormat.setCompressOutput(job, false);
    }
  }

  @Override
  public OutputFormat<K, V> getOutputFormat() throws IOException {
    return new SequenceFileOutputFormat<K, V>();
  }

  @Override
  @SuppressWarnings({ "rawtypes", "unchecked" })
  public void prepareToWrite(RecordWriter writer) throws IOException {
    this.writer = writer;
  }

  @Override
  public void putNext(Tuple t) throws IOException {
    // validate input tuple
    if (t == null) {
      counterHelper.incrCounter(Error.NULL_TUPLE, 1);
      return;
    }
    if (t.size() != 2) {
      counterHelper.incrCounter(Error.TUPLE_SIZE, 1);
      return;
    }

    // convert key from pig to writable
    K key = keyConverter.toWritable(t.get(0));
    if (key == null) {
      counterHelper.incrCounter(Error.NULL_KEY, 1);
      return;
    }

    // convert value from pig to writable
    V value = valueConverter.toWritable(t.get(1));
    if (value == null) {
      counterHelper.incrCounter(Error.NULL_VALUE, 1);
      return;
    }

    // write key-value pair
    try {
      writer.write(key, value);
    } catch (InterruptedException e) {
      throw new IOException(e);
    }
  }

  @Override
  public void cleanupOnFailure(String location, Job job) throws IOException {
    StoreFunc.cleanupOnFailureImpl(location, job);
  }
}=======
package com.twitter.elephantbird.pig.store;

import java.io.IOException;

import org.apache.commons.cli.Option;
import org.apache.commons.cli.OptionBuilder;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionCodecFactory;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.OutputFormat;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
import org.apache.pig.LoadFunc;
import org.apache.pig.ResourceSchema;
import org.apache.pig.ResourceSchema.ResourceFieldSchema;
import org.apache.pig.StoreFunc;
import org.apache.pig.StoreFuncInterface;
import org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.util.UDFContext;

import com.google.common.base.Preconditions;
import com.twitter.elephantbird.pig.load.SequenceFileLoader;
import com.twitter.elephantbird.pig.util.GenericWritableConverter;
import com.twitter.elephantbird.pig.util.PigCounterHelper;
import com.twitter.elephantbird.pig.util.WritableConverter;

/**
 * Pig StoreFunc supporting conversion between Pig tuples and arbitrary key-value pairs stored
 * within {@link SequenceFile}s. Example usage:
 *
 * <pre>
 * pairs = LOAD '$INPUT' AS (key: int, value: chararray);
 *
 * STORE pairs INTO '$OUTPUT' USING com.twitter.elephantbird.pig.store.SequenceFileStorage (
 *   '-c com.twitter.elephantbird.pig.util.IntWritableConverter',
 *   '-c com.twitter.elephantbird.pig.util.TextConverter'
 * );
 * </pre>
 *
 * @author Andy Schlaikjer
 */
public class SequenceFileStorage<K extends Writable, V extends Writable> extends
    SequenceFileLoader<K, V> implements StoreFuncInterface {
  /**
   * Failure modes for use with {@link PigCounterHelper} to keep track of runtime error counts.
   *
   * @author Andy Schlaikjer
   */
  public static enum Error {
    /**
     * Null tuple was supplied to {@link SequenceFileStorage#putNext(Tuple)}.
     */
    NULL_TUPLE,
    /**
     * Tuple supplied to {@link SequenceFileStorage#putNext(Tuple)} whose length is not 2.
     */
    TUPLE_SIZE,
    /**
     * Null key was supplied to {@link SequenceFileStorage#putNext(Tuple)} and key type is not
     * {@link NullWritable}.
     */
    NULL_KEY,
    /**
     * Null value was supplied to {@link SequenceFileStorage#putNext(Tuple)} and value type is not
     * {@link NullWritable}.
     */
    NULL_VALUE;
  }

  public static final String TYPE_PARAM = "type";
  private final PigCounterHelper counterHelper = new PigCounterHelper();
  private Class<K> keyClass;
  private Class<V> valueClass;
  private RecordWriter<K, V> writer;

  /**
   * Parses key and value options from argument strings. Available options for both key and value
   * argument strings match those supported by
   * {@link SequenceFileLoader#SequenceFileLoader(String, String)}, as well as:
   * <dl>
   * <dt>-t|--type cls</dt>
   * <dd>{@link Writable} implementation class of data. If Writable class reported by
   * {@link WritableConverter#getWritableClass()} is null (e.g. when using
   * {@link GenericWritableConverter}), this option must be specified.</dd>
   * </dl>
   *
   * @param keyArgs
   * @param valueArgs
   * @throws ParseException
   * @throws IOException
   * @throws ClassNotFoundException
   */
  public SequenceFileStorage(String keyArgs, String valueArgs) throws ParseException, IOException,
      ClassNotFoundException {
    super(keyArgs, valueArgs);
  }

  /**
   * Default constructor which uses default options for key and value.
   *
   * @throws ClassNotFoundException
   * @throws IOException
   * @throws ParseException
   * @see #SequenceFileStorage(String, String)
   */
  public SequenceFileStorage() throws ParseException, IOException, ClassNotFoundException {
    this("", "");
  }

  @Override
  protected Options getKeyValueOptions() {
    @SuppressWarnings("static-access")
    Option typeOption =
        OptionBuilder
            .withLongOpt(TYPE_PARAM)
            .hasArg()
            .withArgName("cls")
            .withDescription(
                "Writable type of data. Defaults to type returned by getWritableClass()"
                    + " method of configured WritableConverter.").create("t");
    return super.getKeyValueOptions().addOption(typeOption);
  }

  @Override
  protected void initialize() throws IOException {
    /*
     * Attempt to initialize key, value classes using arguments. If user doesn't specify '--type'
     * arg, then class will be null.
     */
    keyClass = getWritableClass(keyArguments.getOptionValue(TYPE_PARAM));
    valueClass = getWritableClass(valueArguments.getOptionValue(TYPE_PARAM));

    // initialize key, value converters
    keyConverter.initialize(keyClass);
    valueConverter.initialize(valueClass);

    // allow converters to define writable classes if not already defined
    if (keyClass == null) {
      keyClass = keyConverter.getWritableClass();
    }
    if (valueClass == null) {
      valueClass = valueConverter.getWritableClass();
    }
  }

  /**
   * @param writableClassName
   * @return {@code null} if writableClassName is {@code null}, otherwise the Class instance named
   * by writableClassName.
   * @throws IOException
   */
  @SuppressWarnings("unchecked")
  private static <W extends Writable> Class<W> getWritableClass(String writableClassName)
      throws IOException {
    if (writableClassName == null) {
      return null;
    }
    try {
      return PigContext.resolveClassName(writableClassName);
    } catch (Exception e) {
      throw new IOException(String.format("Failed to load Writable class '%s'", writableClassName),
          e);
    }
  }

  @Override
  public void setStoreFuncUDFContextSignature(String signature) {
    this.signature = signature;
  }

  @Override
  public void checkSchema(ResourceSchema schema) throws IOException {
    Preconditions.checkNotNull(schema, "Schema is null");
    ResourceFieldSchema[] fields = schema.getFields();
    Preconditions.checkNotNull(fields, "Schema fields are undefined");
    Preconditions.checkArgument(2 == fields.length,
        "Expecting 2 schema fields but found %s", fields.length);
    keyConverter.checkStoreSchema(fields[0]);
    valueConverter.checkStoreSchema(fields[1]);
  }

  @Override
  public String relToAbsPathForStoreLocation(String location, Path cwd) throws IOException {
    return LoadFunc.getAbsolutePath(location, cwd);
  }

  @SuppressWarnings("unchecked")
  @Override
  public void setStoreLocation(String location, Job job) throws IOException {
    ensureUDFContext(job.getConfiguration());
    verifyWritableClass(keyClass, true, keyConverter);
    verifyWritableClass(valueClass, false, valueConverter);
    job.setOutputKeyClass(keyClass);
    job.setOutputValueClass(valueClass);
    FileOutputFormat.setOutputPath(job, new Path(location));
    if ("true".equals(job.getConfiguration().get("output.compression.enabled"))) {
      FileOutputFormat.setCompressOutput(job, true);
      String codec = job.getConfiguration().get("output.compression.codec");
      FileOutputFormat.setOutputCompressorClass(job,
          PigContext.resolveClassName(codec).asSubclass(CompressionCodec.class));
    } else {
      // This makes it so that storing to a directory ending with ".gz" or ".bz2" works.
      setCompression(new Path(location), job);
    }
  }

  private void ensureUDFContext(Configuration conf) throws IOException {
    if (UDFContext.getUDFContext().isUDFConfEmpty()
        && conf.get("pig.udf.context") != null) {
      MapRedUtil.setupUDFContext(conf);
    }
  }

  /**
   * Tests validity of Writable class, ensures consistent error message for both key and value
   * tests.
   *
   * @param writableClass class being tested.
   * @param isKeyClass {@code true} if testing keyClass, {@code false} otherwise.
   * @param writableConverter associated WritableConverter instance.
   */
  private static <W extends Writable> void verifyWritableClass(Class<W> writableClass,
      boolean isKeyClass, WritableConverter<W> writableConverter) {
    Preconditions.checkNotNull(writableClass, "%s Writable class is undefined;"
        + " WritableConverter of type '%s' does not define default Writable type,"
        + " and no type was specified by user", isKeyClass ? "Key" : "Value", writableConverter
        .getClass().getName());
  }

  /**
   * @param path
   * @param job
   */
  private void setCompression(Path path, Job job) {
    CompressionCodecFactory codecFactory = new CompressionCodecFactory(job.getConfiguration());
    CompressionCodec codec = codecFactory.getCodec(path);
    if (codec != null) {
      FileOutputFormat.setCompressOutput(job, true);
      FileOutputFormat.setOutputCompressorClass(job, codec.getClass());
    } else {
      FileOutputFormat.setCompressOutput(job, false);
    }
  }

  @Override
  public OutputFormat<K, V> getOutputFormat() throws IOException {
    return new SequenceFileOutputFormat<K, V>();
  }

  @Override
  @SuppressWarnings({ "rawtypes", "unchecked" })
  public void prepareToWrite(RecordWriter writer) throws IOException {
    this.writer = writer;
  }

  @Override
  public void putNext(Tuple t) throws IOException {
    // validate input tuple
    if (t == null) {
      counterHelper.incrCounter(Error.NULL_TUPLE, 1);
      return;
    }
    if (t.size() != 2) {
      counterHelper.incrCounter(Error.TUPLE_SIZE, 1);
      return;
    }

    // convert key from pig to writable
    K key = keyConverter.toWritable(t.get(0));
    if (key == null) {
      counterHelper.incrCounter(Error.NULL_KEY, 1);
      return;
    }

    // convert value from pig to writable
    V value = valueConverter.toWritable(t.get(1));
    if (value == null) {
      counterHelper.incrCounter(Error.NULL_VALUE, 1);
      return;
    }

    // write key-value pair
    try {
      writer.write(key, value);
    } catch (InterruptedException e) {
      throw new IOException(e);
    }
  }

  @Override
  public void cleanupOnFailure(String location, Job job) throws IOException {
    StoreFunc.cleanupOnFailureImpl(location, job);
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_9a3bd70_20697f0/rev_9a3bd70-20697f0/src/java/com/twitter/elephantbird/pig/load/SequenceFileLoader.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.load;

import java.io.EOFException;
import java.io.IOException;
import java.lang.reflect.Constructor;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;
import java.util.Properties;

import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.GnuParser;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Option;
import org.apache.commons.cli.OptionBuilder;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.commons.cli.UnrecognizedOptionException;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.DataInputBuffer;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.mapreduce.InputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.pig.Expression;
import org.apache.pig.FileInputLoadFunc;
import org.apache.pig.LoadCaster;
import org.apache.pig.LoadMetadata;
import org.apache.pig.LoadPushDown;
import org.apache.pig.PigException;
import org.apache.pig.ResourceSchema;
import org.apache.pig.ResourceSchema.ResourceFieldSchema;
import org.apache.pig.ResourceStatistics;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.util.UDFContext;

import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import com.twitter.elephantbird.mapreduce.input.RawSequenceFileInputFormat;
import com.twitter.elephantbird.pig.store.SequenceFileStorage;
import com.twitter.elephantbird.pig.util.PigCounterHelper;
import com.twitter.elephantbird.pig.util.TextConverter;
import com.twitter.elephantbird.pig.util.WritableConverter;

/**
 * Pig LoadFunc supporting conversion from key, value objects stored within {@link SequenceFile}s to
 * Pig objects. Example usage:
 *
 * <pre>
 * pairs = LOAD '$INPUT' USING com.twitter.elephantbird.pig.load.SequenceFileLoader (
 *   '-c com.twitter.elephantbird.pig.util.IntWritableConverter',
 *   '-c com.twitter.elephantbird.pig.util.TextConverter'
 * ) as (
 *   key: int,
 *   value: chararray
 * );
 *
 * -- or, making use of defaults
 * pairs = LOAD '$INPUT' USING com.twitter.elephantbird.pig.load.SequenceFileLoader ();
 * </pre>
 *
 * @author Andy Schlaikjer
 * @see WritableConverter
 */
public class SequenceFileLoader<K extends Writable, V extends Writable> extends FileInputLoadFunc
    implements LoadPushDown, LoadMetadata {
  private static final Logger LOG = LoggerFactory.getLogger(SequenceFileLoader.class);
  public static final String CONVERTER_PARAM = "converter";
  public static final String SKIP_EOF_ERRORS_PARAM = "skipEOFErrors";
  private static final String READ_KEY_PARAM = "_readKey";
  private static final String READ_VALUE_PARAM = "_readValue";
  protected final CommandLine keyArguments;
  protected final CommandLine valueArguments;
  protected final CommandLine otherArguments;
  protected final WritableConverter<K> keyConverter;
  protected final WritableConverter<V> valueConverter;
  private final DataByteArray keyDataByteArray = new DataByteArray();
  private final DataByteArray valueDataByteArray = new DataByteArray();
  private final List<Object> tuple2 = Arrays.asList(new Object(), new Object()), tuple1 = Arrays
      .asList(new Object()), tuple0 = Collections.emptyList();
  private final TupleFactory tupleFactory = TupleFactory.getInstance();
  protected String signature;
  private RecordReader<DataInputBuffer, DataInputBuffer> reader;
  private boolean readKey = true;
  private boolean readValue = true;
  private final PigCounterHelper counterHelper_ = new PigCounterHelper();
  private enum SequenceFileLoaderCounters { EOFError };
  
  /**
   * Parses key and value options from argument strings. Available options for both key and value
   * argument strings include:
   * <dl>
   * <dt>-c|--converter cls</dt>
   * <dd>{@link WritableConverter} implementation class to use for conversion of data. Defaults to
   * {@link TextConverter} for both key and value.</dd>
   * </dl>
   * Any extra arguments found will be treated as String arguments for the WritableConverter
   * constructor. For instance, the argument string {@code "-c MyConverter 123 abc"} specifies
   * WritableConverter class {@code MyConverter} along with two constructor arguments {@code "123"}
   * and {@code "abc"}. This will cause SequenceFileLoader to attempt to invoke the following
   * constructors, in order, to create a new instance of MyConverter:
   * <ol>
   * <li><code>MyConverter(String arg1, String arg2)</code> -- constructor arguments are passed as
   * explicit arguments.</li>
   * <li><code>MyConverter(String[] args)</code> -- constructor arguments are passed within a String
   * array.</li>
   * <li><code>MyConverter(String... args)</code> -- same as above, with var args syntax.</li>
   * <li><code>MyConverter(String argString)</code> -- constructor arguments are joined with space
   * char to create {@code argString}.</li>
   * </ol>
   * If none of these constructors exist, a RuntimeException will be thrown.
   *
   * <p>
   * Note that WritableConverter constructor arguments prefixed by one or more hyphens will be
   * interpreted as options for SequenceFileLoader itself, resulting in an
   * {@link UnrecognizedOptionException}. To avoid this, place these values after a {@code --}
   * (double-hyphen) token:
   *
   * <pre>
   * A = LOAD '$data' USING com.twitter.elephantbird.pig.load.SequenceFileLoader (
   *   '-c ...IntWritableConverter',
   *   '-c ...MyComplexWritableConverter basic options here -- --complex -options here'
   * );
   * </pre>
   *
   * @param keyArgs
   * @param valueArgs
   * @param otherArgs
   * @throws ParseException
   * @throws IOException
   */
  public SequenceFileLoader(String keyArgs, String valueArgs, String otherArgs) throws ParseException, IOException {
    // parse key, value arguments
    Options options = getOptions();
    keyArguments = parseArguments(options, keyArgs);
    valueArguments = parseArguments(options, valueArgs);
    otherArguments = parseArguments(options, otherArgs);
    
    // construct key, value converters
    keyConverter = getWritableConverter(keyArguments);
    valueConverter = getWritableConverter(valueArguments);

    // initialize key, value converters
    initialize();
  }

  /**
   * Default constructor. Defaults used for all options.
   *
   * @throws ParseException
   * @throws IOException
   */
  public SequenceFileLoader() throws ParseException, IOException {
    this("", "");
  }

  /**
   * Constructor without other args (backwards compatible).
   *
   * @throws ParseException
   * @throws IOException
   */
  public SequenceFileLoader(String keyArgs, String valueArgs) throws ParseException, IOException {
    this(keyArgs, valueArgs, "");
  }
  
  /**
   * @return Options instance containing valid key/value options.
   */
  protected Options getOptions() {
    @SuppressWarnings("static-access")
    Option converterOption =
        OptionBuilder
            .withLongOpt(CONVERTER_PARAM)
            .hasArg()
            .withArgName("cls")
            .withDescription(
                "Converter type to use for conversion of data." + "  Defaults to '"
                    + TextConverter.class.getName() + "'.").create("c");
    Option skipEOFOption = 
        OptionBuilder
            .withLongOpt(SKIP_EOF_ERRORS_PARAM)
            .withDescription(
                "Skip EOFExceptions if they occur while reading the data."
            ).create();
    return new Options().addOption(converterOption).addOption(skipEOFOption);
  }

  /**
   * @param args
   * @return CommandLine instance containing options parsed from argument string.
   * @throws ParseException
   */
  private static CommandLine parseArguments(Options options, String args) throws ParseException {
    CommandLine cmdline = null;
    try {
      cmdline = new GnuParser().parse(options, args.split(" "));
    } catch (ParseException e) {
      new HelpFormatter().printHelp(SequenceFileStorage.class.getName() + "(keyArgs, valueArgs)",
          options);
      throw e;
    }
    return cmdline;
  }

  /**
   * @param arguments
   * @return new WritableConverter instance constructed using given arguments.
   */
  @SuppressWarnings("unchecked")
  private static <T extends Writable> WritableConverter<T> getWritableConverter(
      CommandLine arguments) {
    // get remaining non-empty argument strings from commandline
    String[] converterArgs = removeEmptyArgs(arguments.getArgs());
    try {

      // get converter classname
      String converterClassName =
          arguments.getOptionValue(CONVERTER_PARAM, TextConverter.class.getName());

      // get converter class
      Class<WritableConverter<T>> converterClass =
          PigContext.resolveClassName(converterClassName);

      // construct converter instance
      if (converterArgs == null || converterArgs.length == 0) {

        // use default ctor
        return converterClass.newInstance();

      } else {
        try {

          // look up ctor having explicit number of String arguments
          Class<?>[] parameterTypes = new Class<?>[converterArgs.length];
          Arrays.fill(parameterTypes, String.class);
          Constructor<WritableConverter<T>> ctor = converterClass.getConstructor(parameterTypes);
          return ctor.newInstance((Object[]) converterArgs);

        } catch (NoSuchMethodException e) {
          try {

            // look up ctor having single String[] (or String... varargs) argument
            Constructor<WritableConverter<T>> ctor =
                converterClass.getConstructor(new Class<?>[] { String[].class });
            return ctor.newInstance((Object) converterArgs);

          } catch (NoSuchMethodException e2) {

            // look up ctor having single String argument and join args together
            Constructor<WritableConverter<T>> ctor =
                converterClass.getConstructor(new Class<?>[] { String.class });
            StringBuilder sb = new StringBuilder(converterArgs[0]);
            for (int i = 1; i < converterArgs.length; ++i) {
              sb.append(" ").append(converterArgs[i]);
            }
            return ctor.newInstance(sb.toString());

          }
        }
      }
    } catch (Exception e) {
      throw new RuntimeException("Failed to create WritableConverter instance", e);
    }
  }

  /**
   * @param args
   * @return new String[] containing non-empty values from args.
   */
  private static String[] removeEmptyArgs(String[] args) {
    List<String> converterArgsFiltered = Lists.newArrayList();
    for (String arg : args) {
      if (arg == null || arg.isEmpty())
        continue;
      converterArgsFiltered.add(arg);
    }
    return converterArgsFiltered.toArray(new String[0]);
  }

  /**
   * Initializes key, value WritableConverters.
   *
   * @throws IOException
   */
  protected void initialize() throws IOException {
    keyConverter.initialize(null);
    valueConverter.initialize(null);
  }

  @Override
  public InputFormat<DataInputBuffer, DataInputBuffer> getInputFormat() throws IOException {
    return new RawSequenceFileInputFormat();
  }

  @Override
  public LoadCaster getLoadCaster() throws IOException {
    /*
     * We have two LoadCasters--one for the key type, another for the value type. Unfortunately,
     * LoadCaster doesn't allow clients to specify which field it's casting (nor schema of field),
     * so we're out of luck here. No casting supported.
     */
    return null;
  }

  @Override
  public void setUDFContextSignature(String signature) {
    this.signature = signature;
  }

  protected Properties getContextProperties() {
    return UDFContext.getUDFContext().getUDFProperties(getClass(), new String[] { signature });
  }

  protected String getContextProperty(String name, String defaultValue) {
    return getContextProperties().getProperty(signature + name, defaultValue);
  }

  protected void setContextProperty(String name, String value) {
    Preconditions.checkNotNull(name, "Context property name is null");
    getContextProperties().setProperty(signature + name, value);
  }

  @Override
  public List<OperatorSet> getFeatures() {
    return ImmutableList.of(OperatorSet.PROJECTION);
  }

  @Override
  public RequiredFieldResponse pushProjection(RequiredFieldList requiredFieldList)
      throws FrontendException {
    readKey = readValue = false;
    for (RequiredField field : requiredFieldList.getFields()) {
      // TODO fix Pig's handling of RequiredField type initialization
      int i = field.getIndex();
      switch (i) {
        case 0:
          readKey = true;
          // TODO(Andy Schlaikjer) enable schema checking here?
          // try {
          // keyConverter.checkLoadSchema(ResourceSchemaUtil.createResourceFieldSchema(field));
          // } catch (IOException e) {
          // throw new FrontendException("Key schema check failed", e);
          // }
          break;
        case 1:
          readValue = true;
          // TODO(Andy Schlaikjer) enable schema checking here?
          // try {
          // valueConverter.checkLoadSchema(ResourceSchemaUtil.createResourceFieldSchema(field));
          // } catch (IOException e) {
          // throw new FrontendException("Value schema check failed", e);
          // }
          break;
        default:
          // TODO fix Pig's silent ignorance of FrontendExceptions thrown from here
          throw new FrontendException("Expected field indices in [0, 1] but found index " + i);
      }
    }
    setContextProperty(READ_KEY_PARAM, Boolean.toString(readKey));
    setContextProperty(READ_VALUE_PARAM, Boolean.toString(readValue));
    return new RequiredFieldResponse(true);
  }

  @Override
  public ResourceSchema getSchema(String location, Job job) throws IOException {
    // determine key field schema
    ResourceFieldSchema keySchema = keyConverter.getLoadSchema();
    if (keySchema == null) {
      return null;
    }
    keySchema.setName("key");

    // determine value field schema
    ResourceFieldSchema valueSchema = valueConverter.getLoadSchema();
    if (valueSchema == null) {
      return null;
    }
    valueSchema.setName("value");

    // return tuple schema
    ResourceSchema resourceSchema = new ResourceSchema();
    resourceSchema.setFields(new ResourceFieldSchema[] { keySchema, valueSchema });
    return resourceSchema;
  }

  /**
   * This implementation returns {@code null}.
   *
   * @see org.apache.pig.LoadMetadata#getStatistics(java.lang.String,
   *      org.apache.hadoop.mapreduce.Job)
   */
  @Override
  public ResourceStatistics getStatistics(String location, Job job) throws IOException {
    return null;
  }

  /**
   * This implementation returns {@code null}.
   *
   * @see org.apache.pig.LoadMetadata#getPartitionKeys(java.lang.String,
   *      org.apache.hadoop.mapreduce.Job)
   */
  @Override
  public String[] getPartitionKeys(String location, Job job) throws IOException {
    return null;
  }

  /**
   * This implementation throws {@link UnsupportedOperationException}.
   *
   * @see org.apache.pig.LoadMetadata#setPartitionFilter(org.apache.pig.Expression)
   * @throws UnsupportedOperationException
   */
  @Override
  public void setPartitionFilter(Expression expression) throws IOException {
    throw new UnsupportedOperationException();
  }

  @Override
  public void setLocation(String location, Job job) throws IOException {
    Preconditions.checkNotNull(location, "Location is null");
    Preconditions.checkNotNull(job, "Job is null");
    Path inputPath = new Path(location);
    FileInputFormat.setInputPaths(job, inputPath);
    readKey = Boolean.parseBoolean(getContextProperty(READ_KEY_PARAM, "true"));
    readValue = Boolean.parseBoolean(getContextProperty(READ_VALUE_PARAM, "true"));
  }

  @Override
  @SuppressWarnings({ "rawtypes", "unchecked" })
  public void prepareToRead(RecordReader reader, PigSplit split) throws IOException {
    this.reader = reader;
  }

  @Override
  public Tuple getNext() throws IOException {
    try {
      if (!reader.nextKeyValue())
        return null;
      List<Object> tuple = tuple0;
      if (readKey) {
        if (readValue) {
          tuple = tuple2;
          tuple.set(0, getCurrentKeyObject());
          tuple.set(1, getCurrentValueObject());
        } else {
          tuple = tuple1;
          tuple.set(0, getCurrentKeyObject());
        }
      } else if (readValue) {
        tuple = tuple1;
        tuple.set(0, getCurrentValueObject());
      }
      return tupleFactory.newTupleNoCopy(tuple);
    } catch (EOFException e) {
      if (otherArguments.hasOption(SKIP_EOF_ERRORS_PARAM)) {
        // prefer to keep reading rather than causing the job to fail when it hits a file still 
        // being written
        LOG.warn("EOFException encountered while reading input", e);
        counterHelper_.incrCounter(SequenceFileLoaderCounters.EOFError, 1L);
      } else {
        throw e;
      }
    } catch (InterruptedException e) {
      throw new ExecException("Error while reading input", 6018, PigException.REMOTE_ENVIRONMENT, e);
    }
    
    return null;
  }

  private Object getCurrentKeyObject() throws IOException, InterruptedException {
    DataInputBuffer ibuf = reader.getCurrentKey();
    keyDataByteArray.set(Arrays.copyOf(ibuf.getData(), ibuf.getLength()));
    return keyConverter.bytesToObject(keyDataByteArray);
  }

  private Object getCurrentValueObject() throws IOException, InterruptedException {
    DataInputBuffer ibuf = reader.getCurrentValue();
    valueDataByteArray.set(Arrays.copyOf(ibuf.getData(), ibuf.getLength()));
    return valueConverter.bytesToObject(valueDataByteArray);
  }
}=======
package com.twitter.elephantbird.pig.load;

import java.io.EOFException;
import java.io.IOException;
import java.lang.reflect.Constructor;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;
import java.util.Properties;

import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.GnuParser;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Option;
import org.apache.commons.cli.OptionBuilder;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.commons.cli.UnrecognizedOptionException;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.DataInputBuffer;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.mapreduce.InputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.pig.Expression;
import org.apache.pig.FileInputLoadFunc;
import org.apache.pig.LoadCaster;
import org.apache.pig.LoadMetadata;
import org.apache.pig.LoadPushDown;
import org.apache.pig.PigException;
import org.apache.pig.ResourceSchema;
import org.apache.pig.ResourceSchema.ResourceFieldSchema;
import org.apache.pig.ResourceStatistics;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.util.UDFContext;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import com.twitter.elephantbird.mapreduce.input.RawSequenceFileInputFormat;
import com.twitter.elephantbird.pig.store.SequenceFileStorage;
import com.twitter.elephantbird.pig.util.PigCounterHelper;
import com.twitter.elephantbird.pig.util.TextConverter;
import com.twitter.elephantbird.pig.util.WritableConverter;

/**
 * Pig LoadFunc supporting conversion from key, value objects stored within {@link SequenceFile}s to
 * Pig objects. Example usage:
 *
 * <pre>
 * pairs = LOAD '$INPUT' USING com.twitter.elephantbird.pig.load.SequenceFileLoader (
 *   '-c com.twitter.elephantbird.pig.util.IntWritableConverter',
 *   '-c com.twitter.elephantbird.pig.util.TextConverter'
 * ) as (
 *   key: int,
 *   value: chararray
 * );
 *
 * -- or, making use of defaults
 * pairs = LOAD '$INPUT' USING com.twitter.elephantbird.pig.load.SequenceFileLoader ();
 * </pre>
 *
 * @author Andy Schlaikjer
 * @see WritableConverter
 */
public class SequenceFileLoader<K extends Writable, V extends Writable> extends FileInputLoadFunc
    implements LoadPushDown, LoadMetadata {
  /**
   * Counter enum for error conditions.
   */
  public static enum Counter {
    /**
     * {@link EOFException}s encountered while reading input.
     */
    EOFException
  };

  private static final Logger LOG = LoggerFactory.getLogger(SequenceFileLoader.class);
  public static final String CONVERTER_PARAM = "converter";
  public static final String SKIP_EOF_ERRORS_PARAM = "skipEOFErrors";
  private static final String READ_KEY_PARAM = "_readKey";
  private static final String READ_VALUE_PARAM = "_readValue";
  protected final CommandLine keyArguments;
  protected final CommandLine valueArguments;
  protected final CommandLine otherArguments;
  protected final WritableConverter<K> keyConverter;
  protected final WritableConverter<V> valueConverter;
  private final DataByteArray keyDataByteArray = new DataByteArray();
  private final DataByteArray valueDataByteArray = new DataByteArray();
  private final List<Object> tuple2 = Arrays.asList(new Object(), new Object()), tuple1 = Arrays
      .asList(new Object()), tuple0 = Collections.emptyList();
  private final TupleFactory tupleFactory = TupleFactory.getInstance();
  protected String signature;
  private RecordReader<DataInputBuffer, DataInputBuffer> reader;
  private boolean readKey = true;
  private boolean readValue = true;
  private final PigCounterHelper counterHelper = new PigCounterHelper();

  /**
   * Parses key and value options from argument strings. Available options for key and value
   * argument strings include:
   *
   * <dl>
   * <dt>-c|--converter cls</dt>
   * <dd>{@link WritableConverter} implementation class to use for conversion of data. Defaults to
   * {@link TextConverter} for both key and value.</dd>
   * </dl>
   *
   * Any extra arguments found will be treated as String arguments for the WritableConverter
   * constructor. For instance, the argument string {@code "-c MyConverter 123 abc"} specifies
   * WritableConverter class {@code MyConverter} along with two constructor arguments {@code "123"}
   * and {@code "abc"}. This will cause SequenceFileLoader to attempt to invoke the following
   * constructors, in order, to create a new instance of MyConverter:
   *
   * <ol>
   * <li><code>MyConverter(String arg1, String arg2)</code> -- constructor arguments are passed as
   * explicit arguments.</li>
   * <li><code>MyConverter(String[] args)</code> -- constructor arguments are passed within a String
   * array.</li>
   * <li><code>MyConverter(String... args)</code> -- same as above, with var args syntax.</li>
   * <li><code>MyConverter(String argString)</code> -- constructor arguments are joined with space
   * char to create {@code argString}.</li>
   * </ol>
   *
   * If none of these constructors exist, a RuntimeException will be thrown.
   *
   * <p>
   * Note that WritableConverter constructor arguments prefixed by one or more hyphens will be
   * interpreted as options for SequenceFileLoader itself, resulting in an
   * {@link UnrecognizedOptionException}. To avoid this, place these values after a {@code --}
   * (double-hyphen) token:
   *
   * <pre>
   * A = LOAD '$data' USING com.twitter.elephantbird.pig.load.SequenceFileLoader (
   *   '-c ...IntWritableConverter',
   *   '-c ...MyComplexWritableConverter basic options here -- --complex -options here'
   * );
   * </pre>
   *
   * @param keyArgs argument string containing key options.
   * @param valueArgs argument string containing value options.
   * @param otherArgs argument string containing other options.
   * @throws ParseException
   * @throws IOException
   */
  public SequenceFileLoader(String keyArgs, String valueArgs, String otherArgs)
      throws ParseException, IOException {
    // parse key, value arguments
    Options keyValueOptions = getKeyValueOptions();
    Options otherOptions = getOtherOptions();
    keyArguments = parseArguments(keyValueOptions, keyArgs);
    valueArguments = parseArguments(keyValueOptions, valueArgs);
    otherArguments = parseArguments(otherOptions, otherArgs);

    // construct key, value converters
    keyConverter = getWritableConverter(keyArguments);
    valueConverter = getWritableConverter(valueArguments);

    // initialize key, value converters
    initialize();
  }

  /**
   * Default constructor. Defaults used for all options.
   *
   * @throws ParseException
   * @throws IOException
   */
  public SequenceFileLoader() throws ParseException, IOException {
    this("", "");
  }

  /**
   * Constructor without other arguments (backwards compatible).
   *
   * @throws ParseException
   * @throws IOException
   */
  public SequenceFileLoader(String keyArgs, String valueArgs) throws ParseException, IOException {
    this(keyArgs, valueArgs, "");
  }

  /**
   * @return Options instance containing valid key/value options.
   */
  protected Options getKeyValueOptions() {
    @SuppressWarnings("static-access")
    Option converterOption =
        OptionBuilder
            .withLongOpt(CONVERTER_PARAM)
            .hasArg()
            .withArgName("cls")
            .withDescription(
                String.format("Converter type to use for conversion of data. Defaults to '%s'.",
                    TextConverter.class.getName())).create("c");
    return new Options().addOption(converterOption);
  }

  /**
   * @return Options instance containing valid global options.
   */
  protected Options getOtherOptions() {
    @SuppressWarnings("static-access")
    Option skipEOFOption =
        OptionBuilder
            .withLongOpt(SKIP_EOF_ERRORS_PARAM)
            .withDescription(
                "Skip EOFExceptions if they occur while reading data." +
                    " Useful for reading sequence files while they are being created."
            ).create();
    return new Options().addOption(skipEOFOption);
  }

  /**
   * @param args
   * @return CommandLine instance containing options parsed from argument string.
   * @throws ParseException
   */
  private static CommandLine parseArguments(Options options, String args) throws ParseException {
    CommandLine cmdline = null;
    try {
      cmdline = new GnuParser().parse(options, args.split(" "));
    } catch (ParseException e) {
      new HelpFormatter().printHelp(SequenceFileStorage.class.getName() + "(keyArgs, valueArgs)",
          options);
      throw e;
    }
    return cmdline;
  }

  /**
   * @param arguments
   * @return new WritableConverter instance constructed using given arguments.
   */
  @SuppressWarnings("unchecked")
  private static <T extends Writable> WritableConverter<T> getWritableConverter(
      CommandLine arguments) {
    // get remaining non-empty argument strings from commandline
    String[] converterArgs = removeEmptyArgs(arguments.getArgs());
    try {

      // get converter classname
      String converterClassName =
          arguments.getOptionValue(CONVERTER_PARAM, TextConverter.class.getName());

      // get converter class
      Class<WritableConverter<T>> converterClass =
          PigContext.resolveClassName(converterClassName);

      // construct converter instance
      if (converterArgs == null || converterArgs.length == 0) {

        // use default ctor
        return converterClass.newInstance();

      } else {
        try {

          // look up ctor having explicit number of String arguments
          Class<?>[] parameterTypes = new Class<?>[converterArgs.length];
          Arrays.fill(parameterTypes, String.class);
          Constructor<WritableConverter<T>> ctor = converterClass.getConstructor(parameterTypes);
          return ctor.newInstance((Object[]) converterArgs);

        } catch (NoSuchMethodException e) {
          try {

            // look up ctor having single String[] (or String... varargs) argument
            Constructor<WritableConverter<T>> ctor =
                converterClass.getConstructor(new Class<?>[] { String[].class });
            return ctor.newInstance((Object) converterArgs);

          } catch (NoSuchMethodException e2) {

            // look up ctor having single String argument and join args together
            Constructor<WritableConverter<T>> ctor =
                converterClass.getConstructor(new Class<?>[] { String.class });
            StringBuilder sb = new StringBuilder(converterArgs[0]);
            for (int i = 1; i < converterArgs.length; ++i) {
              sb.append(" ").append(converterArgs[i]);
            }
            return ctor.newInstance(sb.toString());

          }
        }
      }
    } catch (Exception e) {
      throw new RuntimeException("Failed to create WritableConverter instance", e);
    }
  }

  /**
   * @param args
   * @return new String[] containing non-empty values from args.
   */
  private static String[] removeEmptyArgs(String[] args) {
    List<String> converterArgsFiltered = Lists.newArrayList();
    for (String arg : args) {
      if (arg == null || arg.isEmpty())
        continue;
      converterArgsFiltered.add(arg);
    }
    return converterArgsFiltered.toArray(new String[0]);
  }

  /**
   * Initializes key, value WritableConverters.
   *
   * @throws IOException
   */
  protected void initialize() throws IOException {
    keyConverter.initialize(null);
    valueConverter.initialize(null);
  }

  @Override
  public InputFormat<DataInputBuffer, DataInputBuffer> getInputFormat() throws IOException {
    return new RawSequenceFileInputFormat();
  }

  @Override
  public LoadCaster getLoadCaster() throws IOException {
    /*
     * We have two LoadCasters--one for the key type, another for the value type. Unfortunately,
     * LoadCaster doesn't allow clients to specify which field it's casting (nor schema of field),
     * so we're out of luck here. No casting supported.
     */
    return null;
  }

  @Override
  public void setUDFContextSignature(String signature) {
    this.signature = signature;
  }

  protected Properties getContextProperties() {
    return UDFContext.getUDFContext().getUDFProperties(getClass(), new String[] { signature });
  }

  protected String getContextProperty(String name, String defaultValue) {
    return getContextProperties().getProperty(signature + name, defaultValue);
  }

  protected void setContextProperty(String name, String value) {
    Preconditions.checkNotNull(name, "Context property name is null");
    getContextProperties().setProperty(signature + name, value);
  }

  @Override
  public List<OperatorSet> getFeatures() {
    return ImmutableList.of(OperatorSet.PROJECTION);
  }

  @Override
  public RequiredFieldResponse pushProjection(RequiredFieldList requiredFieldList)
      throws FrontendException {
    readKey = readValue = false;
    for (RequiredField field : requiredFieldList.getFields()) {
      // TODO fix Pig's handling of RequiredField type initialization
      int i = field.getIndex();
      switch (i) {
        case 0:
          readKey = true;
          // TODO(Andy Schlaikjer) enable schema checking here?
          // try {
          // keyConverter.checkLoadSchema(ResourceSchemaUtil.createResourceFieldSchema(field));
          // } catch (IOException e) {
          // throw new FrontendException("Key schema check failed", e);
          // }
          break;
        case 1:
          readValue = true;
          // TODO(Andy Schlaikjer) enable schema checking here?
          // try {
          // valueConverter.checkLoadSchema(ResourceSchemaUtil.createResourceFieldSchema(field));
          // } catch (IOException e) {
          // throw new FrontendException("Value schema check failed", e);
          // }
          break;
        default:
          // TODO fix Pig's silent ignorance of FrontendExceptions thrown from here
          throw new FrontendException("Expected field indices in [0, 1] but found index " + i);
      }
    }
    setContextProperty(READ_KEY_PARAM, Boolean.toString(readKey));
    setContextProperty(READ_VALUE_PARAM, Boolean.toString(readValue));
    return new RequiredFieldResponse(true);
  }

  @Override
  public ResourceSchema getSchema(String location, Job job) throws IOException {
    // determine key field schema
    ResourceFieldSchema keySchema = keyConverter.getLoadSchema();
    if (keySchema == null) {
      return null;
    }
    keySchema.setName("key");

    // determine value field schema
    ResourceFieldSchema valueSchema = valueConverter.getLoadSchema();
    if (valueSchema == null) {
      return null;
    }
    valueSchema.setName("value");

    // return tuple schema
    ResourceSchema resourceSchema = new ResourceSchema();
    resourceSchema.setFields(new ResourceFieldSchema[] { keySchema, valueSchema });
    return resourceSchema;
  }

  /**
   * This implementation returns {@code null}.
   *
   * @see org.apache.pig.LoadMetadata#getStatistics(java.lang.String,
   * org.apache.hadoop.mapreduce.Job)
   */
  @Override
  public ResourceStatistics getStatistics(String location, Job job) throws IOException {
    return null;
  }

  /**
   * This implementation returns {@code null}.
   *
   * @see org.apache.pig.LoadMetadata#getPartitionKeys(java.lang.String,
   * org.apache.hadoop.mapreduce.Job)
   */
  @Override
  public String[] getPartitionKeys(String location, Job job) throws IOException {
    return null;
  }

  /**
   * This implementation throws {@link UnsupportedOperationException}.
   *
   * @see org.apache.pig.LoadMetadata#setPartitionFilter(org.apache.pig.Expression)
   * @throws UnsupportedOperationException
   */
  @Override
  public void setPartitionFilter(Expression expression) throws IOException {
    throw new UnsupportedOperationException();
  }

  @Override
  public void setLocation(String location, Job job) throws IOException {
    Preconditions.checkNotNull(location, "Location is null");
    Preconditions.checkNotNull(job, "Job is null");
    Path inputPath = new Path(location);
    FileInputFormat.setInputPaths(job, inputPath);
    readKey = Boolean.parseBoolean(getContextProperty(READ_KEY_PARAM, "true"));
    readValue = Boolean.parseBoolean(getContextProperty(READ_VALUE_PARAM, "true"));
  }

  @Override
  @SuppressWarnings({ "rawtypes", "unchecked" })
  public void prepareToRead(RecordReader reader, PigSplit split) throws IOException {
    this.reader = reader;
  }

  @Override
  public Tuple getNext() throws IOException {
    try {
      if (!reader.nextKeyValue())
        return null;
      List<Object> tuple = tuple0;
      if (readKey) {
        if (readValue) {
          tuple = tuple2;
          tuple.set(0, getCurrentKeyObject());
          tuple.set(1, getCurrentValueObject());
        } else {
          tuple = tuple1;
          tuple.set(0, getCurrentKeyObject());
        }
      } else if (readValue) {
        tuple = tuple1;
        tuple.set(0, getCurrentValueObject());
      }
      return tupleFactory.newTupleNoCopy(tuple);
    } catch (EOFException e) {
      if (!otherArguments.hasOption(SKIP_EOF_ERRORS_PARAM)) {
        throw e;
      }

      /*
       * Prefer to keep reading rather than causing the job to fail when it hits a file still being
       * written.
       */
      LOG.warn("EOFException encountered while reading input", e);
      counterHelper.incrCounter(Counter.EOFException, 1L);
    } catch (InterruptedException e) {
      throw new ExecException("Error while reading input", 6018, PigException.REMOTE_ENVIRONMENT, e);
    }

    return null;
  }

  private Object getCurrentKeyObject() throws IOException, InterruptedException {
    DataInputBuffer ibuf = reader.getCurrentKey();
    keyDataByteArray.set(Arrays.copyOf(ibuf.getData(), ibuf.getLength()));
    return keyConverter.bytesToObject(keyDataByteArray);
  }

  private Object getCurrentValueObject() throws IOException, InterruptedException {
    DataInputBuffer ibuf = reader.getCurrentValue();
    valueDataByteArray.set(Arrays.copyOf(ibuf.getData(), ibuf.getLength()));
    return valueConverter.bytesToObject(valueDataByteArray);
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_2ad1a24_abcabcd/rev_2ad1a24-abcabcd/src/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;<<<<<<< MINE
||||||| BASE
package com.twitter.elephantbird.pig.util;

import java.nio.ByteBuffer;
import java.util.Arrays;
import java.util.Collection;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;

import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.pig.LoadFunc;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.apache.thrift.TBase;
import org.apache.thrift.protocol.TType;

import com.google.common.collect.Lists;
import com.twitter.elephantbird.pig.load.ThriftPigLoader;
import com.twitter.elephantbird.thrift.TStructDescriptor;
import com.twitter.elephantbird.thrift.TStructDescriptor.Field;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;

/**
 * <li> converts a Thrift struct to a Pig tuple
 * <li> utilities to provide schema for Pig loaders and Pig scripts
 */
public class ThriftToPig<M extends TBase<?, ?>> {

  public static final Logger LOG = LogManager.getLogger(ThriftToPig.class);

  private static BagFactory bagFactory = BagFactory.getInstance();
  private static TupleFactory tupleFactory  = TupleFactory.getInstance();

  private TStructDescriptor structDesc;

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(Class<M> tClass) {
    return new ThriftToPig<M>(tClass);
  }

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(TypeRef<M> typeRef) {
    return new ThriftToPig<M>(typeRef.getRawClass());
  }

  public ThriftToPig(Class<M> tClass) {
    structDesc = TStructDescriptor.getInstance(tClass);
  }

  public TStructDescriptor getTStructDescriptor() {
    return structDesc;
  }

  /**
   * Converts a thrift object to Pig tuple.
   * All the fields are deserialized.
   * It might be better to use getLazyTuple() if not all fields
   * are required.
   */
  public Tuple getPigTuple(M thriftObj) {
    return toTuple(structDesc, thriftObj);
  }

  /**
   * Similar to {@link #getPigTuple(TBase)}. This delays
   * serialization of tuple contents until they are requested.
   * @param thriftObj
   * @return
   */
  public Tuple getLazyTuple(M thriftObj) {
    return new LazyTuple(structDesc, thriftObj);
  }

  @SuppressWarnings("unchecked")
  private static <T extends TBase>Tuple toTuple(TStructDescriptor tDesc, T tObj) {
    int size = tDesc.getFields().size();
    Tuple tuple = tupleFactory.newTuple(size);
    for (int i=0; i<size; i++) {
      Field field = tDesc.getFieldAt(i);
      Object value = tDesc.getFieldValue(i, tObj);
      try {
        tuple.set(i, toPigObject(field, value, false));
      } catch (ExecException e) { // not expected
        throw new RuntimeException(e);
      }
    }
    return tuple;
  }

  @SuppressWarnings("unchecked")
  public static Object toPigObject(Field field, Object value, boolean lazy) {
    if (value == null) {
      return null;
    }

    switch (field.getType()) {
    case TType.BOOL:
      return Integer.valueOf((Boolean)value ? 1 : 0);
    case TType.BYTE :
      return Integer.valueOf((Byte)value);
    case TType.I16 :
      return Integer.valueOf((Short)value);
    case TType.STRING:
      return stringTypeToPig(value);
    case TType.STRUCT:
      if (lazy) {
        return new LazyTuple(field.gettStructDescriptor(), (TBase<?, ?>)value);
      } else {
        return toTuple(field.gettStructDescriptor(), (TBase<?, ?>)value);
      }
    case TType.MAP:
      return toPigMap(field, (Map<Object, Object>)value, lazy);
    case TType.SET:
      return toPigBag(field.getSetElemField(), (Collection<Object>)value, lazy);
    case TType.LIST:
      return toPigBag(field.getListElemField(), (Collection<Object>)value, lazy);
    case TType.ENUM:
      return value.toString();
    default:
      // standard types : I32, I64, DOUBLE, etc.
      return value;
    }
  }

  /**
   * TType.STRING is a mess in Thrift. It could be byte[], ByteBuffer,
   * or even a String!.
   */
  private static Object stringTypeToPig(Object value) {
    if (value instanceof String) {
      return value;
    }
    if (value instanceof byte[]) {
      byte[] buf = (byte[])value;
      return new DataByteArray(Arrays.copyOf(buf, buf.length));
    }
    if (value instanceof ByteBuffer) {
      ByteBuffer bin = (ByteBuffer)value;
      byte[] buf = new byte[bin.remaining()];
      bin.mark();
      bin.get(buf);
      bin.reset();
      return new DataByteArray(buf);
    }
    return null;
  }

  private static Map<String, Object> toPigMap(Field field,
                                              Map<Object, Object> map,
                                              boolean lazy) {
    // PIG map's key always a String. just use toString() and hope
    // things would work out ok.
    HashMap<String, Object> out = new HashMap<String, Object>(map.size());
    Field valueField = field.getMapValueField();
    for(Entry<Object, Object> e : map.entrySet()) {
      Object prev = out.put(e.getKey().toString(),
                            toPigObject(valueField, e.getValue(), lazy));
      if (prev != null) {
        String msg = "Duplicate keys while converting to String while "
          + " processing map " + field.getName() + " (key type : "
          + field.getMapKeyField().getType() + " value type : "
          + field.getMapValueField().getType() + ")";
        LOG.warn(msg);
        throw new RuntimeException(msg);
      }
    }
    return out;
  }

  private static DataBag toPigBag(Field field,
                                  Collection<Object> values,
                                  boolean lazy) {
    List<Tuple> tuples = Lists.newArrayListWithExpectedSize(values.size());
    for(Object value : values) {
      Object pValue = toPigObject(field, value, lazy);
      if (pValue instanceof Tuple) { // DataBag should contain Tuples
        tuples.add((Tuple)pValue);
      } else {
        tuples.add(tupleFactory.newTuple(pValue));
      }
    }
    return bagFactory.newDefaultBag(tuples);
  }

  @SuppressWarnings("serial")
  /**
   * Delays serialization of Thrift fields until they are requested.
   */
  private static class LazyTuple extends AbstractLazyTuple {
    /* NOTE : This is only a partial optimization. The other part
     * is to avoid deserialization of the Thrift fields from the
     * binary buffer.
     *
     * Currently TDeserializer allows deserializing just one field,
     * psuedo-skipping over the fields before it.
     * But if we are going deserialize 5 fields out of 20, we will be
     * skipping over same set of fields multiple times. OTOH this might
     * still be better than a full deserialization.
     *
     * We need to write our own version of TBinaryProtocol that truly skips.
     * Even TDeserializer 'skips'/ignores only after deserializing fields.
     * (e.g. Strings, Integers, buffers etc).
     */
    private TBase<?, ?> tObject;
    private TStructDescriptor desc;

    LazyTuple(TStructDescriptor desc, TBase<?, ?> tObject) {
      initRealTuple(desc.getFields().size());
      this.tObject = tObject;
      this.desc = desc;
    }

    @Override
    protected Object getObjectAt(int index) {
      Field field = desc.getFieldAt(index);
      return toPigObject(field, desc.getFieldValue(index, tObject), true);
    }
  }

  /**
   * Returns Pig schema for the Thrift struct.
   */
  public static Schema toSchema(Class<? extends TBase<?, ?>> tClass) {
    return toSchema(TStructDescriptor.getInstance(tClass));
  }

  public Schema toSchema() {
    return toSchema(structDesc);
  }

  public static Schema toSchema(TStructDescriptor tDesc ) {
    Schema schema = new Schema();

    try {
      for(Field field : tDesc.getFields()) {
        schema.add(singleFieldToFieldSchema(field.getName(), field));
      }
    } catch (FrontendException t) {
      throw new RuntimeException(t);
    }

    return schema;
  }

  /**
   * return {@link FieldSchema} for a given field.
   */
  private static FieldSchema singleFieldToFieldSchema(String fieldName, Field field) throws FrontendException {
    //TODO we should probably implement better naming, the current system is pretty nonsensical now
    switch (field.getType()) {
      case TType.STRUCT:
        return new FieldSchema(fieldName, toSchema(field.gettStructDescriptor()), DataType.TUPLE);
      case TType.LIST:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", field.getListElemField()), DataType.BAG);
      case TType.SET:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", field.getSetElemField()), DataType.BAG);
      case TType.MAP:
        if (field.getMapKeyField().getType() != TType.STRING
            && field.getMapKeyField().getType() != TType.ENUM) {
          LOG.warn("Using a map with non-string key for field " + field.getName()
              + ". while converting to PIG Tuple, toString() is used for the key."
              + " It could result in incorrect maps.");
        }
        return new FieldSchema(fieldName, new Schema(singleFieldToFieldSchema(fieldName, field.getMapValueField())), DataType.MAP);
      default:
        return new FieldSchema(fieldName, null, getPigDataType(field));
    }
  }

  /**
   * A helper function which wraps a Schema in a tuple (for Pig bags) if our version of pig makes it necessary
   */
  private static Schema wrapInTupleIfPig9(Schema schema) throws FrontendException {
      if (PigUtil.Pig9orNewer) {
          return new Schema(new FieldSchema("t",schema,DataType.TUPLE));
      } else {
          return schema;
      }
  }

  /**
   * Returns a schema with single tuple (for Pig bags).
   */
  private static Schema singleFieldToTupleSchema(String fieldName, Field field) throws FrontendException {
    switch (field.getType()) {
      case TType.STRUCT:
        return wrapInTupleIfPig9(toSchema(field.gettStructDescriptor()));
      case TType.LIST:
      case TType.SET:
      case TType.MAP:
        return wrapInTupleIfPig9(new Schema(singleFieldToFieldSchema(fieldName, field)));
      default:
        return wrapInTupleIfPig9(new Schema(new FieldSchema(fieldName, null, getPigDataType(field))));
    }
  }

  private static byte getPigDataType(Field field) {
    switch (field.getType()) {
      case TType.BOOL:
      case TType.BYTE:
      case TType.I16:
      case TType.I32:
        return DataType.INTEGER;
      case TType.ENUM:
        return DataType.CHARARRAY;
      case TType.I64:
        return DataType.LONG;
      case TType.DOUBLE:
        return DataType.DOUBLE;
      case TType.STRING:
        return field.isBuffer() ? DataType.BYTEARRAY : DataType.CHARARRAY;
      default:
        throw new IllegalArgumentException("Unexpected type where a simple type is expected : " + field.getType());
    }
  }

  /**
   * Turn a Thrift Struct into a loading schema for a pig script.
   */
  public static String toPigScript(Class<? extends TBase<?, ?>> thriftClass,
                                   Class<? extends LoadFunc> pigLoader) {
    StringBuilder sb = new StringBuilder();
    /* we are commenting out explicit schema specification. The schema is
     * included mainly to help the readers of the pig script. Pig learns the
     * schema directly from the loader.
     * If explicit schema is not commented, we might have surprising results
     * when a Thrift class (possibly in control of another team) changes,
     * but the Pig script is not updated. Commenting it out avoids this.
     */
    StringBuilder prefix = new StringBuilder("       --  ");
    sb.append("raw_data = load '$INPUT_FILES' using ")
      .append(pigLoader.getName())
      .append("('")
      .append(thriftClass.getName())
      .append("');\n")
      .append(prefix)
      .append("as ");
    prefix.append("   ");

    try {
      stringifySchema(sb, toSchema(thriftClass), DataType.TUPLE, prefix);
    } catch (FrontendException e) {
      throw new RuntimeException(e);
    }

    sb.append("\n");
    return sb.toString();
  }

  /**
   * Print formatted schema. This is a modified version of
   * {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
   * with support for (indented) pretty printing.
   */
  // This is used for building up output string
  // type can only be BAG or TUPLE
  public static void stringifySchema(StringBuilder sb,
                                     Schema schema,
                                     byte type,
                                     StringBuilder prefix)
                                          throws FrontendException{
      // this is a modified version of {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
      if (type == DataType.TUPLE) {
          sb.append("(") ;
      }
      else if (type == DataType.BAG) {
          sb.append("{") ;
      }

      prefix.append("  ");
      sb.append("\n").append(prefix);

      if (schema == null) {
          sb.append("null") ;
      }
      else {
          boolean isFirst = true ;
          for (int i=0; i< schema.size() ;i++) {

              if (!isFirst) {
                  sb.append(",\n").append(prefix);
              }
              else {
                  isFirst = false ;
              }

              FieldSchema fs = schema.getField(i) ;

              if(fs == null) {
                  sb.append("null");
                  continue;
              }

              if (fs.alias != null) {
                  sb.append(fs.alias);
                  sb.append(": ");
              }

              if (DataType.isAtomic(fs.type)) {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
              else if ( (fs.type == DataType.TUPLE) ||
                        (fs.type == DataType.BAG) ) {
                  // safety net
                  if (schema != fs.schema) {
                      stringifySchema(sb, fs.schema, fs.type, prefix) ;
                  }
                  else {
                      throw new AssertionError("Schema refers to itself "
                                               + "as inner schema") ;
                  }
              } else if (fs.type == DataType.MAP) {
                sb.append(DataType.findTypeName(fs.type) + "[");
                if (fs.schema!=null)
                    stringifySchema(sb, fs.schema, fs.type, prefix);
                sb.append("]");
              } else {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
          }
      }

      prefix.setLength(prefix.length()-2);
      sb.append("\n").append(prefix);

      if (type == DataType.TUPLE) {
          sb.append(")") ;
      }
      else if (type == DataType.BAG) {
          sb.append("}") ;
      }
  }

  public static void main(String[] args) throws Exception {
    if (args.length > 0) {
      Class<? extends TBase<?, ?>> tClass = ThriftUtils.getTypeRef(args[0]).getRawClass();
      System.out.println(args[0] + " : " + toSchema(tClass).toString());
      System.out.println(toPigScript(tClass, ThriftPigLoader.class));
    }
  }
}=======
package com.twitter.elephantbird.pig.util;

import java.nio.ByteBuffer;
import java.util.Arrays;
import java.util.Collection;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;

import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.pig.LoadFunc;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.apache.thrift.TBase;
import org.apache.thrift.protocol.TType;

import com.google.common.collect.Lists;
import com.twitter.elephantbird.pig.load.ThriftPigLoader;
import com.twitter.elephantbird.thrift.TStructDescriptor;
import com.twitter.elephantbird.thrift.TStructDescriptor.Field;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;

/**
 * <li> converts a Thrift struct to a Pig tuple
 * <li> utilities to provide schema for Pig loaders and Pig scripts
 */
public class ThriftToPig<M extends TBase<?, ?>> {

  public static final Logger LOG = LogManager.getLogger(ThriftToPig.class);

  private static BagFactory bagFactory = BagFactory.getInstance();
  private static TupleFactory tupleFactory  = TupleFactory.getInstance();

  private TStructDescriptor structDesc;

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(Class<M> tClass) {
    return new ThriftToPig<M>(tClass);
  }

  public static <M extends TBase<?, ?>> ThriftToPig<M> newInstance(TypeRef<M> typeRef) {
    return new ThriftToPig<M>(typeRef.getRawClass());
  }

  public ThriftToPig(Class<M> tClass) {
    structDesc = TStructDescriptor.getInstance(tClass);
  }

  public TStructDescriptor getTStructDescriptor() {
    return structDesc;
  }

  /**
   * Converts a thrift object to Pig tuple.
   * All the fields are deserialized.
   * It might be better to use getLazyTuple() if not all fields
   * are required.
   */
  public Tuple getPigTuple(M thriftObj) {
    return toTuple(structDesc, thriftObj);
  }

  /**
   * Similar to {@link #getPigTuple(TBase)}. This delays
   * serialization of tuple contents until they are requested.
   * @param thriftObj
   * @return
   */
  public Tuple getLazyTuple(M thriftObj) {
    return new LazyTuple(structDesc, thriftObj);
  }

  @SuppressWarnings("rawtypes")
  private static <T extends TBase>Tuple toTuple(TStructDescriptor tDesc, T tObj) {
    int size = tDesc.getFields().size();
    Tuple tuple = tupleFactory.newTuple(size);
    for (int i=0; i<size; i++) {
      Field field = tDesc.getFieldAt(i);
      Object value = tDesc.getFieldValue(i, tObj);
      try {
        tuple.set(i, toPigObject(field, value, false));
      } catch (ExecException e) { // not expected
        throw new RuntimeException(e);
      }
    }
    return tuple;
  }

  @SuppressWarnings("unchecked")
  public static Object toPigObject(Field field, Object value, boolean lazy) {
    if (value == null) {
      return null;
    }

    switch (field.getType()) {
    case TType.BOOL:
      return Integer.valueOf((Boolean)value ? 1 : 0);
    case TType.BYTE :
      return Integer.valueOf((Byte)value);
    case TType.I16 :
      return Integer.valueOf((Short)value);
    case TType.STRING:
      return stringTypeToPig(value);
    case TType.STRUCT:
      if (lazy) {
        return new LazyTuple(field.gettStructDescriptor(), (TBase<?, ?>)value);
      } else {
        return toTuple(field.gettStructDescriptor(), (TBase<?, ?>)value);
      }
    case TType.MAP:
      return toPigMap(field, (Map<Object, Object>)value, lazy);
    case TType.SET:
      return toPigBag(field.getSetElemField(), (Collection<Object>)value, lazy);
    case TType.LIST:
      return toPigBag(field.getListElemField(), (Collection<Object>)value, lazy);
    case TType.ENUM:
      return value.toString();
    default:
      // standard types : I32, I64, DOUBLE, etc.
      return value;
    }
  }

  /**
   * TType.STRING is a mess in Thrift. It could be byte[], ByteBuffer,
   * or even a String!.
   */
  private static Object stringTypeToPig(Object value) {
    if (value instanceof String) {
      return value;
    }
    if (value instanceof byte[]) {
      byte[] buf = (byte[])value;
      return new DataByteArray(Arrays.copyOf(buf, buf.length));
    }
    if (value instanceof ByteBuffer) {
      ByteBuffer bin = (ByteBuffer)value;
      byte[] buf = new byte[bin.remaining()];
      bin.mark();
      bin.get(buf);
      bin.reset();
      return new DataByteArray(buf);
    }
    return null;
  }

  private static Map<String, Object> toPigMap(Field field,
                                              Map<Object, Object> map,
                                              boolean lazy) {
    // PIG map's key always a String. just use toString() and hope
    // things would work out ok.
    HashMap<String, Object> out = new HashMap<String, Object>(map.size());
    Field valueField = field.getMapValueField();
    for(Entry<Object, Object> e : map.entrySet()) {
      Object prev = out.put(e.getKey().toString(),
                            toPigObject(valueField, e.getValue(), lazy));
      if (prev != null) {
        String msg = "Duplicate keys while converting to String while "
          + " processing map " + field.getName() + " (key type : "
          + field.getMapKeyField().getType() + " value type : "
          + field.getMapValueField().getType() + ")";
        LOG.warn(msg);
        throw new RuntimeException(msg);
      }
    }
    return out;
  }

  private static DataBag toPigBag(Field field,
                                  Collection<Object> values,
                                  boolean lazy) {
    List<Tuple> tuples = Lists.newArrayListWithExpectedSize(values.size());
    for(Object value : values) {
      Object pValue = toPigObject(field, value, lazy);
      if (pValue instanceof Tuple) { // DataBag should contain Tuples
        tuples.add((Tuple)pValue);
      } else {
        tuples.add(tupleFactory.newTuple(pValue));
      }
    }
    return bagFactory.newDefaultBag(tuples);
  }

  @SuppressWarnings("serial")
  /**
   * Delays serialization of Thrift fields until they are requested.
   */
  private static class LazyTuple extends AbstractLazyTuple {
    /* NOTE : This is only a partial optimization. The other part
     * is to avoid deserialization of the Thrift fields from the
     * binary buffer.
     *
     * Currently TDeserializer allows deserializing just one field,
     * psuedo-skipping over the fields before it.
     * But if we are going deserialize 5 fields out of 20, we will be
     * skipping over same set of fields multiple times. OTOH this might
     * still be better than a full deserialization.
     *
     * We need to write our own version of TBinaryProtocol that truly skips.
     * Even TDeserializer 'skips'/ignores only after deserializing fields.
     * (e.g. Strings, Integers, buffers etc).
     */
    private TBase<?, ?> tObject;
    private TStructDescriptor desc;

    LazyTuple(TStructDescriptor desc, TBase<?, ?> tObject) {
      initRealTuple(desc.getFields().size());
      this.tObject = tObject;
      this.desc = desc;
    }

    @Override
    protected Object getObjectAt(int index) {
      Field field = desc.getFieldAt(index);
      return toPigObject(field, desc.getFieldValue(index, tObject), true);
    }
  }

  /**
   * Returns Pig schema for the Thrift struct.
   */
  public static Schema toSchema(Class<? extends TBase<?, ?>> tClass) {
    return toSchema(TStructDescriptor.getInstance(tClass));
  }

  public Schema toSchema() {
    return toSchema(structDesc);
  }

  public static Schema toSchema(TStructDescriptor tDesc ) {
    Schema schema = new Schema();

    try {
      for(Field field : tDesc.getFields()) {
        schema.add(singleFieldToFieldSchema(field.getName(), field));
      }
    } catch (FrontendException t) {
      throw new RuntimeException(t);
    }

    return schema;
  }

  /**
   * return {@link FieldSchema} for a given field.
   */
  private static FieldSchema singleFieldToFieldSchema(String fieldName, Field field) throws FrontendException {
    //TODO we should probably implement better naming, the current system is pretty nonsensical now
    switch (field.getType()) {
      case TType.STRUCT:
        return new FieldSchema(fieldName, toSchema(field.gettStructDescriptor()), DataType.TUPLE);
      case TType.LIST:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", field.getListElemField()), DataType.BAG);
      case TType.SET:
        return new FieldSchema(fieldName, singleFieldToTupleSchema(fieldName + "_tuple", field.getSetElemField()), DataType.BAG);
      case TType.MAP:
        if (field.getMapKeyField().getType() != TType.STRING
            && field.getMapKeyField().getType() != TType.ENUM) {
          LOG.warn("Using a map with non-string key for field " + field.getName()
              + ". while converting to PIG Tuple, toString() is used for the key."
              + " It could result in incorrect maps.");
        }
        return new FieldSchema(fieldName, new Schema(singleFieldToFieldSchema(null, field.getMapValueField())), DataType.MAP);
      default:
        return new FieldSchema(fieldName, null, getPigDataType(field));
    }
  }

  /**
   * A helper function which wraps a Schema in a tuple (for Pig bags) if our version of pig makes it necessary
   */
  private static Schema wrapInTupleIfPig9(Schema schema) throws FrontendException {
      if (PigUtil.Pig9orNewer) {
          return new Schema(new FieldSchema("t",schema,DataType.TUPLE));
      } else {
          return schema;
      }
  }

  /**
   * Returns a schema with single tuple (for Pig bags).
   */
  private static Schema singleFieldToTupleSchema(String fieldName, Field field) throws FrontendException {
    switch (field.getType()) {
      case TType.STRUCT:
        return wrapInTupleIfPig9(toSchema(field.gettStructDescriptor()));
      case TType.LIST:
      case TType.SET:
      case TType.MAP:
        return wrapInTupleIfPig9(new Schema(singleFieldToFieldSchema(fieldName, field)));
      default:
        return wrapInTupleIfPig9(new Schema(new FieldSchema(fieldName, null, getPigDataType(field))));
    }
  }

  private static byte getPigDataType(Field field) {
    switch (field.getType()) {
      case TType.BOOL:
      case TType.BYTE:
      case TType.I16:
      case TType.I32:
        return DataType.INTEGER;
      case TType.ENUM:
        return DataType.CHARARRAY;
      case TType.I64:
        return DataType.LONG;
      case TType.DOUBLE:
        return DataType.DOUBLE;
      case TType.STRING:
        return field.isBuffer() ? DataType.BYTEARRAY : DataType.CHARARRAY;
      default:
        throw new IllegalArgumentException("Unexpected type where a simple type is expected : " + field.getType());
    }
  }

  /**
   * Turn a Thrift Struct into a loading schema for a pig script.
   */
  public static String toPigScript(Class<? extends TBase<?, ?>> thriftClass,
                                   Class<? extends LoadFunc> pigLoader) {
    StringBuilder sb = new StringBuilder();
    /* we are commenting out explicit schema specification. The schema is
     * included mainly to help the readers of the pig script. Pig learns the
     * schema directly from the loader.
     * If explicit schema is not commented, we might have surprising results
     * when a Thrift class (possibly in control of another team) changes,
     * but the Pig script is not updated. Commenting it out avoids this.
     */
    StringBuilder prefix = new StringBuilder("       --  ");
    sb.append("raw_data = load '$INPUT_FILES' using ")
      .append(pigLoader.getName())
      .append("('")
      .append(thriftClass.getName())
      .append("');\n")
      .append(prefix)
      .append("as ");
    prefix.append("   ");

    try {
      stringifySchema(sb, toSchema(thriftClass), DataType.TUPLE, prefix);
    } catch (FrontendException e) {
      throw new RuntimeException(e);
    }

    sb.append("\n");
    return sb.toString();
  }

  /**
   * Print formatted schema. This is a modified version of
   * {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
   * with support for (indented) pretty printing.
   */
  // This is used for building up output string
  // type can only be BAG or TUPLE
  public static void stringifySchema(StringBuilder sb,
                                     Schema schema,
                                     byte type,
                                     StringBuilder prefix)
                                          throws FrontendException{
      // this is a modified version of {@link Schema#stringifySchema(StringBuilder, Schema, byte)}
      if (type == DataType.TUPLE) {
          sb.append("(") ;
      }
      else if (type == DataType.BAG) {
          sb.append("{") ;
      }

      prefix.append("  ");
      sb.append("\n").append(prefix);

      if (schema == null) {
          sb.append("null") ;
      }
      else {
          boolean isFirst = true ;
          for (int i=0; i< schema.size() ;i++) {

              if (!isFirst) {
                  sb.append(",\n").append(prefix);
              }
              else {
                  isFirst = false ;
              }

              FieldSchema fs = schema.getField(i) ;

              if(fs == null) {
                  sb.append("null");
                  continue;
              }

              if (fs.alias != null) {
                  sb.append(fs.alias);
                  sb.append(": ");
              }

              if (DataType.isAtomic(fs.type)) {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
              else if ( (fs.type == DataType.TUPLE) ||
                        (fs.type == DataType.BAG) ) {
                  // safety net
                  if (schema != fs.schema) {
                      stringifySchema(sb, fs.schema, fs.type, prefix) ;
                  }
                  else {
                      throw new AssertionError("Schema refers to itself "
                                               + "as inner schema") ;
                  }
              } else if (fs.type == DataType.MAP) {
                sb.append(DataType.findTypeName(fs.type) + "[");
                if (fs.schema!=null)
                    stringifySchema(sb, fs.schema, fs.type, prefix);
                sb.append("]");
              } else {
                  sb.append(DataType.findTypeName(fs.type)) ;
              }
          }
      }

      prefix.setLength(prefix.length()-2);
      sb.append("\n").append(prefix);

      if (type == DataType.TUPLE) {
          sb.append(")") ;
      }
      else if (type == DataType.BAG) {
          sb.append("}") ;
      }
  }

  public static void main(String[] args) throws Exception {
    if (args.length > 0) {
      Class<? extends TBase<?, ?>> tClass = ThriftUtils.getTypeRef(args[0]).getRawClass();
      System.out.println(args[0] + " : " + toSchema(tClass).toString());
      System.out.println(toPigScript(tClass, ThriftPigLoader.class));
    }
  }
}>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/core/src/main/java/com/twitter/elephantbird/util/ThriftToDynamicProto.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/core/src/main/java/com/twitter/elephantbird/util/ThriftToDynamicProto.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/core/src/main/java/com/twitter/elephantbird/util/LzoUtils.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/core/src/main/java/com/twitter/elephantbird/util/LzoUtils.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/core/src/main/java/com/twitter/elephantbird/util/ThriftToProto.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/core/src/main/java/com/twitter/elephantbird/util/ThriftToProto.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/core/src/main/java/com/twitter/elephantbird/util/ThriftToProto.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/core/src/main/java/com/twitter/elephantbird/mapreduce/output/LzoOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/core/src/main/java/com/twitter/elephantbird/mapreduce/output/LzoOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/core/src/main/java/com/twitter/elephantbird/mapreduce/output/LzoOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/core/src/main/java/com/twitter/elephantbird/mapreduce/input/LzoGenericProtobufBlockInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/core/src/main/java/com/twitter/elephantbird/mapreduce/io/ThriftConverter.java;<<<<<<< MINE
import com.twitter.elephantbird.thrift.ThriftBinaryDeserializer;
||||||| BASE
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
=======
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/core/src/main/java/com/twitter/elephantbird/mapreduce/io/ThriftConverter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/core/src/main/java/com/twitter/elephantbird/mapreduce/io/ThriftConverter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/rcfile/src/main/java/com/twitter/elephantbird/mapreduce/output/RCFileThriftOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/rcfile/src/main/java/com/twitter/elephantbird/mapreduce/output/RCFileProtobufOutputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/rcfile/src/main/java/com/twitter/elephantbird/mapreduce/input/RCFileThriftInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/rcfile/src/main/java/com/twitter/elephantbird/mapreduce/input/RCFileProtobufInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/rcfile/src/main/java/com/twitter/elephantbird/mapreduce/input/RCFileProtobufInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/rcfile/src/main/java/com/twitter/elephantbird/mapreduce/input/RCFileProtobufInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/rcfile/src/main/java/com/twitter/elephantbird/mapreduce/input/RCFileProtobufInputFormat.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/pig/src/main/java/com/twitter/elephantbird/pig/store/LzoThriftBlockPigStorage.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/pig/src/main/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/pig/src/main/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/pig/src/main/java/com/twitter/elephantbird/pig/util/ThriftToPig.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/pig/src/main/java/com/twitter/elephantbird/pig/piggybank/JsonStringToMap.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/pig/src/main/java/com/twitter/elephantbird/pig/piggybank/JsonStringToMap.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_56e97f9_c1ca987/rev_56e97f9-c1ca987/pig/src/main/java/com/twitter/elephantbird/pig/piggybank/JsonStringToMap.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_643ff62_8dd1811/rev_643ff62-8dd1811/src/java/com/twitter/elephantbird/mapreduce/output/RCFileThriftOutputFormat.java;<<<<<<< MINE
package com.twitter.elephantbird.mapreduce.output;

import java.io.IOException;
import java.util.List;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hive.serde2.ByteStream;
import org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable;
import org.apache.hadoop.hive.serde2.columnar.BytesRefWritable;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.thrift.TBase;
import org.apache.thrift.TException;
import org.apache.thrift.protocol.TBinaryProtocol;
import org.apache.thrift.protocol.TField;
import org.apache.thrift.protocol.TProtocolUtil;
import org.apache.thrift.protocol.TType;
import org.apache.thrift.transport.TIOStreamTransport;
import org.apache.thrift.transport.TMemoryInputTransport;

import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Maps;
import com.twitter.data.proto.Misc.ColumnarMetadata;
import com.twitter.elephantbird.mapreduce.io.ThriftWritable;
import com.twitter.elephantbird.thrift.TStructDescriptor;
import com.twitter.elephantbird.thrift.TStructDescriptor.Field;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;

/**
 * OutputFormat for storing Thrift objects in RCFile.<p>
 *
 * Each of the top level fields is stored in a separate column.
 * Thrift field ids are stored in RCFile metadata.<p>
 *
 * The user can write either a {@link ThriftWritable} with the Thrift object
 * or a {@link BytesWritable} with serialized Thrift bytes. The latter
 * ensures that all the fields are preserved even if the current Thrift
 * definition does not match the definition represented in the serialized bytes.
 * Any fields not recognized by current Thrift class are stored in the last
 * column.
 */
public class RCFileThriftOutputFormat extends RCFileOutputFormat {

  // typeRef is only required for setting metadata for the RCFile
  private TypeRef<? extends TBase<?, ?>> typeRef;
  private TStructDescriptor tDesc;
  private List<Field> tFields;
  private int numColumns;

  private BytesRefArrayWritable rowWritable = new BytesRefArrayWritable();
  private BytesRefWritable[] colValRefs;

  /** internal, for MR use only. */
  public RCFileThriftOutputFormat() {
  }

  public RCFileThriftOutputFormat(TypeRef<? extends TBase<?, ?>> typeRef) { // for PigLoader etc.
    this.typeRef = typeRef;
    init();
  }

  private void init() {
    tDesc = TStructDescriptor.getInstance(typeRef.getRawClass());
    tFields = tDesc.getFields();
    numColumns = tFields.size() + 1; // known fields + 1 for unknown fields
    colValRefs = new BytesRefWritable[numColumns];

    for (int i = 0; i < numColumns; i++) {
      colValRefs[i] = new BytesRefWritable();
      rowWritable.set(i, colValRefs[i]);
    }
  }

  protected ColumnarMetadata makeColumnarMetadata() {
    ColumnarMetadata.Builder metadata = ColumnarMetadata.newBuilder();

    metadata.setClassname(typeRef.getRawClass().getName());
    for(Field fd : tDesc.getFields()) {
      metadata.addFieldId(fd.getFieldId());
    }
    metadata.addFieldId(-1); // -1 for unknown fields

    return metadata.build();
  }

  private class ProtobufWriter extends RCFileOutputFormat.Writer {

    private ByteStream.Output byteStream = new ByteStream.Output();
    private TBinaryProtocol tProto = new TBinaryProtocol(
                                        new TIOStreamTransport(byteStream));

    // used when deserializing thrift bytes
    private Map<Short, Integer> idMap;
    private TMemoryInputTransport mTransport;
    private TBinaryProtocol skipProto;

    ProtobufWriter(TaskAttemptContext job) throws IOException {
      super(RCFileThriftOutputFormat.this, job, Protobufs.toText(makeColumnarMetadata()));
    }

    @Override @SuppressWarnings("unchecked")
    public void write(NullWritable key, Writable value) throws IOException, InterruptedException {
      try {
        if (value instanceof BytesWritable) {
          // TODO: handled errors
          fromBytes((BytesWritable)value);
        } else {
          fromObject((TBase<?, ?>)((ThriftWritable)value).get());
        }
      } catch (TException e) {
        // might need to tolerate a few errors.
        throw new IOException(e);
      }

      super.write(null, rowWritable);
    }

    @SuppressWarnings("unchecked")
    private void fromObject(TBase tObj)
                    throws IOException, InterruptedException, TException {

      byteStream.reset(); // reinitialize the byteStream if buffer is too large?
      int startPos = 0;

      // top level fields are split across the columns.
      for (int i=0; i < numColumns; i++) {

        if (i < (numColumns - 1)) {

          Field fd = tFields.get(i);
          if (tObj.isSet(fd.getFieldIdEnum())) {
            ThriftUtils.writeFieldNoTag(tProto, fd, tDesc.getFieldValue(i, tObj));
          }

        } // else { }  : no 'unknown fields' in thrift object

        colValRefs[i].set(byteStream.getData(),
                          startPos,
                          byteStream.getCount() - startPos);
        startPos = byteStream.getCount();
      }
    }

    /**
     * extract serialized bytes for each field, including unknown fields and
     * store those byes in columns.
     */
    private void fromBytes(BytesWritable bytesWritable)
                       throws IOException, InterruptedException, TException {

      if (mTransport == null) {
        initIdMap();
        mTransport = new TMemoryInputTransport();
        skipProto = new TBinaryProtocol(mTransport);
      }

      byte[] bytes = bytesWritable.getBytes();
      mTransport.reset(bytes, 0, bytesWritable.getLength());
      byteStream.reset();

      // set all the fields to null
      for(BytesRefWritable ref : colValRefs) {
        ref.set(bytes, 0, 0);
      }

      skipProto.readStructBegin();

      while (true) {
        int start = mTransport.getBufferPosition();

        TField field = skipProto.readFieldBegin();
        if (field.type == TType.STOP) {
          break;
        }

        int fieldStart = mTransport.getBufferPosition();

        // skip still creates and copies primitive objects (String, buffer, etc)
        // skipProto could override readString() and readBuffer() to avoid that.
        TProtocolUtil.skip(skipProto, field.type);

        int end = mTransport.getBufferPosition();

        Integer idx = idMap.get(field.id);

        if (idx != null && field.type == tFields.get(idx).getType()) {
          // known field
          colValRefs[idx].set(bytes, fieldStart, end-fieldStart);
        } else {
          // unknown field, copy the bytes to last column (with field id)
          byteStream.write(bytes, start, end-start);
        }
      }

      if (byteStream.getCount() > 0) {
        byteStream.write(TType.STOP);
        colValRefs[colValRefs.length-1].set(byteStream.getData(),
                                            0,
                                            byteStream.getCount());
      }
    }

    private void initIdMap() {
      idMap = Maps.newHashMap();
      for(int i=0; i<tFields.size(); i++) {
        idMap.put(tFields.get(i).getFieldId(), i);
      }
      idMap = ImmutableMap.copyOf(idMap);
    }
  }

  /**
   * Stores supplied class name in configuration. This configuration is
   * read on the remote tasks to initialize the output format correctly.
   */
  public static void setClassConf(Class<? extends TBase<?, ?> > thriftClass, Configuration conf) {
    ThriftUtils.setClassConf(conf, RCFileThriftOutputFormat.class, thriftClass);
  }

  @Override
  public RecordWriter<NullWritable, Writable>
    getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException {

    if (typeRef == null) {
      typeRef = ThriftUtils.getTypeRef(job.getConfiguration(), RCFileProtobufOutputFormat.class);
      init();
    }

    RCFileOutputFormat.setColumnNumber(job.getConfiguration(), numColumns);
    return new ProtobufWriter(job);
  }
}||||||| BASE
package com.twitter.elephantbird.mapreduce.output;

import java.io.IOException;
import java.util.List;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hive.serde2.ByteStream;
import org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable;
import org.apache.hadoop.hive.serde2.columnar.BytesRefWritable;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.thrift.TBase;
import org.apache.thrift.TException;
import org.apache.thrift.protocol.TBinaryProtocol;
import org.apache.thrift.protocol.TField;
import org.apache.thrift.protocol.TProtocolUtil;
import org.apache.thrift.protocol.TType;
import org.apache.thrift.transport.TIOStreamTransport;
import org.apache.thrift.transport.TMemoryInputTransport;

import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Maps;
import com.twitter.data.proto.Misc.ColumnarMetadata;
import com.twitter.elephantbird.mapreduce.io.ThriftWritable;
import com.twitter.elephantbird.thrift.TStructDescriptor;
import com.twitter.elephantbird.thrift.TStructDescriptor.Field;
import com.twitter.elephantbird.util.ThriftUtils;
import com.twitter.elephantbird.util.TypeRef;

/**
 * OutputFormat for storing Thrift objects in RCFile.<p>
 *
 * Each of the top level fields is stored in a separate column.
 * Thrift field ids are stored in RCFile metadata.<p>
 *
 * The user can write either a {@link ThriftWritable} with the Thrift object
 * or a {@link BytesWritable} with serialized Thrift bytes. The latter
 * ensures that all the fields are preserved even if the current Thrift
 * definition does not match the definition represented in the serialized bytes.
 * Any fields not recognized by current Thrift class are stored in the last
 * column.
 */
public class RCFileThriftOutputFormat extends RCFileOutputFormat {

  // typeRef is only required for setting metadata for the RCFile
  private TypeRef<? extends TBase<?, ?>> typeRef;
  private TStructDescriptor tDesc;
  private List<Field> tFields;
  private int numColumns;

  private BytesRefArrayWritable rowWritable = new BytesRefArrayWritable();
  private BytesRefWritable[] colValRefs;

  /** internal, for MR use only. */
  public RCFileThriftOutputFormat() {
  }

  public RCFileThriftOutputFormat(TypeRef<? extends TBase<?, ?>> typeRef) { // for PigLoader etc.
    this.typeRef = typeRef;
    init();
  }

  private void init() {
    tDesc = TStructDescriptor.getInstance(typeRef.getRawClass());
    tFields = tDesc.getFields();
    numColumns = tFields.size() + 1; // known fields + 1 for unknown fields
    colValRefs = new BytesRefWritable[numColumns];

    for (int i = 0; i < numColumns; i++) {
      colValRefs[i] = new BytesRefWritable();
      rowWritable.set(i, colValRefs[i]);
    }
  }

  protected ColumnarMetadata makeColumnarMetadata() {
    ColumnarMetadata.Builder metadata = ColumnarMetadata.newBuilder();

    metadata.setClassname(typeRef.getRawClass().getName());
    for(Field fd : tDesc.getFields()) {
      metadata.addFieldId(fd.getFieldId());
    }
    metadata.addFieldId(-1); // -1 for unknown fields

    return metadata.build();
  }

  private class ProtobufWriter extends RCFileOutputFormat.Writer {

    private ByteStream.Output byteStream = new ByteStream.Output();
    private TBinaryProtocol tProto = new TBinaryProtocol(
                                        new TIOStreamTransport(byteStream));

    // used when deserializing thrift bytes
    private Map<Short, Integer> idMap;
    private TMemoryInputTransport mTransport;
    private TBinaryProtocol skipProto;

    ProtobufWriter(TaskAttemptContext job) throws IOException {
      super(RCFileThriftOutputFormat.this, job, makeColumnarMetadata());
    }

    @Override @SuppressWarnings("unchecked")
    public void write(NullWritable key, Writable value) throws IOException, InterruptedException {
      try {
        if (value instanceof BytesWritable) {
          // TODO: handled errors
          fromBytes((BytesWritable)value);
        } else {
          fromObject((TBase<?, ?>)((ThriftWritable)value).get());
        }
      } catch (TException e) {
        // might need to tolerate a few errors.
        throw new IOException(e);
      }

      super.write(null, rowWritable);
    }

    @SuppressWarnings("unchecked")
    private void fromObject(TBase tObj)
                    throws IOException, InterruptedException, TException {

      byteStream.reset(); // reinitialize the byteStream if buffer is too large?
      int startPos = 0;

      // top level fields are split across the columns.
      for (int i=0; i < numColumns; i++) {

        if (i < (numColumns - 1)) {

          Field fd = tFields.get(i);
          if (tObj.isSet(fd.getFieldIdEnum())) {
            ThriftUtils.writeFieldNoTag(tProto, fd, tDesc.getFieldValue(i, tObj));
          }

        } // else { }  : no 'unknown fields' in thrift object

        colValRefs[i].set(byteStream.getData(),
                          startPos,
                          byteStream.getCount() - startPos);
        startPos = byteStream.getCount();
      }
    }

    /**
     * extract serialized bytes for each field, including unknown fields and
     * store those byes in columns.
     */
    private void fromBytes(BytesWritable bytesWritable)
                       throws IOException, InterruptedException, TException {

      if (mTransport == null) {
        initIdMap();
        mTransport = new TMemoryInputTransport();
        skipProto = new TBinaryProtocol(mTransport);
      }

      byte[] bytes = bytesWritable.getBytes();
      mTransport.reset(bytes, 0, bytesWritable.getLength());
      byteStream.reset();

      // set all the fields to null
      for(BytesRefWritable ref : colValRefs) {
        ref.set(bytes, 0, 0);
      }

      skipProto.readStructBegin();

      while (true) {
        int start = mTransport.getBufferPosition();

        TField field = skipProto.readFieldBegin();
        if (field.type == TType.STOP) {
          break;
        }

        int fieldStart = mTransport.getBufferPosition();

        // skip still creates and copies primitive objects (String, buffer, etc)
        // skipProto could override readString() and readBuffer() to avoid that.
        TProtocolUtil.skip(skipProto, field.type);

        int end = mTransport.getBufferPosition();

        Integer idx = idMap.get(field.id);

        if (idx != null && field.type == tFields.get(idx).getType()) {
          // known field
          colValRefs[idx].set(bytes, fieldStart, end-fieldStart);
        } else {
          // unknown field, copy the bytes to last column (with field id)
          byteStream.write(bytes, start, end-start);
        }
      }

      if (byteStream.getCount() > 0) {
        byteStream.write(TType.STOP);
        colValRefs[colValRefs.length-1].set(byteStream.getData(),
                                            0,
                                            byteStream.getCount());
      }
    }

    private void initIdMap() {
      idMap = Maps.newHashMap();
      for(int i=0; i<tFields.size(); i++) {
        idMap.put(tFields.get(i).getFieldId(), i);
      }
      idMap = ImmutableMap.copyOf(idMap);
    }
  }

  /**
   * Stores supplied class name in configuration. This configuration is
   * read on the remote tasks to initialize the output format correctly.
   */
  public static void setClassConf(Class<? extends TBase<?, ?> > thriftClass, Configuration conf) {
    ThriftUtils.setClassConf(conf, RCFileThriftOutputFormat.class, thriftClass);
  }

  @Override
  public RecordWriter<NullWritable, Writable>
    getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException {

    if (typeRef == null) {
      typeRef = ThriftUtils.getTypeRef(job.getConfiguration(), RCFileProtobufOutputFormat.class);
      init();
    }

    RCFileOutputFormat.setColumnNumber(job.getConfiguration(), numColumns);
    return new ProtobufWriter(job);
  }
}=======
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_643ff62_8dd1811/rev_643ff62-8dd1811/src/java/com/twitter/elephantbird/mapreduce/output/RCFileOutputFormat.java;<<<<<<< MINE
package com.twitter.elephantbird.mapreduce.output;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hive.ql.io.RCFile;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.SequenceFile.Metadata;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.GzipCodec;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.ReflectionUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.twitter.elephantbird.pig.util.RCFileUtil;

/**
 * Hive's {@link org.apache.hadoop.hive.ql.io.RCFileOutputFormat} is written for
 * deprecated OutputFormat. Pig requires newer OutputFormat.
 * In addition RCFileOutputFormat's functionality this class adds RCFile
 * metadata support.
 *
 * TODO: contribute this to PIG.
 */

public class RCFileOutputFormat extends FileOutputFormat<NullWritable, Writable> {

  private static final Logger LOG = LoggerFactory.getLogger(RCFileOutputFormat.class);

  // in case we need different compression from global default compression
  public static String COMPRESSION_CODEC_CONF = "elephantbird.rcfile.output.compression.codec";

  public static String DEFAULT_EXTENSION = ".rc";
  public static String EXTENSION_OVERRIDE_CONF = "elephantbird.refile.output.filename.extension"; // "none" disables it.

  /**
   * set number of columns into the given configuration.
   *
   * @param conf
   *          configuration instance which need to set the column number
   * @param columnNum
   *          column number for RCFile's Writer
   *
   */
  public static void setColumnNumber(Configuration conf, int columnNum) {
    assert columnNum > 0;
    conf.setInt(RCFile.COLUMN_NUMBER_CONF_STR, columnNum);
  }

  /**
   * Returns the number of columns set in the conf for writers.
   *
   * @param conf
   * @return number of columns for RCFile's writer
   */
  public static int getColumnNumber(Configuration conf) {
    return conf.getInt(RCFile.COLUMN_NUMBER_CONF_STR, 0);
  }

  protected RCFile.Writer createRCFileWriter(TaskAttemptContext job,
                                             Text columnMetadata)
                                             throws IOException {
    Configuration conf = job.getConfiguration();

    // override compression codec if set.
    String codecOverride = conf.get(COMPRESSION_CODEC_CONF);
    if (codecOverride != null) {
      conf.setBoolean("mapred.output.compress", true);
      conf.set("mapred.output.compression.codec", codecOverride);
    }

    CompressionCodec codec = null;
    if (getCompressOutput(job)) {
      Class<? extends CompressionCodec> codecClass = getOutputCompressorClass(job, GzipCodec.class);
      codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, conf);
    }

    Metadata metadata = null;
    if (columnMetadata != null) {
      metadata = new Metadata();
      metadata.set(new Text(RCFileUtil.COLUMN_METADATA_PROTOBUF_KEY), columnMetadata);
    }

    String ext = conf.get(EXTENSION_OVERRIDE_CONF, DEFAULT_EXTENSION);
    Path file = getDefaultWorkFile(job, ext.equalsIgnoreCase("none") ? null : ext);

    LOG.info("writing to rcfile " + file.toString());

    return new RCFile.Writer(file.getFileSystem(conf), conf, file, job, metadata, codec);
  }

  /**
   * RecordWriter wrapper around an RCFile.Writer
   */
  static protected class Writer extends RecordWriter<NullWritable, Writable> {

    private RCFile.Writer rcfile;

    protected Writer(RCFileOutputFormat outputFormat,
                     TaskAttemptContext job,
                     Text columnMetadata) throws IOException {
      rcfile = outputFormat.createRCFileWriter(job, columnMetadata);
    }

    @Override
    public void close(TaskAttemptContext context) throws IOException, InterruptedException {
      rcfile.close();
    }

    @Override
    public void write(NullWritable key, Writable value) throws IOException, InterruptedException {
      rcfile.append(value);
      // add counters
    }
  }

  @Override
  public RecordWriter<NullWritable, Writable> getRecordWriter(
      TaskAttemptContext job) throws IOException, InterruptedException {
    return new Writer(this, job, null);
  }
}||||||| BASE
package com.twitter.elephantbird.mapreduce.output;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hive.ql.io.RCFile;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.SequenceFile.Metadata;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.GzipCodec;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.ReflectionUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.twitter.data.proto.Misc.ColumnarMetadata;
import com.twitter.elephantbird.pig.util.RCFileUtil;
import com.twitter.elephantbird.util.Protobufs;

/**
 * Hive's {@link org.apache.hadoop.hive.ql.io.RCFileOutputFormat} is written for
 * deprecated OutputFormat. Pig requires newer OutputFormat.
 * In addition RCFileOutputFormat's functionality this class adds RCFile
 * metadata support.
 *
 * TODO: contribute this to PIG.
 */

public class RCFileOutputFormat extends FileOutputFormat<NullWritable, Writable> {

  private static final Logger LOG = LoggerFactory.getLogger(RCFileOutputFormat.class);

  // in case we need different compression from global default compression
  public static String COMPRESSION_CODEC_CONF = "elephantbird.rcfile.output.compression.codec";

  public static String DEFAULT_EXTENSION = ".rc";
  public static String EXTENSION_OVERRIDE_CONF = "elephantbird.refile.output.filename.extension"; // "none" disables it.

  /**
   * set number of columns into the given configuration.
   *
   * @param conf
   *          configuration instance which need to set the column number
   * @param columnNum
   *          column number for RCFile's Writer
   *
   */
  public static void setColumnNumber(Configuration conf, int columnNum) {
    assert columnNum > 0;
    conf.setInt(RCFile.COLUMN_NUMBER_CONF_STR, columnNum);
  }

  /**
   * Returns the number of columns set in the conf for writers.
   *
   * @param conf
   * @return number of columns for RCFile's writer
   */
  public static int getColumnNumber(Configuration conf) {
    return conf.getInt(RCFile.COLUMN_NUMBER_CONF_STR, 0);
  }

  protected RCFile.Writer createRCFileWriter(TaskAttemptContext job,
                                             ColumnarMetadata columnInfo)
                                             throws IOException {
    Configuration conf = job.getConfiguration();

    // override compression codec if set.
    String codecOverride = conf.get(COMPRESSION_CODEC_CONF);
    if (codecOverride != null) {
      conf.setBoolean("mapred.output.compress", true);
      conf.set("mapred.output.compression.codec", codecOverride);
    }

    CompressionCodec codec = null;
    if (getCompressOutput(job)) {
      Class<? extends CompressionCodec> codecClass = getOutputCompressorClass(job, GzipCodec.class);
      codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, conf);
    }

    Metadata metadata = null;
    if (columnInfo != null) {
      metadata = new Metadata();
      metadata.set(new Text(RCFileUtil.COLUMN_METADATA_PROTOBUF_KEY), Protobufs.toText(columnInfo));
    }

    String ext = conf.get(EXTENSION_OVERRIDE_CONF, DEFAULT_EXTENSION);
    Path file = getDefaultWorkFile(job, ext.equalsIgnoreCase("none") ? null : ext);

    LOG.info("writing to rcfile " + file.toString());

    return new RCFile.Writer(file.getFileSystem(conf), conf, file, job, metadata, codec);
  }

  /**
   * RecordWriter wrapper around an RCFile.Writer
   */
  static protected class Writer extends RecordWriter<NullWritable, Writable> {

    private RCFile.Writer rcfile;

    protected Writer(RCFileOutputFormat outputFormat,
                     TaskAttemptContext job,
                     ColumnarMetadata columnInfo) throws IOException {
      rcfile = outputFormat.createRCFileWriter(job, columnInfo);
    }

    @Override
    public void close(TaskAttemptContext context) throws IOException, InterruptedException {
      rcfile.close();
    }

    @Override
    public void write(NullWritable key, Writable value) throws IOException, InterruptedException {
      rcfile.append(value);
      // add counters
    }
  }

  @Override
  public RecordWriter<NullWritable, Writable> getRecordWriter(
      TaskAttemptContext job) throws IOException, InterruptedException {
    return new Writer(this, job, null);
  }
}=======
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_643ff62_8dd1811/rev_643ff62-8dd1811/src/java/com/twitter/elephantbird/mapreduce/output/RCFileProtobufOutputFormat.java;<<<<<<< MINE
package com.twitter.elephantbird.mapreduce.output;

import java.io.IOException;
import java.util.List;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hive.serde2.ByteStream;
import org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable;
import org.apache.hadoop.hive.serde2.columnar.BytesRefWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;

import com.google.protobuf.CodedOutputStream;
import com.google.protobuf.Message;
import com.google.protobuf.Descriptors.FieldDescriptor;
import com.google.protobuf.Message.Builder;
import com.twitter.data.proto.Misc.ColumnarMetadata;
import com.twitter.elephantbird.mapreduce.io.ProtobufWritable;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * OutputFormat for storing protobufs in RCFile.<p>
 *
 * Each of the top level fields is stored in a separate column.
 * The protobuf field numbers are stored in RCFile metadata.<p>
 *
 * A protobuf message can contain <a href="https://developers.google.com/protocol-buffers/docs/proto#updating">
 * "unknown fields"</a>. These fields are preserved and stored
 * in the last column. e.g. if protobuf A with 4 fields (a, b, c, d) is
 * serialized and when it is deserialized A has only 3 fields (a, c, d),
 * then 'b' is carried over as an unknown field.
 */
public class RCFileProtobufOutputFormat extends RCFileOutputFormat {

  /* typeRef is only required for setting metadata for the RCFile
   * if we delay file creation until the first row is written,
   * this info could be derived from protobuf being written.
   */
  private TypeRef<? extends Message> typeRef;
  private List<FieldDescriptor> msgFields;
  private int numColumns;

  private BytesRefArrayWritable rowWritable = new BytesRefArrayWritable();
  private BytesRefWritable[] colValRefs;
  private ByteStream.Output byteStream = new ByteStream.Output();
  private CodedOutputStream protoStream = CodedOutputStream.newInstance(byteStream);

  /** internal, for MR use only. */
  public RCFileProtobufOutputFormat() {
  }

  public RCFileProtobufOutputFormat(TypeRef<? extends Message> typeRef) { // for PigLoader etc.
    this.typeRef = typeRef;
    init();
  }

  private void init() {
    Builder msgBuilder = Protobufs.getMessageBuilder(typeRef.getRawClass());
    msgFields = msgBuilder.getDescriptorForType().getFields();
    numColumns = msgFields.size() + 1; // known fields + 1 for unknown fields
    colValRefs = new BytesRefWritable[numColumns];

    for (int i = 0; i < numColumns; i++) {
      colValRefs[i] = new BytesRefWritable();
      rowWritable.set(i, colValRefs[i]);
    }
  }

  protected ColumnarMetadata makeColumnarMetadata() {
    ColumnarMetadata.Builder metadata = ColumnarMetadata.newBuilder();

    metadata.setClassname(typeRef.getRawClass().getName());
    for(FieldDescriptor fd : msgFields) {
      metadata.addFieldId(fd.getNumber());
    }
    metadata.addFieldId(-1); // -1 for unknown fields

    return metadata.build();
  }

  private class ProtobufWriter extends RCFileOutputFormat.Writer {

    ProtobufWriter(TaskAttemptContext job) throws IOException {
      super(RCFileProtobufOutputFormat.this, job, Protobufs.toText(makeColumnarMetadata()));
    }

    @Override
    public void write(NullWritable key, Writable value) throws IOException, InterruptedException {
      @SuppressWarnings("unchecked")
      Message msg = ((ProtobufWritable<Message>)value).get();

      protoStream.flush();
      byteStream.reset(); // reinitialize the byteStream if buffer is too large?
      int startPos = 0;

      // top level fields are split across the columns.
      for (int i=0; i < numColumns; i++) {

        if (i < (numColumns - 1)) {

          FieldDescriptor fd = msgFields.get(i);
          if (fd.isRepeated() || msg.hasField(fd)) {
            Protobufs.writeFieldNoTag(protoStream, fd, msg.getField(fd));
          }

        } else { // last column : write unknown fields
          msg.getUnknownFields().writeTo(protoStream); // could be empty
        }

        protoStream.flush();
        colValRefs[i].set(byteStream.getData(),
                          startPos,
                          byteStream.getCount() - startPos);
        startPos = byteStream.getCount();
      }

      super.write(null, rowWritable);
    }
  }

  /**
   * Stores supplied class name in configuration. This configuration is
   * read on the remote tasks to initialize the output format correctly.
   */
  public static void setClassConf(Class<? extends Message> protoClass, Configuration conf) {
    Protobufs.setClassConf(conf, RCFileProtobufOutputFormat.class, protoClass);
  }

  @Override
  public RecordWriter<NullWritable, Writable>
    getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException {

    if (typeRef == null) {
      typeRef = Protobufs.getTypeRef(job.getConfiguration(), RCFileProtobufOutputFormat.class);
      init();
    }

    RCFileOutputFormat.setColumnNumber(job.getConfiguration(), numColumns);
    return new ProtobufWriter(job);
  }

}||||||| BASE
package com.twitter.elephantbird.mapreduce.output;

import java.io.IOException;
import java.util.List;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hive.serde2.ByteStream;
import org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable;
import org.apache.hadoop.hive.serde2.columnar.BytesRefWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;

import com.google.protobuf.CodedOutputStream;
import com.google.protobuf.Message;
import com.google.protobuf.Descriptors.FieldDescriptor;
import com.google.protobuf.Message.Builder;
import com.twitter.data.proto.Misc.ColumnarMetadata;
import com.twitter.elephantbird.mapreduce.io.ProtobufWritable;
import com.twitter.elephantbird.util.Protobufs;
import com.twitter.elephantbird.util.TypeRef;

/**
 * OutputFormat for storing protobufs in RCFile.<p>
 *
 * Each of the top level fields is stored in a separate column.
 * The protobuf field numbers are stored in RCFile metadata.<p>
 *
 * A protobuf message can contain <a href="https://developers.google.com/protocol-buffers/docs/proto#updating">
 * "unknown fields"</a>. These fields are preserved and stored
 * in the last column. e.g. if protobuf A with 4 fields (a, b, c, d) is
 * serialized and when it is deserialized A has only 3 fields (a, c, d),
 * then 'b' is carried over as an unknown field.
 */
public class RCFileProtobufOutputFormat extends RCFileOutputFormat {

  /* typeRef is only required for setting metadata for the RCFile
   * if we delay file creation until the first row is written,
   * this info could be derived from protobuf being written.
   */
  private TypeRef<? extends Message> typeRef;
  private List<FieldDescriptor> msgFields;
  private int numColumns;

  private BytesRefArrayWritable rowWritable = new BytesRefArrayWritable();
  private BytesRefWritable[] colValRefs;
  private ByteStream.Output byteStream = new ByteStream.Output();
  private CodedOutputStream protoStream = CodedOutputStream.newInstance(byteStream);

  /** internal, for MR use only. */
  public RCFileProtobufOutputFormat() {
  }

  public RCFileProtobufOutputFormat(TypeRef<? extends Message> typeRef) { // for PigLoader etc.
    this.typeRef = typeRef;
    init();
  }

  private void init() {
    Builder msgBuilder = Protobufs.getMessageBuilder(typeRef.getRawClass());
    msgFields = msgBuilder.getDescriptorForType().getFields();
    numColumns = msgFields.size() + 1; // known fields + 1 for unknown fields
    colValRefs = new BytesRefWritable[numColumns];

    for (int i = 0; i < numColumns; i++) {
      colValRefs[i] = new BytesRefWritable();
      rowWritable.set(i, colValRefs[i]);
    }
  }

  protected ColumnarMetadata makeColumnarMetadata() {
    ColumnarMetadata.Builder metadata = ColumnarMetadata.newBuilder();

    metadata.setClassname(typeRef.getRawClass().getName());
    for(FieldDescriptor fd : msgFields) {
      metadata.addFieldId(fd.getNumber());
    }
    metadata.addFieldId(-1); // -1 for unknown fields

    return metadata.build();
  }

  private class ProtobufWriter extends RCFileOutputFormat.Writer {

    ProtobufWriter(TaskAttemptContext job) throws IOException {
      super(RCFileProtobufOutputFormat.this, job, makeColumnarMetadata());
    }

    @Override
    public void write(NullWritable key, Writable value) throws IOException, InterruptedException {
      @SuppressWarnings("unchecked")
      Message msg = ((ProtobufWritable<Message>)value).get();

      protoStream.flush();
      byteStream.reset(); // reinitialize the byteStream if buffer is too large?
      int startPos = 0;

      // top level fields are split across the columns.
      for (int i=0; i < numColumns; i++) {

        if (i < (numColumns - 1)) {

          FieldDescriptor fd = msgFields.get(i);
          if (fd.isRepeated() || msg.hasField(fd)) {
            Protobufs.writeFieldNoTag(protoStream, fd, msg.getField(fd));
          }

        } else { // last column : write unknown fields
          msg.getUnknownFields().writeTo(protoStream); // could be empty
        }

        protoStream.flush();
        colValRefs[i].set(byteStream.getData(),
                          startPos,
                          byteStream.getCount() - startPos);
        startPos = byteStream.getCount();
      }

      super.write(null, rowWritable);
    }
  }

  /**
   * Stores supplied class name in configuration. This configuration is
   * read on the remote tasks to initialize the output format correctly.
   */
  public static void setClassConf(Class<? extends Message> protoClass, Configuration conf) {
    Protobufs.setClassConf(conf, RCFileProtobufOutputFormat.class, protoClass);
  }

  @Override
  public RecordWriter<NullWritable, Writable>
    getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException {

    if (typeRef == null) {
      typeRef = Protobufs.getTypeRef(job.getConfiguration(), RCFileProtobufOutputFormat.class);
      init();
    }

    RCFileOutputFormat.setColumnNumber(job.getConfiguration(), numColumns);
    return new ProtobufWriter(job);
  }

}=======
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_34cf049_2c0f0fe/rev_34cf049-2c0f0fe/rcfile/src/main/java/com/twitter/elephantbird/mapreduce/output/RCFileOutputFormat.java;<<<<<<< MINE
import com.twitter.elephantbird.pig.util.RCFileUtil;
||||||| BASE
import com.twitter.data.proto.Misc.ColumnarMetadata;
import com.twitter.elephantbird.pig.util.RCFileUtil;
import com.twitter.elephantbird.util.Protobufs;
=======
import com.twitter.data.proto.Misc.ColumnarMetadata;
import com.twitter.elephantbird.util.RCFileUtil;
import com.twitter.elephantbird.util.Protobufs;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_6d28c12_2c0f0fe/rev_6d28c12-2c0f0fe/rcfile/src/main/java/com/twitter/elephantbird/mapreduce/input/RCFileProtobufInputFormat.java;<<<<<<< MINE
    private Message               msgInstance;
    private Builder               msgBuilder;
    private boolean               readUnknownsColumn = false;
    private List<FieldDescriptor> knownRequiredFields = Lists.newArrayList();
    private ArrayList<Integer>    columnsBeingRead = Lists.newArrayList();
||||||| BASE
    private Builder               msgBuilder;
    private boolean               readUnknownsColumn = false;
    private List<FieldDescriptor> knownRequiredFields = Lists.newArrayList();
    private ArrayList<Integer>    columnsBeingRead = Lists.newArrayList();
=======
    protected Builder               msgBuilder;
    protected boolean               readUnknownsColumn = false;
    protected List<FieldDescriptor> knownRequiredFields = Lists.newArrayList();
    protected ArrayList<Integer>    columnsBeingRead = Lists.newArrayList();
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_6d28c12_2c0f0fe/rev_6d28c12-2c0f0fe/rcfile/src/main/java/com/twitter/elephantbird/mapreduce/input/RCFileProtobufInputFormat.java;<<<<<<< MINE
      currentValue = builder.build();
      return currentValue;
    }

    /**
     * Returns a Tuple consisting of required fields with out creating
     * a Protobuf message at the top level.
     */
    public Tuple getCurrentTupleValue() throws IOException, InterruptedException {

      BytesRefArrayWritable byteRefs = getCurrentValue();
      if (byteRefs == null) {
        return null;
      }

      Tuple tuple = tf.newTuple(knownRequiredFields.size());

      for (int i=0; i < knownRequiredFields.size(); i++) {
        BytesRefWritable buf = byteRefs.get(columnsBeingRead.get(i));
        FieldDescriptor fd = knownRequiredFields.get(i);
        Object value = null;
        if (buf.getLength() > 0) {
          value = Protobufs.readFieldNoTag(
              CodedInputStream.newInstance(buf.getData(), buf.getStart(), buf.getLength()),
              knownRequiredFields.get(i),
              msgBuilder);
        } else { // use the value from default instance
          value = msgInstance.getField(fd);
        }
        tuple.set(i, protoToPig.fieldToPig(fd, value));
      }

      if (readUnknownsColumn) {
        // we can handle this if needed.
        throw new IOException("getCurrentTupleValue() is not supported when 'readUnknownColumns' is set");
      }

      return tuple;
||||||| BASE
      currentValue = builder.build();
      return currentValue;
    }

    /**
     * Returns a Tuple consisting of required fields with out creating
     * a Protobuf message at the top level.
     */
    public Tuple getCurrentTupleValue() throws IOException, InterruptedException {

      BytesRefArrayWritable byteRefs = getCurrentValue();
      if (byteRefs == null) {
        return null;
      }

      Tuple tuple = tf.newTuple(knownRequiredFields.size());

      for (int i=0; i < knownRequiredFields.size(); i++) {
        BytesRefWritable buf = byteRefs.get(columnsBeingRead.get(i));
        FieldDescriptor fd = knownRequiredFields.get(i);
        Object value = null;
        if (buf.getLength() > 0) {
          value = Protobufs.readFieldNoTag(
              CodedInputStream.newInstance(buf.getData(), buf.getStart(), buf.getLength()),
              knownRequiredFields.get(i),
              msgBuilder);
        } else if (fd.getType() != FieldDescriptor.Type.MESSAGE) {
          value = fd.getDefaultValue();
        }
        tuple.set(i, protoToPig.fieldToPig(fd, value));
      }

      if (readUnknownsColumn) {
        // we can handle this if needed.
        throw new IOException("getCurrentTupleValue() is not supported when 'readUnknownColumns' is set");
      }

      return tuple;
=======
      return builder.build();
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_923d61a_468dd30/rev_923d61a-468dd30/src/java/com/twitter/elephantbird/thrift/TStructDescriptor.java;<<<<<<< MINE
package com.twitter.elephantbird.thrift;

import java.nio.ByteBuffer;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;

import org.apache.thrift.TBase;
import org.apache.thrift.TEnum;
import org.apache.thrift.TException;
import org.apache.thrift.TFieldIdEnum;
import org.apache.thrift.TUnion;
import org.apache.thrift.meta_data.EnumMetaData;
import org.apache.thrift.meta_data.FieldMetaData;
import org.apache.thrift.meta_data.FieldValueMetaData;
import org.apache.thrift.meta_data.ListMetaData;
import org.apache.thrift.meta_data.MapMetaData;
import org.apache.thrift.meta_data.SetMetaData;
import org.apache.thrift.meta_data.StructMetaData;
import org.apache.thrift.protocol.TType;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Maps;
import com.twitter.elephantbird.util.ThriftUtils;


/**
 * Expanded metadata of a Thrift class. The main purpose is
 * help recursive traversals of Thrift structs or objects
 * with the following :
 * <ul>
 *  <li> build much more detailed information about fields so that
 *       other iterators don't need to.
 *  <li> avoids runtime type checking while processing many objects of the
 *       same class (common case).
 *  <li> Handles different Thrift quirks.
 *
 */
public class TStructDescriptor {

  private static Map<Class<?>, TStructDescriptor> structMap = Maps.newHashMap();

  private List<Field> fields;
  private Class<? extends TBase<?, ?>> tClass;
  private boolean isUnion;

  public Class<? extends TBase<?, ?>> getThriftClass() {
    return tClass;
  }

  public TBase<?, ?> newThriftObject() throws TException {
    try {
      return tClass.newInstance();
    } catch (Exception e) { //not excpected
      throw new TException(e);
    }
  }
  /**
   * The list of fields returned is immutable.
   */
  public List<Field> getFields() {
    return fields;
  }

  public Field getFieldAt(int idx) {
    return fields.get(idx);
  }

  @SuppressWarnings("unchecked")
  public Object getFieldValue(int fieldIdx, TBase tObject) {
    /* Thrift 0.5 throws an NPE when the field is binary and
     * happens to be null. Otherwise user could just
     * invoke tObject.getFieldValue(field.getFieldIdEnum()).
     * Should revisit this once we move to a newer version.
     *
     * here, the assumption is the field is not null most of the
     * time. otherwise, rather than catching the exception, we
     * could cache 'BufferForFieldName()' method and invoke it.
     *
     * this also helps with unions.
     */
    Field field = fields.get(fieldIdx);
    try {
      if (isUnion && field.getFieldIdEnum() != ((TUnion<?, ?>)tObject).getSetField()) {
        return null;
      }
      return tObject.getFieldValue(field.getFieldIdEnum());
    } catch (NullPointerException e) {
      return null;
    }
  }

  /**
   * Creates a descriptor for a Thrift class
   */
  public static TStructDescriptor getInstance(Class<? extends TBase<?, ?>> tClass) {
    synchronized (structMap) {
      TStructDescriptor desc = structMap.get(tClass);
      if (desc == null) {
        desc = new TStructDescriptor();
        desc.tClass = tClass;
        structMap.put(tClass, desc);
        desc.build(tClass);
      }
      return desc;
    }
  }

  private TStructDescriptor() {
    // all the initialization is done in build().
  }

  private void build(Class<? extends TBase<?, ?>> tClass) {
    Map<? extends TFieldIdEnum, FieldMetaData> fieldMap = FieldMetaData.getStructMetaDataMap(tClass);
    Field[] arr = new Field[fieldMap.size()];

    isUnion = TUnion.class.isAssignableFrom(tClass);

    int idx = 0;
    for (Entry<? extends TFieldIdEnum, FieldMetaData> e : fieldMap.entrySet()) {
      arr[idx++] = new Field(e.getKey(),
                             e.getValue(),
                             e.getKey().getFieldName(),
                             tClass,
                             e.getValue().valueMetaData);
    }
    // make it immutable since users have access.
    fields = ImmutableList.copyOf(arr);
  }

  /**
   * returns 'enum name -> enum object' mapping.
   * Currently used for converting Tuple to a Thrift object.
   */
  static private Map<String, TEnum> extractEnumMap(Class<? extends TEnum> enumClass) {
    ImmutableMap.Builder<String, TEnum> builder = ImmutableMap.builder();
    for(TEnum e : enumClass.getEnumConstants()) {
      builder.put(e.toString(), e);
    }
    return builder.build();
  }

  /**
   * Maintains all the relevant info for a field
   */
  public static class Field {

    private final TFieldIdEnum fieldIdEnum;
    private final FieldMetaData fieldMetaData;
    private final short fieldId;
    private final String fieldName;
    private final FieldValueMetaData field;

    // following fields are set when they are relevant.
    private final Field listElemField;    // lists
    private final Field setElemField;     // sets
    private final Field mapKeyField;      // maps
    private final Field mapValueField;    // maps
    private final Map<String, TEnum> enumMap; // enums
    private final Map<Integer, TEnum> enumIdMap; // enums
    private final TStructDescriptor tStructDescriptor; // Structs
    private final boolean isBuffer_;  // strings


    @SuppressWarnings("unchecked") // for casting 'structClass' below
    private Field(TFieldIdEnum fieldIdEnum, FieldMetaData fieldMetaData, String fieldName, Class<?> enclosingClass,
                  FieldValueMetaData field) {
      // enclosingClass is only to check a TType.STRING is actually a buffer.
      this.fieldIdEnum = fieldIdEnum;
      this.fieldMetaData = fieldMetaData;
      this.fieldId = fieldIdEnum == null ? 1 : fieldIdEnum.getThriftFieldId();
      this.fieldName = fieldName;
      this.field = field;

      // common case, avoids type checks below.
      boolean simpleField = field.getClass() == FieldValueMetaData.class;

      if (!simpleField && field instanceof ListMetaData) {
        listElemField = new Field(null, null, fieldName + "_list_elem", null,
                                  ((ListMetaData)field).elemMetaData);
      } else {
        listElemField = null;
      }

      if (!simpleField && field instanceof MapMetaData) {
        mapKeyField = new Field(null, null, fieldName + "_map_key", null,
                                ((MapMetaData)field).keyMetaData);
        mapValueField = new Field(null, null, fieldName + "_map_value", null,
                            ((MapMetaData)field).valueMetaData);

      } else {
        mapKeyField = null;
        mapValueField = null;
      }

      if (!simpleField && field instanceof SetMetaData) {
        setElemField = new Field(null, null, fieldName + "_set_elem", null,
                                ((SetMetaData)field).elemMetaData);
      } else {
        setElemField = null;
      }

      if (!simpleField && field instanceof EnumMetaData) {
        enumMap = extractEnumMap(((EnumMetaData)field).enumClass);

        ImmutableMap.Builder<Integer, TEnum> builder = ImmutableMap.builder();
        for(TEnum e : enumMap.values()) {
          builder.put(e.getValue(), e);
        }
        enumIdMap = builder.build();
      } else {
        enumMap = null;
        enumIdMap = null;
      }

      if (field.isStruct()) {
        tStructDescriptor =
          getInstance((Class<? extends TBase<?, ?>>)
                      ((StructMetaData)field).structClass);

      } else {
        tStructDescriptor = null;
      }

      if (field.type == TType.STRING && enclosingClass != null) {
        // only Thrift 0.6 and above have explicit isBuffer() method.
        // until then a partial work around that works only if
        // the field is not inside a container.
        isBuffer_ =
          ThriftUtils.getFieldType(enclosingClass, fieldName) != String.class;
      } else {
        isBuffer_= false;
      }
    }

    public short getFieldId() {
      return fieldId;
    }

    public byte getType() {
      return field.type;
    }

    /**
     * This valid only for fields of a Struct. It is null for other fields
     * in containers like List.
     */
    public TFieldIdEnum getFieldIdEnum() {
      return fieldIdEnum;
    }

    public FieldValueMetaData getField() {
      return field;
    }

    public boolean isBuffer() {
      return isBuffer_;
    }

    public boolean isList() {
      return listElemField != null;
    }

    public Field getListElemField() {
      return listElemField;
    }

    public boolean isSet() {
      return setElemField != null;
    }

    public Field getSetElemField() {
      return setElemField;
    }

    public boolean isMap() {
      return mapKeyField != null;
    }

    public Field getMapKeyField() {
      return mapKeyField;
    }

    public Field getMapValueField() {
      return mapValueField;
    }

    public boolean isStruct() {
      return tStructDescriptor != null;
    }

    public TStructDescriptor gettStructDescriptor() {
      return tStructDescriptor;
    }

    public boolean isEnum() {
      return enumMap != null;
    }

    public TEnum getEnumValueOf(String name) {
      return enumMap.get(name);
    }

    public TEnum getEnumValueOf(int id) {
      return enumIdMap.get(id);
    }

    public String getName() {
      return fieldName;
    }

    public short getId() {
     return fieldId;
    }

    public FieldMetaData getFieldMetaData() {
      return fieldMetaData;
    }
  }
}||||||| BASE
package com.twitter.elephantbird.thrift;

import java.nio.ByteBuffer;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;

import org.apache.thrift.TBase;
import org.apache.thrift.TEnum;
import org.apache.thrift.TException;
import org.apache.thrift.TFieldIdEnum;
import org.apache.thrift.TUnion;
import org.apache.thrift.meta_data.EnumMetaData;
import org.apache.thrift.meta_data.FieldMetaData;
import org.apache.thrift.meta_data.FieldValueMetaData;
import org.apache.thrift.meta_data.ListMetaData;
import org.apache.thrift.meta_data.MapMetaData;
import org.apache.thrift.meta_data.SetMetaData;
import org.apache.thrift.meta_data.StructMetaData;
import org.apache.thrift.protocol.TType;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Maps;
import com.twitter.elephantbird.util.ThriftUtils;


/**
 * Expanded metadata of a Thrift class. The main purpose is
 * help recursive traversals of Thrift structs or objects
 * with the following :
 * <ul>
 *  <li> build much more detailed information about fields so that
 *       other iterators don't need to.
 *  <li> avoids runtime type checking while processing many objects of the
 *       same class (common case).
 *  <li> Handles different Thrift quirks.
 *
 */
public class TStructDescriptor {

  private static Map<Class<?>, TStructDescriptor> structMap = Maps.newHashMap();

  private List<Field> fields;
  private Class<? extends TBase<?, ?>> tClass;
  private boolean isUnion;

  public Class<? extends TBase<?, ?>> getThriftClass() {
    return tClass;
  }

  public TBase<?, ?> newThriftObject() throws TException {
    try {
      return tClass.newInstance();
    } catch (Exception e) { //not excpected
      throw new TException(e);
    }
  }
  /**
   * The list of fields returned is immutable.
   */
  public List<Field> getFields() {
    return fields;
  }

  public Field getFieldAt(int idx) {
    return fields.get(idx);
  }

  @SuppressWarnings("unchecked")
  public Object getFieldValue(int fieldIdx, TBase tObject) {
    /* Thrift 0.5 throws an NPE when the field is binary and
     * happens to be null. Otherwise user could just
     * invoke tObject.getFieldValue(field.getFieldIdEnum()).
     * Should revisit this once we move to a newer version.
     *
     * here, the assumption is the field is not null most of the
     * time. otherwise, rather than catching the exception, we
     * could cache 'BufferForFieldName()' method and invoke it.
     *
     * this also helps with unions.
     */
    Field field = fields.get(fieldIdx);
    try {
      if (isUnion && field.getFieldIdEnum() != ((TUnion<?, ?>)tObject).getSetField()) {
        return null;
      }
      return tObject.getFieldValue(field.getFieldIdEnum());
    } catch (NullPointerException e) {
      return null;
    }
  }

  /**
   * Creates a descriptor for a Thrift class
   */
  public static TStructDescriptor getInstance(Class<? extends TBase<?, ?>> tClass) {
    synchronized (structMap) {
      TStructDescriptor desc = structMap.get(tClass);
      if (desc == null) {
        desc = new TStructDescriptor();
        desc.tClass = tClass;
        structMap.put(tClass, desc);
        desc.build(tClass);
      }
      return desc;
    }
  }

  private TStructDescriptor() {
    // all the initialization is done in build().
  }

  private void build(Class<? extends TBase<?, ?>> tClass) {
    Map<? extends TFieldIdEnum, FieldMetaData> fieldMap = FieldMetaData.getStructMetaDataMap(tClass);
    Field[] arr = new Field[fieldMap.size()];

    isUnion = TUnion.class.isAssignableFrom(tClass);

    int idx = 0;
    for (Entry<? extends TFieldIdEnum, FieldMetaData> e : fieldMap.entrySet()) {
      arr[idx++] = new Field(e.getKey(),
                             e.getKey().getFieldName(),
                             tClass,
                             e.getValue().valueMetaData);
    }
    // make it immutable since users have access.
    fields = ImmutableList.copyOf(arr);
  }

  /**
   * returns 'enum name -> enum object' mapping.
   * Currently used for converting Tuple to a Thrift object.
   */
  static private Map<String, TEnum> extractEnumMap(Class<? extends TEnum> enumClass) {
    ImmutableMap.Builder<String, TEnum> builder = ImmutableMap.builder();
    for(TEnum e : enumClass.getEnumConstants()) {
      builder.put(e.toString(), e);
    }
    return builder.build();
  }

  /**
   * Maintains all the relevant info for a field
   */
  public static class Field {

    private final TFieldIdEnum fieldIdEnum;
    private final short fieldId;
    private final String fieldName;
    private final FieldValueMetaData field;

    // following fields are set when they are relevant.
    private final Field listElemField;    // lists
    private final Field setElemField;     // sets
    private final Field mapKeyField;      // maps
    private final Field mapValueField;    // maps
    private final Map<String, TEnum> enumMap; // enums
    private final Map<Integer, TEnum> enumIdMap; // enums
    private final TStructDescriptor tStructDescriptor; // Structs
    private final boolean isBuffer_;  // strings


    @SuppressWarnings("unchecked") // for casting 'structClass' below
    private Field(TFieldIdEnum fieldIdEnum, String fieldName, Class<?> enclosingClass,
                  FieldValueMetaData field) {
      // enclosingClass is only to check a TType.STRING is actually a buffer.
      this.fieldIdEnum = fieldIdEnum;
      this.fieldId = fieldIdEnum == null ? 1 : fieldIdEnum.getThriftFieldId();
      this.fieldName = fieldName;
      this.field = field;

      // common case, avoids type checks below.
      boolean simpleField = field.getClass() == FieldValueMetaData.class;

      if (!simpleField && field instanceof ListMetaData) {
        listElemField = new Field(null, fieldName + "_list_elem", null,
                                  ((ListMetaData)field).elemMetaData);
      } else {
        listElemField = null;
      }

      if (!simpleField && field instanceof MapMetaData) {
        mapKeyField = new Field(null, fieldName + "_map_key", null,
                                ((MapMetaData)field).keyMetaData);
        mapValueField = new Field(null, fieldName + "_map_value", null,
                            ((MapMetaData)field).valueMetaData);

      } else {
        mapKeyField = null;
        mapValueField = null;
      }

      if (!simpleField && field instanceof SetMetaData) {
        setElemField = new Field(null, fieldName + "_set_elem", null,
                                ((SetMetaData)field).elemMetaData);
      } else {
        setElemField = null;
      }

      if (!simpleField && field instanceof EnumMetaData) {
        enumMap = extractEnumMap(((EnumMetaData)field).enumClass);

        ImmutableMap.Builder<Integer, TEnum> builder = ImmutableMap.builder();
        for(TEnum e : enumMap.values()) {
          builder.put(e.getValue(), e);
        }
        enumIdMap = builder.build();
      } else {
        enumMap = null;
        enumIdMap = null;
      }

      if (field.isStruct()) {
        tStructDescriptor =
          getInstance((Class<? extends TBase<?, ?>>)
                      ((StructMetaData)field).structClass);

      } else {
        tStructDescriptor = null;
      }

      if (field.type == TType.STRING && enclosingClass != null) {
        // only Thrift 0.6 and above have explicit isBuffer() method.
        // until then a partial work around that works only if
        // the field is not inside a container.
        isBuffer_ =
          ThriftUtils.getFieldType(enclosingClass, fieldName) != String.class;
      } else {
        isBuffer_= false;
      }
    }

    public short getFieldId() {
      return fieldId;
    }

    public byte getType() {
      return field.type;
    }

    /**
     * This valid only for fields of a Struct. It is null for other fields
     * in containers like List.
     */
    public TFieldIdEnum getFieldIdEnum() {
      return fieldIdEnum;
    }

    public FieldValueMetaData getField() {
      return field;
    }

    public boolean isBuffer() {
      return isBuffer_;
    }

    public boolean isList() {
      return listElemField != null;
    }

    public Field getListElemField() {
      return listElemField;
    }

    public boolean isSet() {
      return setElemField != null;
    }

    public Field getSetElemField() {
      return setElemField;
    }

    public boolean isMap() {
      return mapKeyField != null;
    }

    public Field getMapKeyField() {
      return mapKeyField;
    }

    public Field getMapValueField() {
      return mapValueField;
    }

    public boolean isStruct() {
      return tStructDescriptor != null;
    }

    public TStructDescriptor gettStructDescriptor() {
      return tStructDescriptor;
    }

    public boolean isEnum() {
      return enumMap != null;
    }

    public TEnum getEnumValueOf(String name) {
      return enumMap.get(name);
    }

    public TEnum getEnumValueOf(int id) {
      return enumIdMap.get(id);
    }

    public String getName() {
      return fieldName;
    }

    public short getId() {
     return fieldId;
    }
  }
}=======
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_460cf9d_df176b3/rev_460cf9d-df176b3/core/src/test/java/com/twitter/elephantbird/mapreduce/input/TestLzoTextInputFormat.java;<<<<<<< MINE
||||||| BASE
import com.twitter.elephantbird.mapreduce.input.LzoTextInputFormat;
=======

import com.twitter.elephantbird.util.CoreTestUtil;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_460cf9d_df176b3/rev_460cf9d-df176b3/rcfile/src/test/java/com/twitter/elephantbird/pig/load/TestRCFileProtobufStorage.java;<<<<<<< MINE
import com.twitter.elephantbird.pig.util.UnitTestUtil;
import com.twitter.elephantbird.util.ContextUtil;
||||||| BASE
import com.twitter.elephantbird.pig.util.UnitTestUtil;
=======
import com.twitter.elephantbird.pig.util.PigTestUtil;
import com.twitter.elephantbird.util.CoreTestUtil;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_460cf9d_df176b3/rev_460cf9d-df176b3/rcfile/src/test/java/com/twitter/elephantbird/pig/load/TestRCFileProtobufStorage.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_460cf9d_df176b3/rev_460cf9d-df176b3/rcfile/src/test/java/com/twitter/elephantbird/pig/load/TestRCFileThriftStorage.java;<<<<<<< MINE
import com.twitter.elephantbird.pig.util.UnitTestUtil;
import com.twitter.elephantbird.util.ContextUtil;
||||||| BASE
import com.twitter.elephantbird.pig.util.UnitTestUtil;
=======
import com.twitter.elephantbird.pig.util.PigTestUtil;
import com.twitter.elephantbird.util.CoreTestUtil;
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_460cf9d_df176b3/rev_460cf9d-df176b3/rcfile/src/test/java/com/twitter/elephantbird/pig/load/TestRCFileThriftStorage.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_460cf9d_df176b3/rev_460cf9d-df176b3/pig/src/test/java/com/twitter/elephantbird/pig/util/AbstractTestWritableConverter.java;<<<<<<< MINE
import com.twitter.elephantbird.util.ContextUtil;
||||||| BASE
import com.twitter.elephantbird.pig.util.UnitTestUtil;
=======
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_460cf9d_df176b3/rev_460cf9d-df176b3/pig/src/test/java/com/twitter/elephantbird/pig/util/AbstractTestWritableConverter.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_dec9394_814e05e/rev_dec9394-814e05e/core/src/main/java/com/twitter/elephantbird/thrift/TStructDescriptor.java;<<<<<<< MINE
import org.apache.thrift.meta_data.FieldValueMetaData;
import org.apache.thrift.meta_data.ListMetaData;
import org.apache.thrift.meta_data.MapMetaData;
import org.apache.thrift.meta_data.SetMetaData;
||||||| BASE
import org.apache.thrift.meta_data.FieldValueMetaData;
import org.apache.thrift.meta_data.ListMetaData;
import org.apache.thrift.meta_data.MapMetaData;
import org.apache.thrift.meta_data.SetMetaData;
import org.apache.thrift.meta_data.StructMetaData;
=======
>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_c8c3348_ea96846/rev_c8c3348-ea96846/hadoop-compat/src/main/java/com/twitter/elephantbird/util/HadoopCompat.java;<<<<<<< MINE

      GET_CONFIGURATION_METHOD  = JobContext.class        .getMethod("getConfiguration");
      SET_STATUS_METHOD         = TaskAttemptContext.class.getMethod("setStatus", String.class);
      GET_TASK_ATTEMPT_ID       = TaskAttemptContext.class.getMethod("getTaskAttemptID");
      INCREMENT_COUNTER_METHOD  = Counter.class           .getMethod("increment", Long.TYPE);
      GET_COUNTER_VALUE_METHOD  = Counter.class           .getMethod("getValue");
      GET_JOB_ID_METHOD         = JobContext.class        .getMethod("getJobID");
      GET_JOB_NAME_METHOD       = JobContext.class        .getMethod("getJobName");
      GET_INPUT_SPLIT_METHOD    = MapContext.class        .getMethod("getInputSplit");

||||||| BASE
      GET_CONFIGURATION_METHOD = Class.forName(PACKAGE+".JobContext")
                                    .getMethod("getConfiguration");
      SET_STATUS_METHOD = Class.forName(PACKAGE+".TaskAttemptContext")
                                    .getMethod("setStatus", String.class);
      GET_TASK_ATTEMPT_ID = Class.forName(PACKAGE+".TaskAttemptContext")
                                    .getMethod("getTaskAttemptID");
      INCREMENT_COUNTER_METHOD = Class.forName(PACKAGE+".Counter")
                                    .getMethod("increment", Long.TYPE);
=======
      GET_CONFIGURATION_METHOD = Class.forName(PACKAGE+".JobContext")
                                    .getMethod("getConfiguration");
      SET_STATUS_METHOD = Class.forName(PACKAGE+".TaskAttemptContext")
                                    .getMethod("setStatus", String.class);
      GET_TASK_ATTEMPT_ID = Class.forName(PACKAGE+".TaskAttemptContext")
                                    .getMethod("getTaskAttemptID");
      INCREMENT_COUNTER_METHOD = Class.forName(PACKAGE+".Counter")
                                    .getMethod("increment", Long.TYPE);
      PROGRESS_METHOD = Class.forName(PACKAGE+".TaskAttemptContext")
                                    .getMethod("progress");

>>>>>>> YOURS
/home/taes/taes/projects/elephant-bird/revisions/rev_9f1257c_88aca05/rev_9f1257c-88aca05/core/src/main/java/com/twitter/elephantbird/mapreduce/input/LzoBinaryBlockRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_9f1257c_88aca05/rev_9f1257c-88aca05/core/src/main/java/com/twitter/elephantbird/mapreduce/input/LzoBinaryBlockRecordReader.java;null
/home/taes/taes/projects/elephant-bird/revisions/rev_9f1257c_88aca05/rev_9f1257c-88aca05/core/src/main/java/com/twitter/elephantbird/mapreduce/io/BinaryBlockReader.java;null
